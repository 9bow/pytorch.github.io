---
layout: blog_detail
title: 'PyTorch 1.8 Release, including Compiler Updates, New Profiling tools and Distributed Training improvements'
author: Team PyTorch 
---

Today, we’re announcing the availability of PyTorch 1.8 including updates to TorchCSPRNG, TorchVision, TorchText, TorchXLA and TorchAudio. We’ve also released a new library for performance profiling called Kineto, which underpins the new torch.profiler APIs. Lastly, we are providing beta level support for AMD ROCm through binaries that are available via pytorch.org. For more on the library releases, see the post here.

The PyTorch 1.8 release, composed of more than 3000 commits since 1.7, includes major updates and new features for compilation and code optimization, large scale training including pipeline and model parallelism and gradient compression, new and updated frontend APIs for scientific computing and overall a big step forward for performance profiling and visualization. A few of the highlights include:

1. Support for doing python to python functional transformations via torch.fx;

2. Added or stabilized APIs to support FFTs (torch.fft), Linear Algebra functions (torch.linalg), added support for autograd for complex tensors and updates to improve performance for calculating hessians and jacobians;

3. Significant updates and improvement to distributed training including: Improved NCCL reliability; Pipeline parallelism support; RPC profiling; and support for communication hooks adding gradient compression; and
4. Major updates to profiling including: the new Kineto library which underpins torch.profile with support for device and runtime events, stack track traces and visualization through a new TensorBoard plugin (thanks to the MSFT team)

As previously noted, features in PyTorch releases are classified as Stable, Beta and Prototype. Prototype features are not included as part of the binary distribution and are instead available through either building from source, using nightlies or via compiler flag. You can learn more about the definitions in the post here. You can also find the full release notes here.


## New and Updated APIs

The PyTorch 1.8 release brings a host of new and updated API surfaces ranging from additional APIs for numpy compatibility, as well support for ways to improve and scale your code for performance at both inference and training time. Here is a brief summary of the major features coming in this release:

### [Stable] Torch.fft support for high performance numpy style FFTs
As part of PyTorch’s goal to support scientific computing, we’ve invested in improving our FFT support and with PyTorch 1.8 we’re releasing the torch.fft module. This module implements the same functions as NumPy’s np.fft module, but with support for accelerators, like GPUs, and autograd.

* See the post here for more details
* Documentation - https://pytorch.org/docs/1.8.0/fft.html

### [Beta] Support for numpy style linear algebra functions via torch.linalg
The torch.linalg module, modeled after NumPy’s np.linalg module, brings numpy-style support for common linear algebra operations including Cholesky decompositions, determinants, eigenvalues and many others.

Documentation - https://pytorch.org/docs/1.8.0/linalg.html

## [Beta] Python code Transformations with FX
torch.fx is a toolkit that allows you to write transformations over PyTorch Python code, modifying the behavior of the model without having to modify the original source code. Concretely, FX allows you to write transformations of the form transform(input_module : nn.Module) -> nn.Module, where you can feed in a Module instance and get a transformed Module instance out of it.

Because transforms written with FX work on nn.Modules, the results of these transforms can be used in any place a normal Module can be used, including in training and with TorchScript.

### An FX transform example:


```python
import torch
import torch.fx

def transform(m: nn.Module,
             tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:
   # Step 1: Acquire a Graph representing the code in `m`
  
   # NOTE: torch.fx.symbolic_trace is a wrapper around a call to
   # fx.Tracer.trace and constructing a GraphModule. We'll
   # split that out in our transform to allow the caller to
   # customize tracing behavior.
   graph : torch.fx.Graph = tracer_class().trace(m)
  
   # Step 2: Modify this Graph or create a new one
   graph = ...
  
   # Step 3: Construct a Module to return
   return torch.fx.GraphModule(m, graph)

```

* Documentation - https://pytorch.org/docs/master/fx.html

* Share feedback about FX on the forums or issue tracker

## Distributed Training
The PyTorch 1.8 release added a number of new features as well as improvements to reliability and usability. Concretely, support for: Stable level async error/timeout handling was added to improve NCCL reliability; and stable support for RPC based profiling. Additionally we’ve added support for pipeline parallelism as well as gradient compression through the use of communication hooks in DDP. Details are below:

### [Beta] Pipeline Parallelism
As machine learning models continue to grow in size, traditional Distributed DataParallel (DDP) training no longer scales as these models don’t fit on a single GPU device. The new pipeline parallelism feature provides an easy to use PyTorch API to leverage pipeline parallelism as part of your training loop.

```python
torch.distributed.pipeline.sync.Pipe(
   module,
   chunks=1,
   checkpoint='except_last',
   deferred_batch_norm=False)
```

* RFC - https://github.com/pytorch/pytorch/issues/44827
* Documentation - https://pytorch.org/docs/1.8.0/pipeline.html?highlight=pipeline#

### [Beta] DDP Communication Hook
The DDP communication hook is a generic interface to control how to communicate gradients across workers by overriding the vanilla allreduce in DistributedDataParallel. A few built-in communication hooks are provided including PowerSGD, and users can easily apply any of these hooks to optimize communication. Additionally, the communication hook interface can also support user-defined communication strategies for more advanced use cases.

* RFC - https://github.com/pytorch/pytorch/issues/39272

* Documentation - https://pytorch.org/docs/1.8.0/ddp_comm_hooks.html?highlight=powersgd

### Additional Prototype Features
In addition to the major stable and beta distributed training features in this release, we also have a number of prototype features available in our nightlies to try out and provide feedback. We’ve linked in the draft docs below for reference:

* **(Prototype) ZeroRedundancyOptimizer** - based on and in partnership with the Microsoft DeepSpeed team, this feature helps reduce per-process memory footprint by sharding optimizer states across all participating processes in the ProcessGroup gang.

* **(Prototype) Process Group NCCL Send/Recv** - The NCCL send/recv API was introduced in v2.7 and this feature adds support for it in NCCL process groups. This feature will provide an option for users to implement collective operations at Python layer instead of C++ layer.

* **(Prototype) CUDA-support in RPC using TensorPipe** - This feature should bring consequent speed improvements for users of PyTorch RPC with multiple-GPU machines, as TensorPipe will automatically leverage NVLink when available, and avoid costly copies to and from host memory when exchanging GPU tensors between processes. When not on the same machine, TensorPipe will fall back to copying the tensor to host memory and sending it as a regular CPU tensor. This will also improve the user experience as users will be able to treat GPU tensors like regular CPU tensors in their code.

* **(Prototype) Remote Module** - This feature allows users to operate a module on a remote worker like using a local module, where the RPCs are transparent to the user. In the past, this functionality was implemented in an ad-hoc way and overall this feature will improve the usability of model parallelism on PyTorch.

### PyTorch Mobile
Support for PyTorch Mobile is expanding with a new set of tutorials to help new users more quickly launch models on-device, and give existing users tool to get more out of our framework. These include:

* Image segmentation DeepLabV3 on iOS: https://pytorch.org/tutorials/beginner/deeplabv3_on_ios.html

* Image segmentation DeepLabV3 on Android: https://pytorch.org/tutorials/beginner/deeplabv3_on_android.html

Our new demo apps also include examples of image segmentation, object detection, neural machine translation, question answering and vision transformers. They are available on both iOS and Android:

* iOS demo app: https://github.com/pytorch/ios-demo-app
* Android demo app: https://github.com/pytorch/android-demo-app

In addition to performance improvements on CPU for MobileNetV3 and other models, we also revamped our Android GPU backend prototype for broader models coverage and faster inferencing:

* Android tutorial: https://pytorch.org/tutorials/prototype/vulkan_workflow.html

Finally we are launching the PyTorch Mobile Lite Interpreted as a prototype feature in this release. The Lite Interpreter allows users to reduce the runtime binary size. Please try these out and send us your feedback on the [PyTorch Forums](https://discuss.pytorch.org/c/mobile/). All our latest updates can be found on the [PyTorch Mobile page](https://pytorch.org/mobile/home/)

### [Prototype] PyTorch Mobile Lite Interpreter

PyTorch Lite Interpreter is a streamlined version of the PyTorch runtime that can execute PyTorch programs in resource constrained devices, with reduced binary size footprint. This prototype feature reduces binary sizes significantly compared to the current on-device runtime. Today, you can get a 69% reduction with DeepLabV3 in armeabi-v7a Android, going from 9.7 (full build) to 4.6 MB (custom build) to 3 MB (lite interpreter + custom build), compressed. Further size reduction is expected in beta (specified file list, template selective build, etc.). For DeepLabV3 In iOS, the static library (libtorch_cpu.a) size goes from 78MB (full build), to 51 MB(lite interpreter + custom build). As a note, the static library size is an intermediate number, and the actual library size will be further optimized when it is linked to the final binary.

* [iOS/Android Tutorial](https://pytorch.org/tutorials/prototype/lite_interpreter.html)

## Profiling, Performance, & Model Optimization
In 1.8, we are introducing significant updates to PyTorch Profiler to enable better performance analysis and troubleshooting for large scale deep learning models. Updates include a creation of a new GPU profiling library integrated with PyTorch and a TensorBoard plugin for intuitive visualization of profiling results. This new combination makes it easy to highlight performance bottlenecks and display detailed timelines for both CPU and GPU events for deep dives.

Note that the PyTorch Profiler API is moving from torch.autograd.profiler to torch.profiler.

### (Stable) Profiler Stack Trace Support

Support for stack traces allows users the ability to see not only operator name/inputs in the profiler output table but also where the operator is in the code. A Workflow for example would be: user uses profiler as before, with optional new with_stack and group_by_stack_n parameters.

* [Documentation](https://pytorch.org/docs/stable/autograd.html)

### (Beta) Powering the new torch.Profiler API with Kineto
Kineto is a PyTorch performance profiling library (libkineto) focused on providing low-overhead full-system instrumentation for production workloads. Libkineto is fully integrated with the PyTorch Profiler, providing GPU profiling capabilities and in the future other system-level profiling. When using torch.profiler, Kineto is enabled by default.

* Documentation
* Blog Post

### (Beta) Tensorboard Plugin
The visualization and analysis plugin in Tensboard is now available as beta. This plugin provides a summary view such as execution time across data pipeline, CPU, GPU, and kernel time. Users can also get specific views such as Chrome view (host and GPU views), CUDA runtime activity, and GPU Kernel views. The plugin is also integrated with VSCode. You can see the demo here.

* [Documentation](https://github.com/pytorch/kineto/tree/master/tb_plugin)

### (Beta) Benchmark utils
Benchmark utils allows user to take accurate performance measurements, and provides composable tools to help with both benchmark formulation and post processing. This expected to be helpful for contributors to PyTorch to quickly understand how their contributions are impacting PyTorch performance.

Example:
```Demonstration of how size can affect efficiency.

1K:  Task is small, overhead dominates.
1M:  Good performance.
16M: Task does not fit into L2 cache which degrades performance.
```

```python
import torch
import torch.utils._benchmark as benchmark_utils

for n in [1024, 1024 ** 2, 32 * 1024 ** 2]:
   timer = benchmark_utils.Timer(
       "torch.dot(x, y)",
       description=f"n = {n}",
       globals={
           "x": torch.ones((n,)),
           "y": torch.ones((n,)),
       }
   )
   m = timer.blocked_autorange(min_run_time=1)
   print(f"{m}\n{m.median / n * 1e9:5.2f} ns / element\n")
  
  
<torch.utils._benchmark.utils.common.Measurement object at 0x7fdd96947a50>
torch.dot(x, y)
n = 1024
Median: 2.37 us
 IQR:    0.07 us (2.34 to 2.41)
 415 measurements, 1000 runs per measurement, 1 thread
2.31 ns / element

<torch.utils._benchmark.utils.common.Measurement object at 0x7fdd95be15d0>
torch.dot(x, y)
n = 1048576
 Median: 265.56 us
 IQR:    5.56 us (263.17 to 268.74)
 374 measurements, 10 runs per measurement, 1 thread
0.25 ns / element

<torch.utils._benchmark.utils.common.Measurement object at 0x7fdd91e14110>
torch.dot(x, y)
n = 33554432
 Median: 29.80 ms
 IQR:    0.58 ms (29.53 to 30.10)
 34 measurements, 1 runs per measurement, 1 thread
0.89 ns / element
```

* [Documentation](https://pytorch.org/docs/master/benchmark_utils.html?highlight=timer#torch.utils.benchmark.Timer)

* [Tutorial](https://pytorch.org/tutorials/recipes/recipes/benchmark.html)

## Additional Prototype Features
In addition to the major stable and beta features in this release, we are also opening up a new automated quantization API. See the details below:

***(Prototype) FX Graph Mode Quantization*** -  FX Graph Mode Quantization is the new automated quantization API in PyTorch. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx).

* [Documentation](https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization)

* Tutorial

## Hardware Support

### [Beta] Ability to Extend the PyTorch Dispatcher for a new backend in C++
In PyTorch 1.8, you can now create new out of tree devices that live outside the pytorch/pytorch repo. The tutorial linked below shows how to register your device and keep it in sync with native PyTorch devices.

* [Tutorial](https://pytorch.org/tutorials/advanced/extend_dispatcher.html)

### [Beta] AMD GPU Binaries Now Available
Starting in Pytorch 1.8, we’ve added support for ROCm wheels providing an easy onboarding to using AMD GPUs. You can simply go to the standard PyTorch installation selector and choose ROCm as an installation option and execute the provided command.

Cheers!

***Team PyTorch***


