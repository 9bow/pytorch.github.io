{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveGlow\n",
    "\n",
    "*Author: NVIDIA*\n",
    "\n",
    "**WaveGlow model for generating speech from mel spectrograms (generated by Tacotron2)**\n",
    "\n",
    "![alt](https://pytorch.org/assets/images/waveglow_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "hub_model = torch.hub.load('nvidia/DeepLearningExamples', 'nvidia_waveglow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will load the WaveGlow model pre-trained on [LJ Speech dataset](https://keithito.com/LJ-Speech-Dataset/)\n",
    "\n",
    "### Model Description\n",
    "\n",
    "The Tacotron 2 and WaveGlow model form a text-to-speech system that enables user to synthesise a natural sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model (also available via torch.hub) produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech.\n",
    "\n",
    "### Example\n",
    "\n",
    "In the example below:\n",
    "- pretrained Tacotron2 and Waveglow models are loaded from torch.hub\n",
    "- Tacotron2 generates mel spectrogram given tensor represantation of an input text (\"Hello world, I missed you\")\n",
    "- Waveglow generates sound given the mel spectrogram\n",
    "- the output sound is saved in an 'audio.wav' file\n",
    "\n",
    "To run the example you need the following python packages installed:\n",
    "> numpy, scipy, librosa, unidecode, inflect, librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "tacotron2 = torch.hub.load('nvidia/DeepLearningExamples', 'nvidia_tacotron2')\n",
    "tacotron2 = tacotron2.cuda()\n",
    "tacotron2.eval()\n",
    "\n",
    "waveglow = torch.hub.load('nvidia/DeepLearningExamples', 'nvidia_waveglow')\n",
    "waveglow = waveglow.remove_weightnorm(waveglow)\n",
    "waveglow = waveglow.cuda()\n",
    "waveglow.eval()\n",
    "\n",
    "text = \"hello world, I missed you\"\n",
    "sequence = np.array(tacotron2.text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "sequence = torch.from_numpy(sequence).cuda().long()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, mel, _, _ = tacotron2.infer(sequence)\n",
    "    audio = waveglow.infer(mel)\n",
    "\n",
    "write(\"audio.wav\", 22050, audio[0].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details\n",
    "For detailed information on model input and output, training recipies, inference and performance visit: [github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2) and/or [NGC](https://ngc.nvidia.com/catalog/model-scripts/nvidia:tacotron_2_and_waveglow_for_pytorch)\n",
    "\n",
    "### References\n",
    "\n",
    " - [Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/abs/1712.05884)\n",
    " - [WaveGlow: A Flow-based Generative Network for Speech Synthesis](https://arxiv.org/abs/1811.00002)\n",
    " - [Tacotron2 and WaveGlow on NGC](https://ngc.nvidia.com/catalog/model-scripts/nvidia:tacotron_2_and_waveglow_for_pytorch)\n",
    " - [Tacotron2 and Waveglow on github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
