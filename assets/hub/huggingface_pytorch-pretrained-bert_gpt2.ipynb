{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is optionally accelerated with a GPU runtime.\n",
    "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# GPT-2\n",
    "\n",
    "*Author: HuggingFace Team*\n",
    "\n",
    "**Language Models are Unsupervised Multitask Learners**\n",
    "\n",
    "\n",
    "### Model Description\n",
    "\n",
    "GPT-2 was released together with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford by Alec Radford et al at OpenAI. It is a development of [GPT](https://github.com/pytorch/hub/blob/master/huggingface_pytorch-pretrained-bert_gpt.md) introduced in [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf). It further asseses the impressive natural language generation abilities of large language models along with the ability to perform reasonably well on a diverse range of tasks in a zero-shot setting.\n",
    "\n",
    "Here are three models based on OpenAI's pre-trained weights along with the associated Tokenizer.\n",
    "It includes:\n",
    "- `gpt2Model`: raw OpenAI GPT-2 Transformer model (fully pre-trained)\n",
    "- `gpt2LMHeadModel`: OpenAI GPT-2 Transformer with the tied language modeling head on top (fully pre-trained)\n",
    "- `gpt2DoubleHeadsModel`: OpenAI GPT-2 Transformer with the tied language modeling head and a multiple choice classification head on top (OpenAI GPT-2 Transformer is pre-trained, the multiple choice classification head is only initialized and has to be trained)\n",
    "\n",
    "Note that two versions of GPT-2 are available for use: the small version (`gpt2`: English model with 12-layer, 768-hidden, 12-heads, 117M parameters) and the medium version (`gpt2-medium`: English model with 24-layer, 1024-hidden, 16-heads, 345M parameters).\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Unlike most other PyTorch Hub models, GPT requires a few additional Python packages to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install tqdm boto3 requests regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `python3` is recommended to use these models especially regarding the use of the tokenizer.\n",
    "\n",
    "### Example\n",
    "\n",
    "Here is an example on how to tokenize the text with `gpt2Tokenizer`, and then get the hidden states computed by `gpt2Model` or predict the next token using `gpt2LMHeadModel`. Finally, we showcase how to use `gpt2DoubleHeadsModel` to combine the language modeling head and a multiple choice classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, tokenize the input\n",
    "#############################\n",
    "import torch\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'gpt2Tokenizer', 'gpt2')\n",
    "\n",
    "#  Prepare tokenized input\n",
    "text_1 = \"Who was Jim Henson ? Jim Henson was a puppeteer\"\n",
    "text_2 = \"Who was Jim Henson ? Jim Henson was a mysterious young man\"\n",
    "tokenized_text_1 = tokenizer.tokenize(text_1)\n",
    "tokenized_text_2 = tokenizer.tokenize(text_2)\n",
    "indexed_tokens1 = tokenizer.convert_tokens_to_ids(tokenized_text_1)\n",
    "indexed_tokens2 = tokenizer.convert_tokens_to_ids(tokenized_text_2)\n",
    "tokens_tensor_1 = torch.tensor([indexed_tokens1])\n",
    "tokens_tensor_2 = torch.tensor([indexed_tokens2])\n",
    "\n",
    "\n",
    "### Get the hidden states computed by `gpt2Model`\n",
    "#################################################\n",
    "model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'gpt2Model', 'gpt2')\n",
    "model.eval()\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "# past can be used to reuse precomputed hidden state in a subsequent predictions\n",
    "with torch.no_grad():\n",
    "\thidden_states_1, past = model(tokens_tensor_1)\n",
    "\thidden_states_2, past = model(tokens_tensor_2, past=past)\n",
    "\n",
    "\n",
    "### Predict the next token using `gpt2LMHeadModel`\n",
    "##################################################\n",
    "lm_model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'gpt2LMHeadModel', 'gpt2')\n",
    "lm_model.eval()\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "\tpredictions_1, past = lm_model(tokens_tensor_1)\n",
    "\tpredictions_2, past = lm_model(tokens_tensor_2, past=past)\n",
    "\n",
    "# Get the predicted last token\n",
    "predicted_index = torch.argmax(predictions_2[0, -1, :]).item()\n",
    "predicted_token = tokenizer.decode([predicted_index])\n",
    "assert predicted_token == ' who'\n",
    "\n",
    "\n",
    "### Language modeling and multiple choice classification `gpt2DoubleHeadsModel`\n",
    "###############################################################################\n",
    "double_head_model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'gpt2DoubleHeadsModel', 'gpt2')\n",
    "double_head_model.eval() # Set the model to train mode if used for training\n",
    "\n",
    "tokens_tensor = torch.tensor([[indexed_tokens1, indexed_tokens2]])\n",
    "mc_token_ids = torch.LongTensor([[len(tokenized_text_1) - 1, len(tokenized_text_2) - 1]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    lm_logits, multiple_choice_logits, presents = double_head_model(tokens_tensor, mc_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    " - Paper: [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/)\n",
    " - [Blogpost from OpenAI](https://openai.com/blog/better-language-models/)\n",
    " - Initial repository (with detailed examples and documentation): [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
