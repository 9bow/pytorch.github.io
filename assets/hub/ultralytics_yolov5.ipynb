{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is optionally accelerated with a GPU runtime.\n",
    "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# YOLOv5\n",
    "\n",
    "*Author: Ultralytics LLC*\n",
    "\n",
    "**YOLOv5 in PyTorch > ONNX > CoreML > TFLite**\n",
    "\n",
    "_ | _\n",
    "- | -\n",
    "![alt](https://pytorch.org/assets/images/ultralytics_yolov5_img1.jpg) | ![alt](https://pytorch.org/assets/images/ultralytics_yolov5_img2.png)\n",
    "\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "Start from a working python environment with **Python>=3.8** and **PyTorch>=1.6** installed, as well as `pyyaml>=5.3` for reading YOLOv5 configuration files. To install PyTorch see [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/). To install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "$ pip install -U opencv-python pillow pyyaml tqdm  # install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description\n",
    "\n",
    "<img width=\"800\" alt=\"YOLOv5 Models\" src=\"https://user-images.githubusercontent.com/26833433/97808084-edfcb100-1c64-11eb-83eb-ffed43a0859f.png\">\n",
    "&nbsp;\n",
    "\n",
    "YOLOv5 is a family of compound-scaled object detection models trained on COCO 2017, and includes built-in functionality for Test Time Augmentation (TTA), Model Ensembling, Rectangular Inference, Hyperparameter Evolution.\n",
    "\n",
    "| Model | AP<sup>val</sup> | AP<sup>test</sup> | AP<sub>50</sub> | Speed<sub>GPU</sub> | FPS<sub>GPU</sub> || params | FLOPS |\n",
    "|---------- |------ |------ |------ | -------- | ------| ------ |------  |  :------: |\n",
    "| [YOLOv5s](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | 37.0     | 37.0     | 56.2     | **2.4ms** | **416** || 7.5M   | 13.2B\n",
    "| [YOLOv5m](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | 44.3     | 44.3     | 63.2     | 3.4ms     | 294     || 21.8M  | 39.4B\n",
    "| [YOLOv5l](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | 47.7     | 47.7     | 66.5     | 4.4ms     | 227     || 47.8M  | 88.1B\n",
    "| [YOLOv5x](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | 49.2 | 49.2 | 67.7 | 6.9ms     | 145     || 89.0M  | 166.4B\n",
    "| [YOLOv5x](https://github.com/ultralytics/yolov5/releases/tag/v3.0) + TTA|**50.8**| **50.8** | **68.9** | 25.5ms    | 39      || 89.0M  | 354.3B\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/26833433/90187293-6773ba00-dd6e-11ea-8f90-cd94afc0427f.png\" width=\"800\">  \n",
    "** GPU Speed measures end-to-end time per image averaged over 5000 COCO val2017 images using a V100 GPU with batch size 32, and includes image preprocessing, PyTorch FP16 inference, postprocessing and NMS. EfficientDet data from [google/automl](https://github.com/google/automl) at batch size 8.\n",
    "\n",
    "\n",
    "## Load YOLOv5 From PyTorch Hub\n",
    "\n",
    "To load YOLOv5 from PyTorch Hub for inference with PIL, OpenCV, Numpy or PyTorch inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).fuse().autoshape()  # for PIL/cv2/np inputs and NMS\n",
    "\n",
    "# Images\n",
    "for f in ['zidane.jpg', 'bus.jpg']:  # download 2 images\n",
    "    print(f'Downloading {f}...')\n",
    "    torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/' + f, f)\n",
    "img1 = Image.open('zidane.jpg')  # PIL image\n",
    "img2 = cv2.imread('bus.jpg')[:, :, ::-1]  # OpenCV image (BGR to RGB)\n",
    "imgs = [img1, img2]  # batched list of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs, size=640)  # includes NMS\n",
    "\n",
    "# Results\n",
    "results.show()  # .show() results, .save() jpgs, or .print() to screen\n",
    "\n",
    "# Data\n",
    "print(results.xyxy[0])  # print img1 predictions\n",
    "#          x1 (pixels)  y1 (pixels)  x2 (pixels)  y2 (pixels)   confidence        class\n",
    "# tensor([[7.47613e+02, 4.01168e+01, 1.14978e+03, 7.12016e+02, 8.71210e-01, 0.00000e+00],\n",
    "#         [1.17464e+02, 1.96875e+02, 1.00145e+03, 7.11802e+02, 8.08795e-01, 0.00000e+00],\n",
    "#         [4.23969e+02, 4.30401e+02, 5.16833e+02, 7.20000e+02, 7.77376e-01, 2.70000e+01],\n",
    "#         [9.81310e+02, 3.10712e+02, 1.03111e+03, 4.19273e+02, 2.86850e-01, 2.70000e+01]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/264818686.svg)](https://zenodo.org/badge/latestdoi/264818686)\n",
    "\n",
    "\n",
    "## Contact\n",
    "\n",
    "**Issues should be raised directly in the repository.** For business inquiries or professional support requests please visit [https://www.ultralytics.com](https://www.ultralytics.com) or email Glenn Jocher at [glenn.jocher@ultralytics.com](mailto:glenn.jocher@ultralytics.com). \n",
    "\n",
    "&nbsp;"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
