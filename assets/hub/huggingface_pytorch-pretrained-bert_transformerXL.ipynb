{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is optionally accelerated with a GPU runtime.\n",
    "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# Transformer-XL\n",
    "\n",
    "*Author: HuggingFace Team*\n",
    "\n",
    "**Attentive Language Models Beyond a Fixed-Length Context**\n",
    "\n",
    "\n",
    "### Model Description\n",
    "\n",
    "Transformer-XL was released together with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](http://arxiv.org/abs/1901.02860) by Zihang Dai, et al. This PyTorch implementation of Transformer-XL is an adaptation of the original [PyTorch implementation](https://github.com/kimiyoung/transformer-xl) which has been slightly modified to match the performances of the TensorFlow implementation and allow to re-use the pretrained weights.\n",
    "\n",
    "Here are two models based on the author's pre-trained weights along with the associated Tokenizer.\n",
    "It includes:\n",
    "- `transformerXLModel`: Transformer-XL model which outputs the last hidden state and memory cells (fully pre-trained)\n",
    "- `transformerXLLMHeadModel`: Transformer-XL with the tied adaptive softmax head on top for language modeling which outputs the logits/loss and memory cells (fully pre-trained)\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Unlike most other PyTorch Hub models, Transformer-XL requires a few additional Python packages to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install tqdm boto3 requests regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here is an example on how to tokenize the text with `transformerXLTokenizer`, and then get the hidden states computed by `transformerXLModel` or predict the next token using `transformerXLLMHeadModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, tokenize the input\n",
    "#############################\n",
    "import torch\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'transformerXLTokenizer', 'transfo-xl-wt103')\n",
    "\n",
    "#  Prepare tokenized input\n",
    "text_1 = \"Who was Jim Henson ?\"\n",
    "text_2 = \"Jim Henson was a puppeteer\"\n",
    "tokenized_text_1 = tokenizer.tokenize(text_1)\n",
    "tokenized_text_2 = tokenizer.tokenize(text_2)\n",
    "indexed_tokens_1 = tokenizer.convert_tokens_to_ids(tokenized_text_1)\n",
    "indexed_tokens_2 = tokenizer.convert_tokens_to_ids(tokenized_text_2)\n",
    "tokens_tensor_1 = torch.tensor([indexed_tokens_1])\n",
    "tokens_tensor_2 = torch.tensor([indexed_tokens_2])\n",
    "\n",
    "### Get the hidden states computed by `transformerXLModel`\n",
    "##########################################################\n",
    "model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'transformerXLModel', 'transfo-xl-wt103')\n",
    "model.eval()\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "# past can be used to reuse precomputed hidden state in a subsequent predictions\n",
    "with torch.no_grad():\n",
    "\thidden_states_1, mems_1 = model(tokens_tensor_1)\n",
    "\thidden_states_2, mems_2 = model(tokens_tensor_2, mems=mems_1)\n",
    "\n",
    "### Predict the next token using `transformerXLLMHeadModel`\n",
    "###########################################################\n",
    "lm_model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'transformerXLLMHeadModel', 'transfo-xl-wt103')\n",
    "lm_model.eval()\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "\tpredictions_1, mems_1 = lm_model(tokens_tensor_1)\n",
    "\tpredictions_2, mems_2 = lm_model(tokens_tensor_2, mems=mems_1)\n",
    "\n",
    "# Get the predicted last token\n",
    "predicted_index = torch.argmax(predictions_2[0, -1, :]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'who'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    " - Paper: [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](http://arxiv.org/abs/1901.02860)\n",
    " - Initial repository (with detailed examples and documentation): [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    " - Original author's [implementation](https://github.com/kimiyoung/transformer-xl)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
