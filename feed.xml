<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pytorch.org/" rel="alternate" type="text/html" /><updated>2019-07-10T08:25:01-07:00</updated><id>https://pytorch.org/</id><title type="html">PyTorch Website</title><subtitle>Scientific Computing...</subtitle><author><name>Facebook</name></author><entry><title type="html">Towards Reproducible Research with PyTorch Hub</title><link href="https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/" rel="alternate" type="text/html" title="Towards Reproducible Research with PyTorch Hub" /><published>2019-06-10T00:00:00-07:00</published><updated>2019-06-10T00:00:00-07:00</updated><id>https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub</id><content type="html" xml:base="https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/">&lt;p&gt;Reproducibility is an essential requirement for many fields of research including those based on machine learning techniques. However, many machine learning publications are either not reproducible or are difficult to reproduce. With the continued growth in the number of research publications, including tens of thousands of papers now hosted on arXiv and submissions to conferences at an all time high, research reproducibility is more important than ever. While many of these publications are accompanied by code as well as trained models which is helpful but still leaves a number of steps for users to figure out for themselves.&lt;/p&gt;

&lt;p&gt;We are excited to announce the availability of PyTorch Hub, a simple API and workflow that provides the basic building blocks for improving machine learning research reproducibility. PyTorch Hub consists of a pre-trained model repository designed specifically to facilitate research reproducibility and enable new research. It also has built-in support for &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Colab&lt;/a&gt;, integration with &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;&lt;em&gt;Papers With Code&lt;/em&gt;&lt;/a&gt; and currently contains a broad set of models that include Classification and Segmentation, Generative, Transformers, etc.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/hub-blog-header-1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;owner-publishing-models&quot;&gt;[Owner] Publishing models&lt;/h2&gt;

&lt;p&gt;PyTorch Hub supports the publication of pre-trained models (model definitions and pre-trained weights) to a GitHub repository by adding a simple &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt; file.
This provides an enumeration of which models are to be supported and a list of dependencies needed to run the models.
Examples can be found in the &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/hubconf.py&quot;&gt;torchvision&lt;/a&gt;, &lt;a href=&quot;https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/hubconf.py&quot;&gt;huggingface-bert&lt;/a&gt; and &lt;a href=&quot;https://github.com/facebookresearch/pytorch_GAN_zoo&quot;&gt;gan-model-zoo&lt;/a&gt; repositories.&lt;/p&gt;

&lt;p&gt;Let us look at the simplest case: &lt;code class=&quot;highlighter-rouge&quot;&gt;torchvision&lt;/code&gt;’s &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Optional list of dependencies required by the package&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dependencies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'torch'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.alexnet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alexnet&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.densenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet121&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet169&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet201&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet161&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.inception&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inception_v3&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.resnet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet34&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet152&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;\
&lt;span class=&quot;n&quot;&gt;resnext50_32x4d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnext101_32x8d&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.squeezenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;squeezenet1_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;squeezenet1_1&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.vgg&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg11_bn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg13_bn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg16_bn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg19_bn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.segmentation&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fcn_resnet101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deeplabv3_resnet101&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.googlenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;googlenet&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.shufflenetv2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shufflenet_v2_x0_5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shufflenet_v2_x1_0&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.mobilenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mobilenet_v2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;highlighter-rouge&quot;&gt;torchvision&lt;/code&gt;, the models have the following properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Each model file can function and be executed independently&lt;/li&gt;
  &lt;li&gt;They dont require any package other than PyTorch (encoded in &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt; as &lt;code class=&quot;highlighter-rouge&quot;&gt;dependencies['torch']&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;They dont need separate entry-points, because the models when created, work seamlessly out of the box&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Minimizing package dependencies reduces the friction for users to load your model for immediate experimentation.&lt;/p&gt;

&lt;p&gt;A more involved example is HuggingFace’s BERT models. Here is their &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dependencies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'torch'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'tqdm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'boto3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'requests'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'regex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;hubconfs.bert_hubconf&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForNextSentencePrediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForPreTraining&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForMaskedLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForSequenceClassification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForMultipleChoice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForQuestionAnswering&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForTokenClassification&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each model then requires an entrypoint to be created. Here is a code snippet to specify an entrypoint of the &lt;code class=&quot;highlighter-rouge&quot;&gt;bertForMaskedLM&lt;/code&gt; model, which returns the pre-trained model weights.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bertForMaskedLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    BertForMaskedLM includes the BertModel Transformer followed by the
    pre-trained masked language modeling head.
    Example:
      ...
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForMaskedLM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These entry-points can serve as wrappers around complex model factories. They can give a clean and consistent help docstring, have logic to support downloading of pretrained weights (for example via &lt;code class=&quot;highlighter-rouge&quot;&gt;pretrained=True&lt;/code&gt;) or have additional hub-specific functionality such as visualization.&lt;/p&gt;

&lt;p&gt;With a &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt; in place, you can send a pull request based on the template &lt;a href=&quot;https://github.com/pytorch/hub/blob/master/docs/template.md&quot;&gt;here&lt;/a&gt;.
Our goal is to curate high-quality, easily-reproducible, maximally-beneficial models for research reproducibility.
Hence, we may work with you to refine your pull request and in some cases reject some low-quality models to be published.
Once we accept your pull request, your model will soon appear on &lt;a href=&quot;https://pytorch.org/hub&quot;&gt;Pytorch hub webpage&lt;/a&gt; for all users to explore.&lt;/p&gt;

&lt;h2 id=&quot;user-workflow&quot;&gt;[User] Workflow&lt;/h2&gt;

&lt;p&gt;As a user, PyTorch Hub allows you to follow a few simple steps and do things like: 1) explore available models; 2) load a model; and 3) understand what methods are available for any given model. Let’s walk through some examples of each.&lt;/p&gt;

&lt;h3 id=&quot;explore-available-entrypoints&quot;&gt;Explore available entrypoints.&lt;/h3&gt;

&lt;p&gt;Users can list all available entrypoints in a repo using the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.hub.list()&lt;/code&gt; API.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pytorch/vision'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'alexnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'deeplabv3_resnet101'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'densenet121'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'vgg16'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'vgg16_bn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'vgg19'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;s&quot;&gt;'vgg19_bn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that PyTorch Hub also allows auxillary entrypoints (other than pretrained models), e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;bertTokenizer&lt;/code&gt; for preprocessing in the &lt;a href=&quot;https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/&quot;&gt;BERT&lt;/a&gt; models, to make the user workflow smoother.&lt;/p&gt;

&lt;h3 id=&quot;load-a-model&quot;&gt;Load a model&lt;/h3&gt;

&lt;p&gt;Now that we know which models are available in the Hub, users can load a model entrypoint using the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.hub.load()&lt;/code&gt; API. This only requires a single command without the need to install a wheel. In addition the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.hub.help()&lt;/code&gt; API can provide useful information about how to instantiate the model.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pytorch/vision'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'deeplabv3_resnet101'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pytorch/vision'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'deeplabv3_resnet101'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is also common that repo owners will want to continually add bug fixes or performance improvements. PyTorch Hub makes it super simple for users to get the latest update by calling:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;force_reload&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We believe this will help to alleviate the burden of repetitive package releases by repo owners and instead allow them to focus more on their research.
It also ensures that, as a user, you are getting the freshest available models.&lt;/p&gt;

&lt;p&gt;On the contrary, stability is important for users. Hence, some model owners serve them from a specificed branch or tag, rather than the &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; branch, to ensure stability of the code.
For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch_GAN_zoo&lt;/code&gt; serves them from the &lt;code class=&quot;highlighter-rouge&quot;&gt;hub&lt;/code&gt; branch:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'facebookresearch/pytorch_GAN_zoo:hub'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'DCGAN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;useGPU&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the &lt;code class=&quot;highlighter-rouge&quot;&gt;*args&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;**kwargs&lt;/code&gt; passed to &lt;code class=&quot;highlighter-rouge&quot;&gt;hub.load()&lt;/code&gt; are used to &lt;em&gt;instantiate&lt;/em&gt; a model. In the above example, &lt;code class=&quot;highlighter-rouge&quot;&gt;pretrained=True&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;useGPU=False&lt;/code&gt; are given to the model’s entrypoint.&lt;/p&gt;

&lt;h3 id=&quot;explore-a-loaded-model&quot;&gt;Explore a loaded model&lt;/h3&gt;

&lt;p&gt;Once you have a model from PyTorch Hub loaded, you can use the following workflow to find out the available methods that are supported as well as understand better what arguments are requires to run it.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dir(model)&lt;/code&gt; to see all available methods of the model. Let’s take a look at &lt;code class=&quot;highlighter-rouge&quot;&gt;bertForMaskedLM&lt;/code&gt;’s available methods.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'forward'&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'to'&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'state_dict'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;help(model.forward)&lt;/code&gt; provides a view into what arguments are required to make your loaded model run&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Help&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch_pretrained_bert&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modeling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_type_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;masked_lm_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Have a closer look at the &lt;a href=&quot;https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/&quot;&gt;BERT&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/&quot;&gt;DeepLabV3&lt;/a&gt; pages, where you can see how these models can be used once loaded.&lt;/p&gt;

&lt;h3 id=&quot;other-ways-to-explore&quot;&gt;Other ways to explore&lt;/h3&gt;

&lt;p&gt;Models available in PyTorch Hub also support both &lt;a href=&quot;https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/facebookresearch_pytorch-gan-zoo_pgan.ipynb&quot;&gt;Colab&lt;/a&gt; and are directly linked on &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;Papers With Code&lt;/a&gt; and you can get started with a single click. &lt;a href=&quot;https://paperswithcode.com/paper/densely-connected-convolutional-networks&quot;&gt;Here&lt;/a&gt; is a good example to get started with (shown below).&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/hub-blog-pwc.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;additional-resources&quot;&gt;Additional resources:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch Hub API documentation can be found &lt;a href=&quot;https://pytorch.org/docs/stable/hub.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Submit a model &lt;a href=&quot;https://github.com/pytorch/hub&quot;&gt;here&lt;/a&gt; for publication in PyTorch Hub.&lt;/li&gt;
  &lt;li&gt;Go to &lt;a href=&quot;https://pytorch.org/hub&quot;&gt;https://pytorch.org/hub&lt;/a&gt; to learn more about the available models.&lt;/li&gt;
  &lt;li&gt;Look for more models to come on &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;paperswithcode.com&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A BIG thanks to the folks at HuggingFace, the PapersWithCode team, fast.ai and Nvidia as well as Morgane Riviere (FAIR Paris) and lots of others for helping bootstrap this effort!!&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;

&lt;h2 id=&quot;faq&quot;&gt;FAQ:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q: If we would like to contribute a model that is already in the Hub but perhaps mine has better accuracy, should I still contribute?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: Yes!! A next step for Hub is to implement an upvote/downvote system to surface the best models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: Who hosts the model weights for PyTorch Hub?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: You, as the contributor, are responsible to host the model weights. You can host your model in your favorite cloud storage or, if it fits within the limits, on GitHub. If it is not within your means to host the weights, check with us via opening an issue on the hub repository.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: What if my model is trained on private data? Should I still contribute this model?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: No! PyTorch Hub is centered around open research and that extends to the usage of open datasets to train these models on. If a pull request for a proprietary model is submitted, we will kindly ask that you resubmit a model trained on something open and available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: Where are my downloaded models saved?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: We follow the &lt;a href=&quot;https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html&quot;&gt;XDG Base Directory Specification&lt;/a&gt; and adhere to common standards around cached files and directories.&lt;/p&gt;

&lt;p&gt;The locations are used in the order of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Calling &lt;code class=&quot;highlighter-rouge&quot;&gt;hub.set_dir(&amp;lt;PATH_TO_HUB_DIR&amp;gt;)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$TORCH_HOME/hub&lt;/code&gt;, if environment variable &lt;code class=&quot;highlighter-rouge&quot;&gt;TORCH_HOME&lt;/code&gt; is set.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$XDG_CACHE_HOME/torch/hub&lt;/code&gt;, if environment variable &lt;code class=&quot;highlighter-rouge&quot;&gt;XDG_CACHE_HOME&lt;/code&gt; is set.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~/.cache/torch/hub&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Reproducibility is an essential requirement for many fields of research including those based on machine learning techniques. However, many machine learning publications are either not reproducible or are difficult to reproduce. With the continued growth in the number of research publications, including tens of thousands of papers now hosted on arXiv and submissions to conferences at an all time high, research reproducibility is more important than ever. While many of these publications are accompanied by code as well as trained models which is helpful but still leaves a number of steps for users to figure out for themselves.</summary></entry><entry><title type="html">torchvision 0.3: segmentation, detection models, new datasets and more..</title><link href="https://pytorch.org/blog/torchvision03/" rel="alternate" type="text/html" title="torchvision 0.3: segmentation, detection models, new datasets and more.." /><published>2019-05-22T00:00:00-07:00</published><updated>2019-05-22T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchvision03</id><content type="html" xml:base="https://pytorch.org/blog/torchvision03/">&lt;p&gt;PyTorch domain libraries like torchvision provide convenient access to common datasets and models that can be used to quickly create a state-of-the-art baseline. Moreover, they also provide common abstractions to reduce boilerplate code that users might have to otherwise repeatedly write. The torchvision 0.3 release brings several new features including models for semantic segmentation, object detection, instance segmentation, and person keypoint detection, as well as custom C++ / CUDA ops specific to computer vision.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/torchvision_0.3_headline.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;new-features-include&quot;&gt;New features include:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Reference training / evaluation scripts:&lt;/strong&gt; torchvision now provides, under the references/ folder, scripts for training and evaluation of the following tasks: classification, semantic segmentation, object detection, instance segmentation and person keypoint detection. These serve as a log of how to train a specific model and provide baseline training and evaluation scripts to quickly bootstrap research.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;torchvision ops:&lt;/strong&gt; torchvision now contains custom C++ / CUDA operators. Those operators are specific to computer vision, and make it easier to build object detection models. These operators currently do not support PyTorch script mode, but support for it is planned for in the next release. Some of the ops supported include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;roi_pool (and the module version RoIPool)&lt;/li&gt;
  &lt;li&gt;roi_align (and the module version RoIAlign)&lt;/li&gt;
  &lt;li&gt;nms, for non-maximum suppression of bounding boxes&lt;/li&gt;
  &lt;li&gt;box_iou, for computing the intersection over union metric between two sets of bounding boxes&lt;/li&gt;
  &lt;li&gt;box_area, for computing the area of a set of bounding boxes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are a few examples on using torchvision ops:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# create 10 random boxes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# they need to be in [x0, y0, x1, y1] format&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# create a random image&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# extract regions in `image` defined in `boxes`, rescaling&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# them to have a size of 3x3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pooled_regions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roi_align&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# check the size&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pooled_regions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# torch.Size([10, 3, 3, 3])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# or compute the intersection over union between&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# all pairs of boxes&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;box_iou&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# torch.Size([10, 10])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;New models and datasets:&lt;/strong&gt; torchvision now adds support for object detection, instance segmentation and person keypoint detection models. In addition, several popular datasets have been added. Note: The API is currently experimental and might change in future versions of torchvision. New models include:&lt;/p&gt;

&lt;h3 id=&quot;segmentation-models&quot;&gt;Segmentation Models&lt;/h3&gt;

&lt;p&gt;The 0.3 release also contains models for dense pixelwise prediction on images.
It adds FCN and DeepLabV3 segmentation models, using a ResNet50 and ResNet101 backbones.
Pre-trained weights for ResNet101 backbone are available, and have been trained on a subset of COCO train2017, which contains the same 20 categories as those from Pascal VOC.&lt;/p&gt;

&lt;p&gt;The pre-trained models give the following results on the subset of COCO val2017 which contain the same 20 categories as those present in Pascal VOC:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;mean IoU&lt;/th&gt;
      &lt;th&gt;global pixelwise acc&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN ResNet101&lt;/td&gt;
      &lt;td&gt;63.7&lt;/td&gt;
      &lt;td&gt;91.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 ResNet101&lt;/td&gt;
      &lt;td&gt;67.4&lt;/td&gt;
      &lt;td&gt;92.4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;detection-models&quot;&gt;Detection Models&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;box AP&lt;/th&gt;
      &lt;th&gt;mask AP&lt;/th&gt;
      &lt;th&gt;keypoint AP&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN ResNet-50 FPN trained on COCO&lt;/td&gt;
      &lt;td&gt;37.0&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Mask R-CNN ResNet-50 FPN trained on COCO&lt;/td&gt;
      &lt;td&gt;37.9&lt;/td&gt;
      &lt;td&gt;34.6&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Keypoint R-CNN ResNet-50 FPN trained on COCO&lt;/td&gt;
      &lt;td&gt;54.6&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;65.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The implementations of the models for object detection, instance segmentation and keypoint detection are fast, specially during training.&lt;/p&gt;

&lt;p&gt;In the following table, we use 8 V100 GPUs, with CUDA 10.0 and CUDNN 7.4 to report the results. During training, we use a batch size of 2 per GPU, and during testing a batch size of 1 is used.&lt;/p&gt;

&lt;p&gt;For test time, we report the time for the model evaluation and post-processing (including mask pasting in image), but not the time for computing the precision-recall.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;train time (s / it)&lt;/th&gt;
      &lt;th&gt;test time (s / it)&lt;/th&gt;
      &lt;th&gt;memory (GB)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td&gt;0.2288&lt;/td&gt;
      &lt;td&gt;0.0590&lt;/td&gt;
      &lt;td&gt;5.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Mask R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td&gt;0.2728&lt;/td&gt;
      &lt;td&gt;0.0903&lt;/td&gt;
      &lt;td&gt;5.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Keypoint R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td&gt;0.3789&lt;/td&gt;
      &lt;td&gt;0.1242&lt;/td&gt;
      &lt;td&gt;6.8&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You can load and use pre-trained detection and segmentation models with a few lines of code&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maskrcnn_resnet50_fpn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# set it to evaluation mode, as the model behaves differently&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# during training and during evaluation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PIL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/path/to/an/image.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;image_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# pass a list of (potentially different sized) tensors&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# to the model, in 0-1 range. The model will take care of&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# batching them together and normalizing&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# output is a list of dict, containing the postprocessed predictions&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;classification-models&quot;&gt;Classification Models&lt;/h3&gt;

&lt;p&gt;The following classification models were added:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GoogLeNet (Inception v1)&lt;/li&gt;
  &lt;li&gt;MobileNet V2&lt;/li&gt;
  &lt;li&gt;ShuffleNet v2&lt;/li&gt;
  &lt;li&gt;ResNeXt-50 32x4d and ResNeXt-101 32x8d&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;The following datasets were added:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Caltech101, Caltech256, and CelebA&lt;/li&gt;
  &lt;li&gt;ImageNet dataset (improving on ImageFolder, provides class-strings)&lt;/li&gt;
  &lt;li&gt;Semantic Boundaries Dataset&lt;/li&gt;
  &lt;li&gt;VisionDataset as a base class for all datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, we’ve added more image transforms, general improvements and bug fixes, as well as improved documentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;See the full release notes &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt; as well as this getting started tutorial &lt;a href=&quot;https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb&quot;&gt;on Google Colab here&lt;/a&gt;, which describes how to fine tune your own instance segmentation model on a custom dataset.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Francisco Massa</name></author><summary type="html">PyTorch domain libraries like torchvision provide convenient access to common datasets and models that can be used to quickly create a state-of-the-art baseline. Moreover, they also provide common abstractions to reduce boilerplate code that users might have to otherwise repeatedly write. The torchvision 0.3 release brings several new features including models for semantic segmentation, object detection, instance segmentation, and person keypoint detection, as well as custom C++ / CUDA ops specific to computer vision.</summary></entry><entry><title type="html">Model Serving in PyTorch</title><link href="https://pytorch.org/blog/model-serving-in-pyorch/" rel="alternate" type="text/html" title="Model Serving in PyTorch" /><published>2019-05-08T00:00:00-07:00</published><updated>2019-05-08T00:00:00-07:00</updated><id>https://pytorch.org/blog/model-serving-in-pyorch</id><content type="html" xml:base="https://pytorch.org/blog/model-serving-in-pyorch/">&lt;p&gt;PyTorch has seen a lot of adoption in research, but people can get confused about how well PyTorch models can be taken into production. This blog post is meant to clear up any confusion people might have about the road to production in PyTorch.
Usually when people talk about taking a model “to production,” they usually mean performing &lt;strong&gt;inference&lt;/strong&gt;, sometimes called model evaluation or prediction or serving. At the level of a function call, in PyTorch, inference looks something like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In Python
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;module(input)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In traced modules
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;module(input)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In C++
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;at::Tensor output = module-&amp;gt;forward(inputs).toTensor();&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since we at Facebook perform inference operations using PyTorch hundreds of trillions of times per day, we’ve done a lot to make sure that inference runs as efficiently as possible.&lt;/p&gt;

&lt;h2 id=&quot;serving-strategies&quot;&gt;Serving Strategies&lt;/h2&gt;

&lt;p&gt;That zoomed-in view of how you use models in inference isn’t usually the whole story, though. In a real world machine learning system, you often need to do more than just run a single inference operation in the REPL or Jupyter notebook. Instead, you usually need to integrate your model into a larger application in some way. Depending on what you need to do, you can usually take one of the following approaches.&lt;/p&gt;

&lt;h3 id=&quot;direct-embedding&quot;&gt;Direct embedding&lt;/h3&gt;

&lt;p&gt;In application settings like mobile, we often just directly call the model as part of a larger program. This isn’t just for apps; usually this is how robotics and dedicated devices work as well. At a code-level, the call to the model is exactly the same as what is shown above in the section about inference shown above. A key concern is often that a Python interpreter is not present in such environments, which is why PyTorch allows you to call your models from C++ and ship a model without the need for a Python runtime.&lt;/p&gt;

&lt;h3 id=&quot;model-microservices&quot;&gt;Model microservices&lt;/h3&gt;

&lt;p&gt;If you’re using your model in a server side context and you’re managing multiple models, you might choose to treat each individual model (or each individual model version) as a separate service, usually using some sort of packaging mechanism like a Docker container. Then that service is often made network accessible via some sort of service, either using JSON over HTTP or an RPC technology like gRPC. The key characteristic of this approach is that you’re defining a service with a single endpoint that just calls your model. Then you do do all of your model management (promotion, rollback, etc.) via whatever system you already use to manage your services (e.g. kubernetes, ECS).&lt;/p&gt;

&lt;h3 id=&quot;model-servers&quot;&gt;Model servers&lt;/h3&gt;

&lt;p&gt;An additional possible solution is to use a model server. This is an application built to manage and serve models. It allows you to upload multiple models and get distinct prediction endpoints for each of them. Typically such systems include a number of other features to help solve more of the whole problem of managing and serving models. This can include things like metrics, visualization, data pre-processing, and more. Even something as simple as having a system for automatically versioning models can make building important features like model rollbacks much easier.&lt;/p&gt;

&lt;h3 id=&quot;evolving-patterns&quot;&gt;Evolving Patterns&lt;/h3&gt;

&lt;p&gt;The above is a somewhat arbitrary breakdown of different approaches based on a snapshot in time. Design patterns are still evolving. Recently, model server designs have started to adopt more of the technologies of general service infrastructure such as Docker containers and kubernetes, so many model servers have started to share properties of the model microservice design discussed above. For a deeper dive into the general concepts of model server designs, you can check out my &lt;a href=&quot;https://www.manning.com/books/machine-learning-systems&quot;&gt;book on machine learning systems&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;serving-pytorch-models&quot;&gt;Serving PyTorch Models&lt;/h2&gt;

&lt;p&gt;So, if you’re a PyTorch user, what should you use if you want to take your models to production?&lt;/p&gt;

&lt;p&gt;If you’re on mobile or working on an embedded system like a robot, direct embedding in your application is often the right choice. 
For mobile specifically, your use case might be served by the ONNX export functionality.
Note that ONNX, by its very nature, has limitations and doesn’t support all of the functionality provided by the larger PyTorch project.
You can check out &lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html&quot;&gt;this tutorial&lt;/a&gt; on deploying PyTorch models to mobile using ONNX to see if this path might suit your use case. 
That said, we’ve heard that there’s a lot more that PyTorch users want to do on mobile, so look for more mobile-specific functionality in PyTorch in the future.
For other embedded systems, like robots, running &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;inference on a PyTorch model from the C++ API&lt;/a&gt; could be the right solution.&lt;/p&gt;

&lt;p&gt;If you can’t use the cloud or prefer to manage all services using the same technology, you can follow &lt;a href=&quot;https://medium.com/datadriveninvestor/deploy-your-pytorch-model-to-production-f69460192217&quot;&gt;this example&lt;/a&gt; to build a simple model microservice using the Flask web framework.&lt;/p&gt;

&lt;p&gt;If you want to manage multiple models within a non-cloud service solution, there are teams developing PyTorch support in model servers like &lt;a href=&quot;https://mlflow.org/&quot;&gt;MLFlow&lt;/a&gt;, &lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;Kubeflow&lt;/a&gt;, and &lt;a href=&quot;https://oss.redislabs.com/redisai/&quot;&gt;RedisAI.&lt;/a&gt; We’re excited to see innovation from multiple teams building OSS model servers, and we’ll continue to highlight innovation in the PyTorch ecosystem in the future.&lt;/p&gt;

&lt;p&gt;If you can use the cloud for your application, there are several great choices for working with models in the cloud. For AWS Sagemaker, you can start find a guide to &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/pytorch.html&quot;&gt;all of the resources from AWS for working with PyTorch&lt;/a&gt;, including docs on how to use the &lt;a href=&quot;https://sagemaker.readthedocs.io/en/stable/using_pytorch.html&quot;&gt;Sagemaker Python SDK&lt;/a&gt;. You can also see &lt;a href=&quot;https://youtu.be/5h1Ot2dPi2E&quot;&gt;some&lt;/a&gt; &lt;a href=&quot;https://youtu.be/qc5ZikKw9_w&quot;&gt;talks&lt;/a&gt; we’ve given on using PyTorch on Sagemaker. Finally, if you happen to be using PyTorch via FastAI, then they’ve written a &lt;a href=&quot;https://course.fast.ai/deployment_amzn_sagemaker.html&quot;&gt;really simple guide&lt;/a&gt; to getting up and running on Sagemaker.&lt;/p&gt;

&lt;p&gt;The story is similar across other major clouds. On Google Cloud, you can follow &lt;a href=&quot;https://cloud.google.com/deep-learning-vm/docs/pytorch_start_instance&quot;&gt;these instructions&lt;/a&gt; to get access to a Deep Learning VM with PyTorch pre-installed. On Microsoft Azure, you have a number of ways to get started from &lt;a href=&quot;https://azure.microsoft.com/en-us/services/machine-learning-service/&quot;&gt;Azure Machine Learning Service&lt;/a&gt; to &lt;a href=&quot;https://notebooks.azure.com/pytorch/projects/tutorials&quot;&gt;Azure Notebooks&lt;/a&gt; showing how to use PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;your-models&quot;&gt;Your Models&lt;/h2&gt;

&lt;p&gt;Whichever approach you take to bringing your PyTorch models to production, we want to support you and enable your success. Do you love one of the options above? Are you having difficulty with that one crucial feature you can’t find support for? We’d love to discuss more on the &lt;a href=&quot;https://discuss.pytorch.org/c/deployment&quot;&gt;deployment category&lt;/a&gt; on the PyTorch Discuss forums. We’d love to help, and where you’re seeing success, amplify your story.&lt;/p&gt;</content><author><name>Jeff Smith</name></author><summary type="html">PyTorch has seen a lot of adoption in research, but people can get confused about how well PyTorch models can be taken into production. This blog post is meant to clear up any confusion people might have about the road to production in PyTorch. Usually when people talk about taking a model “to production,” they usually mean performing inference, sometimes called model evaluation or prediction or serving. At the level of a function call, in PyTorch, inference looks something like this:</summary></entry><entry><title type="html">Optimizing CUDA Recurrent Neural Networks with TorchScript</title><link href="https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/" rel="alternate" type="text/html" title="Optimizing CUDA Recurrent Neural Networks with TorchScript" /><published>2019-05-01T06:00:00-07:00</published><updated>2019-05-01T06:00:00-07:00</updated><id>https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript</id><content type="html" xml:base="https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/">&lt;p&gt;This week, we officially released PyTorch 1.1, a large feature update to PyTorch 1.0. One of the new features we’ve added is better support for fast, custom Recurrent Neural Networks (fastrnns) with TorchScript (the PyTorch JIT) (https://pytorch.org/docs/stable/jit.html).&lt;/p&gt;

&lt;p&gt;RNNs are popular models that have shown good performance on a variety of NLP tasks that come in different shapes and sizes. PyTorch implements a number of the most popular ones, the &lt;a href=&quot;https://pytorch.org/docs/master/nn.html?highlight=rnn#torch.nn.RNN&quot;&gt;Elman RNN&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/docs/master/nn.html?highlight=gru#torch.nn.GRU&quot;&gt;GRU&lt;/a&gt;, and &lt;a href=&quot;https://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM&quot;&gt;LSTM&lt;/a&gt; as well as multi-layered and bidirectional variants.&lt;/p&gt;

&lt;p&gt;However, many users want to implement their own custom RNNs, taking ideas from recent literature. Applying &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;Layer Normalization&lt;/a&gt; to LSTMs is one such use case. Because the PyTorch CUDA LSTM implementation uses a fused kernel, it is difficult to insert normalizations or even modify the base LSTM implementation. Many users have turned to writing custom implementations using standard PyTorch operators, but such code suffers from high overhead: most PyTorch operations launch at least one kernel on the GPU and RNNs generally run many operations due to their recurrent nature. However, we can apply TorchScript to fuse operations and optimize our code automatically, launching fewer, more optimized kernels on the GPU.&lt;/p&gt;

&lt;p&gt;Our goal is for users to be able to write fast, custom RNNs in TorchScript without writing specialized CUDA kernels to achieve similar performance. In this post, we’ll provide a tutorial for how to write your own fast RNNs with TorchScript. To better understand the optimizations TorchScript applies, we’ll examine how those work on a standard LSTM implementation but most of the optimizations can be applied to general RNNs.&lt;/p&gt;

&lt;h2 id=&quot;writing-custom-rnns&quot;&gt;Writing custom RNNs&lt;/h2&gt;

&lt;p&gt;To get started, you can use &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py&quot;&gt;this file&lt;/a&gt; as a template to write your own custom RNNs.&lt;/p&gt;

&lt;p&gt;We are constantly improving our infrastructure on trying to make the performance better. If you want to gain the speed/optimizations that TorchScript currently provides (like operator fusion, batch matrix multiplications, etc.), here are some guidelines to follow. The next section explains the optimizations in depth.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;If the customized operations are all element-wise, that’s great because you can get the benefits of the PyTorch JIT’s operator fusion automatically!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you have more complex operations (e.g. reduce ops mixed with element-wise ops), consider grouping the reduce operations and element-wise ops separately in order to fuse the element-wise operations into a single fusion group.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you want to know about what has been fused in your custom RNN, you can inspect the operation’s optimized graph by using &lt;code class=&quot;highlighter-rouge&quot;&gt;graph_for&lt;/code&gt; . Using &lt;code class=&quot;highlighter-rouge&quot;&gt;LSTMCell&lt;/code&gt; as an example:&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;c&quot;&gt;# get inputs and states for LSTMCell&lt;/span&gt;

 &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_lstm_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

 &lt;span class=&quot;c&quot;&gt;# instantiate a ScriptModule&lt;/span&gt;

 &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSTMCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;c&quot;&gt;# print the optimized graph using graph_for&lt;/span&gt;

 &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;This will generate the optimized TorchScript graph (a.k.a PyTorch JIT IR) for the specialized inputs that you provides:&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; graph(%x : Float(*, *),
         %hx : Float(*, *),
         %cx : Float(*, *),
         %w_ih : Float(*, *),
         %w_hh : Float(*, *),
         %b_ih : Float(*),
         %b_hh : Float(*)):
     %hy : Float(*, *), %cy : Float(*, *) = prim::DifferentiableGraph_0(%cx, %b_hh, %b_ih, %hx, %w_hh, %x, %w_ih)
     %30 : (Float(*, *), Float(*, *)) = prim::TupleConstruct(%hy, %cy)
     return (%30)
     with prim::DifferentiableGraph_0 = graph(%13 : Float(*, *),
         %29 : Float(*),
         %33 : Float(*),
         %40 : Float(*, *),
         %43 : Float(*, *),
         %45 : Float(*, *),
         %48 : Float(*, *)):
     %49 : Float(*, *) = aten::t(%48)
     %47 : Float(*, *) = aten::mm(%45, %49)
     %44 : Float(*, *) = aten::t(%43)
     %42 : Float(*, *) = aten::mm(%40, %44)
     ...some broadcast sizes operations...
     %hy : Float(*, *), %287 : Float(*, *), %cy : Float(*, *), %outgate.1 : Float(*, *), %cellgate.1 : Float(*, *), %forgetgate.1 : Float(*, *), %ingate.1 : Float(*, *) = prim::FusionGroup_0(%13, %346, %345, %344, %343)
     ...some broadcast sizes operations...
     return (%hy, %cy, %49, %44, %196, %199, %340, %192, %325, %185, %ingate.1, %forgetgate.1, %cellgate.1, %outgate.1, %395, %396, %287)
     with prim::FusionGroup_0 = graph(%13 : Float(*, *),
         %71 : Tensor,
         %76 : Tensor,
         %81 : Tensor,
         %86 : Tensor):
     ...some chunks, constants, and add operations...
     %ingate.1 : Float(*, *) = aten::sigmoid(%38)
     %forgetgate.1 : Float(*, *) = aten::sigmoid(%34)
     %cellgate.1 : Float(*, *) = aten::tanh(%30)
     %outgate.1 : Float(*, *) = aten::sigmoid(%26)
     %14 : Float(*, *) = aten::mul(%forgetgate.1, %13)
     %11 : Float(*, *) = aten::mul(%ingate.1, %cellgate.1)
     %cy : Float(*, *) = aten::add(%14, %11, %69)
     %4 : Float(*, *) = aten::tanh(%cy)
     %hy : Float(*, *) = aten::mul(%outgate.1, %4)
     return (%hy, %4, %cy, %outgate.1, %cellgate.1, %forgetgate.1, %ingate.1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From the above graph we can see that it has a &lt;code class=&quot;highlighter-rouge&quot;&gt;prim::FusionGroup_0&lt;/code&gt; subgraph that is fusing all element-wise operations in LSTMCell (transpose and matrix multiplication are not element-wise ops). Some graph nodes might be hard to understand in the first place but we will explain some of them in the optimization section, we also omitted some long verbose operators in this post that is there just for correctness.&lt;/p&gt;

&lt;h2 id=&quot;variable-length-sequences-best-practices&quot;&gt;Variable-length sequences best practices&lt;/h2&gt;

&lt;p&gt;TorchScript does not support PackedSequence. Generally, when one is handling variable-length sequences, it is best to pad them into a single tensor and send that tensor through a TorchScript LSTM. Here’s an example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# List[Tensor], each Tensor is T' x C&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;padded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;padded&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# T x N x C, where N is batch size and T is the max of all T'&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hiddens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# T x N x C&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Of course, &lt;code class=&quot;highlighter-rouge&quot;&gt;output&lt;/code&gt; may have some garbage data in the padded regions; use &lt;code class=&quot;highlighter-rouge&quot;&gt;lengths&lt;/code&gt; to keep track of which part you don’t need.&lt;/p&gt;

&lt;h2 id=&quot;optimizations&quot;&gt;Optimizations&lt;/h2&gt;

&lt;p&gt;We will now explain the optimizations performed by the PyTorch JIT to speed up custom RNNs. We will use a simple custom LSTM model in TorchScript to illustrate the optimizations, but many of these are general and apply to other RNNs.&lt;/p&gt;

&lt;p&gt;To illustrate the optimizations we did and how we get benefits from those optimizations, we will run a simple custom LSTM model written in TorchScript (you can refer the code in the custom_lstm.py or the below code snippets) and time our changes.&lt;/p&gt;

&lt;p&gt;We set up the environment in a machine equipped with 2 Intel Xeon chip and one Nvidia P100, with cuDNN v7.3, CUDA 9.2 installed. The basic set up for the LSTM model is as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input_size = 512
hidden_size = 512
mini_batch = 64
numLayers = 1
seq_length = 100 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The most important thing PyTorch JIT did is to compile the python program to a PyTorch JIT IR, which is an intermediate representation used to model the program’s graph structure. This IR can then benefit from whole program optimization, hardware acceleration and overall has the potential to provide large computation gains. In this example, we run the initial TorchScript model with only compiler optimization passes that are provided by the JIT, including common subexpression elimination, constant pooling, constant propagation, dead code elimination and some peephole optimizations. We run the model training for 100 times after warm up and average the training time. The initial results for model forward time is around 27ms and backward time is around 64ms, which is a bit far away from what PyTorch cuDNN LSTM provided. Next we will explain the major optimizations we did on how we improve the performance on training or inferencing, starting with LSTMCell and LSTMLayer, and some misc optimizations.&lt;/p&gt;

&lt;h3 id=&quot;lstm-cell-forward&quot;&gt;LSTM Cell (forward)&lt;/h3&gt;

&lt;p&gt;Almost all the computations in an LSTM happen in the LSTMCell, so it’s important for us to take a look at the computations it contains and how can we improve their speed. Below is a sample LSTMCell implementation in TorchScript:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LSTMCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ScriptModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSTMCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_ih&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_hh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_ih&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_hh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;nd&quot;&gt;@jit.script_method&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# type: (Tensor, Tuple[Tensor, Tensor]) -&amp;gt; Tuple[Tensor, Tuple[Tensor, Tensor]]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;hx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;gates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_ih&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_ih&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_hh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_hh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ingate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forgetgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cellgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gates&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chunk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;ingate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ingate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;forgetgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forgetgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cellgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cellgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;cy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forgetgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ingate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cellgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;hy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This graph representation (IR) that TorchScript generated enables several optimizations and scalable computations. In addition to the typical compiler optimizations that we could do (CSE, constant propagation, etc. ) we can also run other IR transformations to make our code run faster.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Element-wise operator fusion. PyTorch JIT will automatically fuse element-wise ops, so when you have adjacent operators that are all element-wise, JIT will automatically group all those operations together into a single FusionGroup, this FusionGroup can then be launched with a single GPU/CPU kernel and performed in one pass. This avoids expensive memory reads and writes for each operation.&lt;/li&gt;
  &lt;li&gt;Reordering chunks and pointwise ops to enable more fusion. An LSTM cell adds gates together (a pointwise operation), and then chunks the gates into four pieces: the ifco gates. Then, it performs pointwise operations on the ifco gates like above. This leads to two fusion groups in practice: one fusion group for the element-wise ops pre-chunk, and one group for the element-wise ops post-chunk.
  The interesting thing to note here is that pointwise operations commute with &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.chunk&lt;/code&gt;: Instead of performing pointwise ops on some input tensors and chunking the output, we can chunk the input tensors and then perform the same pointwise ops on the output tensors. By moving the chunk to before the first fusion group, we can merge the first and second fusion groups into one big group.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/custom-rnn-chunk.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Tensor creation on the CPU is expensive, but there is ongoing work to make it faster. At this point, a LSTMCell runs three CUDA kernels: two &lt;code class=&quot;highlighter-rouge&quot;&gt;gemm&lt;/code&gt; kernels and one for the single pointwise group. One of the things we noticed was that there was a large gap between the finish of the second &lt;code class=&quot;highlighter-rouge&quot;&gt;gemm&lt;/code&gt; and the start of the single pointwise group. This gap was a period of time when the GPU was idling around and not doing anything. Looking into it more, we discovered that the problem was that &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.chunk&lt;/code&gt; constructs new tensors and that tensor construction was not as fast as it could be. Instead of constructing new Tensor objects, we taught the fusion compiler how to manipulate a data pointer and strides to do the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.chunk&lt;/code&gt; before sending it into the fused kernel, shrinking the amount of idle time between the second gemm and the launch of the element-wise fusion group. This give us around 1.2x increase speed up on the LSTM forward pass.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By doing the above tricks, we are able to fuse the almost all &lt;code class=&quot;highlighter-rouge&quot;&gt;LSTMCell&lt;/code&gt; forward graph (except the two gemm kernels) into a single fusion group, which corresponds to the &lt;code class=&quot;highlighter-rouge&quot;&gt;prim::FusionGroup_0&lt;/code&gt; in the above IR graph. It will then be launched into a single fused kernel for execution. With these optimizations the model performance improves significantly with average forward time reduced by around 17ms (2.7x speedup) to 10ms, and average backward time reduce by 37ms to 27ms (2.37x speed up).&lt;/p&gt;

&lt;h3 id=&quot;lstm-layer-forward&quot;&gt;LSTM Layer (forward)&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LSTMLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ScriptModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSTMLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;nd&quot;&gt;@jit.script_method&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# type: (Tensor, Tuple[Tensor, Tensor]) -&amp;gt; Tuple[Tensor, Tuple[Tensor, Tensor]]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unbind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;annotate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We did several tricks on the IR we generated for TorchScript LSTM to boost the performance, some example optimizations we did:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Loop Unrolling: We automatically unroll loops in the code (for big loops, we unroll a small subset of it), which then empowers us to do further optimizations on the for loops control flow. For example, the fuser can fuse together operations across iterations of the loop body, which results in a good performance improvement for control flow intensive models like LSTMs.&lt;/li&gt;
  &lt;li&gt;Batch Matrix Multiplication: For RNNs where the input is pre-multiplied (i.e. the model has a lot of matrix multiplies with the same LHS or RHS), we can efficiently batch those operations together into a single matrix multiply while chunking the outputs to achieve equivalent semantics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By applying these techniques, we reduced our time in the forward pass by an additional 1.6ms to 8.4ms (1.2x speed up) and timing in backward by 7ms to around 20ms (1.35x speed up).&lt;/p&gt;

&lt;h3 id=&quot;lstm-layer-backward&quot;&gt;LSTM Layer (backward)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“Tree” Batch Matrix Muplication: It is often the case that a single weight is reused multiple times in the LSTM backward graph, forming a tree where the leaves are matrix multiplies and nodes are adds.  These nodes can be combined together by concatenating the LHSs and RHSs in different dimensions, then computed as a single matrix multiplication. The formula of equivalence can be denoted as follows:&lt;/p&gt;

    &lt;p&gt;$L1 * R1 + L2 * R2 = torch.cat((L1, L2), dim=1) * torch.cat((R1, R2), dim=0)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Autograd is a critical component of what makes PyTorch such an elegant ML framework. As such, we carried this through to PyTorch JIT,  but using a new &lt;strong&gt;Automatic Differentiation&lt;/strong&gt; (AD) mechanism that works on the IR level.  JIT automatic differentiation will slice the forward graph into symbolically differentiable subgraphs, and generate backwards nodes for those subgraphs.  Taking the above IR as an example, we group the graph nodes into a single &lt;code class=&quot;highlighter-rouge&quot;&gt;prim::DifferentiableGraph_0&lt;/code&gt; for the operations that has AD formulas. For operations that have not been added to AD formulas, we will fall back to Autograd during execution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Optimizing the backwards path is hard, and the implicit broadcasting semantics make the optimization of automatic differentiation harder. PyTorch makes it convenient to write tensor operations without worrying about the shapes by broadcasting the tensors for you. For performance, the painful point in backward is that we need to have a summation for such kind of broadcastable operations. This results in the derivative of every broadcastable op being followed by a summation. Since we cannot currently fuse reduce operations, this causes FusionGroups to break into multiple small groups leading to bad performance. To deal with this, refer to this great &lt;a href=&quot;http://lernapparat.de/fast-lstm-pytorch/&quot;&gt;post&lt;/a&gt; written by Thomas Viehmann.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;misc-optimizations&quot;&gt;Misc Optimizations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;In addition to the steps laid about above, we also eliminated overhead between CUDA kernel launches and unnecessary tensor allocations. One example is when you do a tensor device look up. This can provide some poor performance initially with a lot of unnecessary allocations. When we remove these this results in a reduction from milliseconds to nanoseconds between kernel launches.&lt;/li&gt;
  &lt;li&gt;Lastly, there might be normalization applied in the custom LSTMCell like LayerNorm. Since LayerNorm and other normalization ops contains reduce operations, it is hard to fuse it in its entirety. Instead, we automatically decompose Layernorm to a statistics computation (reduce operations) + element-wise transformations, and then fuse those element-wise parts together. As of this post, there are some limitations on our auto differentiation and graph fuser infrastructure which limits the current support to inference mode only. We plan to add backward support in a future release.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With the above optimizations on operation fusion, loop unrolling, batch matrix multiplication and some misc optimizations, we can see a clear performance increase on our custom TorchScript LSTM forward and backward from the following figure:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/custom-rnn-improve.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;There are a number of additional optimizations that we did not cover in this post. In addition to the ones laid out in this post, we now see that our custom LSTM forward pass is on par with cuDNN. We are also working on optimizing backward more and expect to see improvements in future releases. Besides the speed that TorchScript provides, we introduced a much more flexible API that enable you to hand draft a lot more custom RNNs, which cuDNN could not provide.&lt;/p&gt;</content><author><name>The PyTorch Team</name></author><summary type="html">This week, we officially released PyTorch 1.1, a large feature update to PyTorch 1.0. One of the new features we’ve added is better support for fast, custom Recurrent Neural Networks (fastrnns) with TorchScript (the PyTorch JIT) (https://pytorch.org/docs/stable/jit.html).</summary></entry><entry><title type="html">PyTorch adds new dev tools as it hits production scale</title><link href="https://pytorch.org/blog/pytorch-adds-new-dev-tools/" rel="alternate" type="text/html" title="PyTorch adds new dev tools as it hits production scale" /><published>2019-05-01T00:00:00-07:00</published><updated>2019-05-01T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-adds-new-dev-tools</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-adds-new-dev-tools/">&lt;p&gt;&lt;em&gt;This is a partial re-post of the original blog post on the Facebook AI Blog. The full post can be &lt;a href=&quot;https://ai.facebook.com/blog/pytorch-adds-new-dev-tools-as-it-hits-production-scale/&quot;&gt;viewed here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Since its release just a few months ago, &lt;a href=&quot;http://pytorch.org/&quot;&gt;PyTorch 1.0&lt;/a&gt; has been rapidly adopted as a powerful, flexible deep learning platform that enables engineers and researchers to move quickly from research to production. We are highlighting some of the ways the AI engineering and research community is using PyTorch 1.0. We’re also sharing new details about the latest release, PyTorch 1.1, and showcasing some of the new development tools created by the community.&lt;/p&gt;

&lt;p&gt;Building on the initial launch of PyTorch in 2017, we partnered with the AI community to ship the stable release of PyTorch 1.0 &lt;a href=&quot;https://code.fb.com/ai-research/pytorch-developer-ecosystem-expands-1-0-stable-release/&quot;&gt;last December&lt;/a&gt;. Along with enhanced production-oriented capabilities and deep integration with leading cloud platforms, PyTorch 1.0 expands on the open source library’s core features, with the addition of PyTorch JIT (Just in time compilation) that seamlessly transitions between eager mode and graph mode to provide both flexibility and speed.&lt;/p&gt;

&lt;p&gt;Leading businesses across industries are beginning to use PyTorch to both facilitate their research and then also deploy at large scale for applications such as translation, computer vision, conversational interfaces, pharmaceutical research, factory optimization, and automated driving research. Community adoption of PyTorch has also continued to expand. Stanford, UC Berkeley, Caltech, and other universities are using PyTorch as a fundamental tool for their machine learning (ML) courses; new ecosystem projects have launched to support development on PyTorch; and major cloud platforms have expanded their integration with PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;using-pytorch-across-industries&quot;&gt;Using PyTorch across industries&lt;/h2&gt;

&lt;p&gt;Many leading businesses are moving to PyTorch 1.0 to accelerate development and deployment of new AI systems. Here are some examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Airbnb leveraged PyTorch’s rich libraries and APIs for conversational AI and deployed a Smart Reply to help the company’s service agents respond more effectively to customers.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://atomscience.org/&quot;&gt;ATOM&lt;/a&gt; is building a platform to generate and optimize new drug candidates significantly faster and with greater success than conventional processes. Using machine learning frameworks such as PyTorch, ATOM was able to design a variational autoencoder for representing diverse chemical structures and designing new drug candidates.&lt;/li&gt;
  &lt;li&gt;Genentech is utilizing PyTorch’s flexible control structures and dynamic graphs to train deep learning models that will aid in the development of individualized cancer therapy.&lt;/li&gt;
  &lt;li&gt;Microsoft is using PyTorch across its organization to develop ML models at scale and deploy them via the ONNX Runtime. Using PyTorch, Microsoft Cognition has built distributed language models that scale to billions of words and are now in production in offerings such as Cognitive Services.&lt;/li&gt;
  &lt;li&gt;Toyota Research Institute (TRI) is developing a two-pronged approach toward automated driving with Toyota Guardian and Toyota Chauffeur technologies. The Machine Learning Team at TRI is creating new deep learning algorithms to leverage Toyota’s 10 million sales per year data advantage. The flexibility of PyTorch has vastly accelerated their pace of exploration and its new production features will enable faster deployment towards their safety critical applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following the release of PyTorch 1.0 in December 2018, we’re now announcing the availability of v1.1, which improves performance, adds new model understanding and visualization tools to improve usability, and provides new APIs.&lt;/p&gt;

&lt;p&gt;Key features of PyTorch v1.1 include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/tensorboard&quot;&gt;TensorBoard&lt;/a&gt;: First-class and native support for visualization and model debugging with TensorBoard, a web application suite for inspecting and understanding training runs and graphs. PyTorch now natively supports TensorBoard with a simple “from torch.utils.tensorboard import SummaryWriter” command.&lt;/li&gt;
  &lt;li&gt;JIT compiler: Improvements to just-in-time (JIT) compilation. These include various bug fixes as well as expanded capabilities in TorchScript, such as support for dictionaries, user classes, and attributes.&lt;/li&gt;
  &lt;li&gt;New APIs: Support for Boolean tensors and better support for custom recurrent neural networks.&lt;/li&gt;
  &lt;li&gt;Distributed Training: Improved performance for common models such as CNNs, added support for multi device modules including the ability to split models across GPUs while still using Distributed Data Parallel (DDP) and support for modules where not all parameters are used in every iteration (e.g. control flow, like adaptive softmax, etc). See the latest tutorials &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ve also continued to partner with the community to foster projects and tools aimed at supporting ML engineers for needs ranging from improved model understanding to auto-tuning using AutoML methods. With the release of Ax and BoTorch (below), we will be sharing some of our core algorithms, including meta-learning for efficiently optimizing hyperparameters from based on historical tasks. We are excited to see this work open-sourced for the community to build on.&lt;/p&gt;

&lt;p&gt;This ecosystem includes open source projects and tools that have been deployed at production scale, as well as products and services from our partnership with industry leaders who share our vision of an open and collaborative AI community. Here are a few of the latest tools:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.facebook.com/blog/open-sourcing-ax-and-botorch-new-ai-tools-for-adaptive-experimentation/&quot;&gt;BoTorch&lt;/a&gt;: BoTorch is a research framework built on top of PyTorch to provide Bayesian optimization, a sample-efficient technique for sequential optimization of costly-to-evaluate black-box functions.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.facebook.com/blog/open-sourcing-ax-and-botorch-new-ai-tools-for-adaptive-experimentation/&quot;&gt;Ax&lt;/a&gt;: Ax is an ML platform for managing adaptive experiments. It enables researchers and engineers to systematically explore large configuration spaces in order to optimize machine learning models, infrastructure, and products.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.facebook.com/blog/open-sourcing-pytorch-biggraph-for-faster-embeddings-of-extremely-large-graphs/&quot;&gt;PyTorch-BigGraph&lt;/a&gt;: PBG is a distributed system for creating embeddings of very large graphs with billions of entities and trillions of edges. It includes support for sharding and negative sampling and it offers sample use cases based on Wikidata embeddings.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/ai-platform-notebooks/&quot;&gt;Google AI Platform Notebooks&lt;/a&gt;: AI Platform Notebooks is a new, hosted JupyterLab service from Google Cloud Platform. Data scientists can quickly create virtual machines running JupyterLab with the latest version of PyTorch preinstalled. It is also tightly integrated with GCP services such as BigQuery, Cloud Dataproc, Cloud Dataflow, and AI Factory, making it easy to execute the full ML cycle without ever leaving JupyterLab.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re also excited to see many interesting new projects from the broader PyTorch community. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ajbrock/BigGAN-PyTorch&quot;&gt;BigGAN-PyTorch&lt;/a&gt;:This is a full PyTorch reimplementation that uses gradient accumulation to provide the benefits of big batches on as few as four GPUs.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kernel-operations.io/geomloss/index.html&quot;&gt;GeomLoss&lt;/a&gt;: A Python API that defines PyTorch layers for geometric loss functions between sampled measures, images, and volumes. It includes MMD, Wasserstein, Sinkhorn, and more.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/geomloss.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/rusty1s/pytorch_geometric&quot;&gt;PyTorch Geometric&lt;/a&gt;: A deep learning extension library for PyTorch that offers several methods for deep learning on graphs and other irregular structures (also known as &lt;a href=&quot;http://geometricdeeplearning.com&quot;&gt;geometric deep learning&lt;/a&gt;) from a variety of published papers.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/fidler-lab/curve-gcn&quot;&gt;Curve-GCN&lt;/a&gt;: A real-time, interactive image annotation approach that uses an end-to-end-trained graph convolutional network (GCN). It supports object annotation by either polygons or splines, facilitating labeling efficiency for both line-based and curved objects. Curve-GCN runs 10x faster than traditional methods, such as Polygon-RNN++.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;udacity-fastai-and-others-develop-new-pytorch-resources&quot;&gt;Udacity, fast.ai, and others develop new PyTorch resources&lt;/h2&gt;

&lt;p&gt;PyTorch is ideal for teaching ML development because it enables rapid experimentation through its flexible, dynamic programming environment and user-friendly Pythonic interface. In addition, Google Colab now offers an interactive Jupyter Notebook environment that natively supports PyTorch, allowing developers to run any PyTorch tutorial immediately with free CPU and GPU resources.&lt;/p&gt;

&lt;p&gt;University-level classes — including &lt;a href=&quot;http://web.stanford.edu/class/cs224n&quot;&gt;Stanford NLP&lt;/a&gt;, &lt;a href=&quot;https://inst.eecs.berkeley.edu/~cs280/sp18/&quot;&gt;UC Berkeley&lt;/a&gt; Computer Vision, and &lt;a href=&quot;http://cast.caltech.edu&quot;&gt;Caltech&lt;/a&gt; Robotics courses — are now being taught on PyTorch. In addition, massive open online courses (MOOCs) are training thousands of new PyTorch developers.&lt;/p&gt;

&lt;p&gt;Today, we’re announcing a &lt;a href=&quot;https://blog.udacity.com/2019/05/announcing-the-secure-and-private-ai-scholarship-challenge-with-facebook.html&quot;&gt;new Udacity course&lt;/a&gt;, building upon the Intro to Deep Learning course launched last year. This new course, led by Andrew Trask of Oxford University and OpenMined, covers important concepts around privacy in AI, including methods such as differential privacy and federated learning. Facebook will also be providing scholarships to support students as they continue their ML education in Udacity’s full Nanodegree programs.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.fast.ai&quot;&gt;fast.ai&lt;/a&gt; community is also continuing to invest energy and resources in PyTorch. In June, fast.ai will launch a new course called Deep Learning from the Foundations, which will show developers how to go all the way from writing matrix multiplication from scratch to how to train and implement a state-of-the-art ImageNet model. The course will include deep dives into the underlying implementation of methods in the PyTorch and fast.ai libraries, and will use the code to explain and illustrate the academic papers that underlie these methods.&lt;/p&gt;

&lt;p&gt;As part of the course, fast.ai will also release new software modules, including fastai.audio, which brings the power of fast.ai’s deep abstractions and curated algorithms to the new PyTorch.audio module, and show how fastai.vision can be used to &lt;a href=&quot;https://www.fast.ai/2019/05/03/decrappify&quot;&gt;create stunning high-resolution videos&lt;/a&gt; from material such as old classic movies, and from cutting-edge microscopy sequences through a collaboration with the &lt;a href=&quot;https://www.salk.edu&quot;&gt;Salk Institute&lt;/a&gt;. In addition, fast.ai is contributing its new X-ResNet module, including a suite of models pretrained on ImageNet.&lt;/p&gt;

&lt;h2 id=&quot;getting-started-with-pytorch&quot;&gt;Getting started with PyTorch&lt;/h2&gt;

&lt;p&gt;Everyone in the AI community — including those new to ML development as well as researchers and engineers looking for ways to accelerate their end-to-end workflows — can experiment with PyTorch instantly by visiting &lt;a href=&quot;https://pytorch.org&quot;&gt;pytorch.org&lt;/a&gt; and launching a &lt;a href=&quot;https://pytorch.org/tutorials&quot;&gt;tutorial&lt;/a&gt; in Colab. There are also many easy ways to &lt;a href=&quot;https://pytorch.org/get-started/locally&quot;&gt;get started&lt;/a&gt; both locally and on popular cloud platforms.&lt;/p&gt;</content><author><name>The PyTorch Team</name></author><summary type="html">This is a partial re-post of the original blog post on the Facebook AI Blog. The full post can be viewed here</summary></entry><entry><title type="html">Stochastic Weight Averaging in PyTorch</title><link href="https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/" rel="alternate" type="text/html" title="Stochastic Weight Averaging in PyTorch" /><published>2019-04-29T00:00:00-07:00</published><updated>2019-04-29T00:00:00-07:00</updated><id>https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch</id><content type="html" xml:base="https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/">&lt;p&gt;In this blogpost we describe the recently proposed Stochastic Weight Averaging (SWA) technique [1, 2], and its new implementation in &lt;a href=&quot;https://github.com/pytorch/contrib&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torchcontrib&lt;/code&gt;&lt;/a&gt;.  SWA is a simple procedure that improves generalization in deep learning over Stochastic Gradient Descent (SGD) at no additional cost, and can be used as a drop-in replacement for any other optimizer in PyTorch. SWA has a wide range of applications and features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SWA has been shown to significantly improve generalization in computer vision tasks, including VGG, ResNets, Wide ResNets and DenseNets on ImageNet and CIFAR benchmarks [1, 2].&lt;/li&gt;
  &lt;li&gt;SWA provides state-of-the-art performance on key benchmarks in semi-supervised learning and domain adaptation [2].&lt;/li&gt;
  &lt;li&gt;SWA is shown to improve the stability of training as well as the final average rewards of policy-gradient methods in deep reinforcement learning [3].&lt;/li&gt;
  &lt;li&gt;An extension of SWA can obtain efficient Bayesian model averaging, as well as high quality uncertainty estimates and calibration in deep learning [4].&lt;/li&gt;
  &lt;li&gt;SWA for low precision training, SWALP, can match the performance of full-precision SGD even with all numbers quantized down to 8 bits, including gradient accumulators [5].&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In short, SWA performs an equal average of the weights traversed by SGD with a modified learning rate schedule (see the left panel of Figure 1.). SWA solutions end up in the center of a wide flat region of loss, while SGD tends to converge to the boundary of the low-loss region, making it susceptible to the shift between train and test error surfaces (see the middle and right panels of Figure 1).&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Illustrations of SWA and SGD with a Preactivation ResNet-164 on CIFAR-100 [1]. &lt;strong&gt;Left:&lt;/strong&gt; test error surface for three FGE samples and the corresponding SWA solution (averaging in weight space). &lt;strong&gt;Middle&lt;/strong&gt; and &lt;strong&gt;Right:&lt;/strong&gt; test error and train loss surfaces showing the weights proposed by SGD (at convergence) and SWA, starting from the same initialization of SGD after 125 training epochs. Please see [1] for details on how these figures were constructed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;With our new implementation in &lt;a href=&quot;https://github.com/pytorch/contrib&quot;&gt;torchcontrib&lt;/a&gt; using SWA is as easy as using any other optimizer in PyTorch:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchcontrib.optim&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SWA&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# training loop&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;base_opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchcontrib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SWA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swap_swa_sgd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can wrap any optimizer from &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.optim&lt;/code&gt; using the &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; class, and then train your model as usual. When training is complete you simply call &lt;code class=&quot;highlighter-rouge&quot;&gt;swap_swa_sgd()&lt;/code&gt; to set the weights of your model to their SWA averages. Below we explain the SWA procedure and the parameters of the &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; class in detail. We emphasize that SWA can be combined with &lt;em&gt;any&lt;/em&gt; optimization procedure, such as Adam, in the same way that it can be combined with SGD.&lt;/p&gt;

&lt;h2 id=&quot;is-this-just-averaged-sgd&quot;&gt;Is this just Averaged SGD?&lt;/h2&gt;

&lt;p&gt;At a high level, averaging SGD iterates dates back several decades in convex optimization [6, 7], where it is sometimes referred to as Polyak-Ruppert averaging, or &lt;em&gt;averaged&lt;/em&gt; SGD. &lt;strong&gt;But the details matter&lt;/strong&gt;. &lt;em&gt;Averaged SGD&lt;/em&gt; is often employed in conjunction with a decaying learning rate, and an exponentially moving average, typically for convex optimization. In convex optimization, the focus has been on improved rates of convergence. In deep learning, this form of averaged SGD smooths the trajectory of SGD iterates, but does not perform very differently.&lt;/p&gt;

&lt;p&gt;By contrast, SWA is focused on an &lt;strong&gt;equal average&lt;/strong&gt; of SGD iterates with a modified &lt;strong&gt;cyclical or high constant learning rate&lt;/strong&gt;, and exploits the flatness of training objectives [8] specific to &lt;strong&gt;deep learning&lt;/strong&gt; for &lt;strong&gt;improved generalization&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;stochastic-weight-averaging&quot;&gt;Stochastic Weight Averaging&lt;/h2&gt;

&lt;p&gt;There are two important ingredients that make SWA work. First, SWA uses a modified learning rate schedule so that SGD continues to explore the set of high-performing networks instead of simply converging to a single solution. For example, we can use the standard decaying learning rate strategy for the first 75% of training time, and then set the learning rate to a reasonably high constant value for the remaining 25% of the time (see the Figure 2 below). The second ingredient is to average the weights of the networks traversed by SGD. For example, we can maintain a running average of the weights obtained in the end of every epoch within the last 25% of training time (see Figure 2).&lt;/p&gt;
&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swa/figure2-highres.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; Illustration of the learning rate schedule adopted by SWA. Standard decaying schedule is used for the first 75% of the training and then a high constant value is used for the remaining 25%. The SWA averages are formed during the last 25% of training.&lt;/p&gt;

&lt;p&gt;In our implementation the auto mode of the &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; optimizer allows us to run the procedure described above. To run SWA in auto mode you just need to wrap your optimizer &lt;code class=&quot;highlighter-rouge&quot;&gt;base_opt&lt;/code&gt; of choice (can be SGD, Adam, or any other &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.optim.Optimizer&lt;/code&gt;) with &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA(base_opt, swa_start, swa_freq, swa_lr)&lt;/code&gt;. After &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_start&lt;/code&gt; optimization steps the learning rate will be switched to a constant value &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_lr&lt;/code&gt;, and in the end of every &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_freq&lt;/code&gt; optimization steps a snapshot of the weights will be added to the SWA running average. Once you run &lt;code class=&quot;highlighter-rouge&quot;&gt;opt.swap_swa_sgd()&lt;/code&gt;, the weights of your model are replaced with their SWA running averages.&lt;/p&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;

&lt;p&gt;One important detail to keep in mind is batch normalization. Batch normalization layers compute running statistics of activations during training. Note that the SWA averages of the weights are never used to make predictions during training, and so the batch normalization layers do not have the activation statistics computed after you reset the weights of your model with &lt;code class=&quot;highlighter-rouge&quot;&gt;opt.swap_swa_sgd()&lt;/code&gt;. To compute the activation statistics you can just make a forward pass on your training data using the SWA model once the training is finished. In the &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; class we provide a helper function &lt;code class=&quot;highlighter-rouge&quot;&gt;opt.bn_update(train_loader, model)&lt;/code&gt;. It updates the activation statistics for every batch normalization layer in the model by making a forward pass on the &lt;code class=&quot;highlighter-rouge&quot;&gt;train_loader&lt;/code&gt; data loader. You only need to call this function once in the end of training.&lt;/p&gt;

&lt;h2 id=&quot;advanced-learning-rate-schedules&quot;&gt;Advanced Learning-Rate Schedules&lt;/h2&gt;

&lt;p&gt;SWA can be used with any learning rate schedule that encourages exploration of the flat region of solutions. For example, you can use cyclical learning rates in the last 25% of the training time instead of a constant value, and average the weights of the networks corresponding to the lowest values of the learning rate within each cycle (see Figure 3).&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swa/figure3-highres.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Illustration of SWA with an alternative learning rate schedule. Cyclical learning rates are adopted in the last 25% of training, and models for averaging are collected in the end of each cycle.&lt;/p&gt;

&lt;p&gt;In our implementation you can implement custom learning rate and weight averaging strategies by using &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; in the manual mode. The following code is equivalent to the auto mode code presented in the beginning of this blogpost.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchcontrib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SWA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_swa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swap_swa_sgd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In manual mode you don’t specify &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_start&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_lr&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_freq&lt;/code&gt;, and just call &lt;code class=&quot;highlighter-rouge&quot;&gt;opt.update_swa()&lt;/code&gt; whenever you want to update the SWA running averages (for example in the end of each learning rate cycle). In manual mode &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; doesn’t change the learning rate, so you can use any schedule you want as you would normally do with any other &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.optim.Optimizer&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;why-does-it-work&quot;&gt;Why does it work?&lt;/h2&gt;

&lt;p&gt;SGD converges to a solution within a wide flat region of loss. The weight space is extremely high-dimensional, and most of the volume of the flat region is concentrated near the boundary, so SGD solutions will always be found near the boundary of the flat region of the loss. SWA on the other hand averages multiple SGD solutions, which allows it to move towards the center of the flat region.&lt;/p&gt;

&lt;p&gt;We expect solutions that are centered in the flat region of the loss to generalize better than those near the boundary. Indeed, train and test error surfaces are not perfectly aligned in the weight space. Solutions that are centered in the flat region are not as susceptible to the shifts between train and test error surfaces as those near the boundary. In Figure 4 below we show the train loss and test error surfaces along the direction connecting the SWA and SGD solutions. As you can see, while SWA solution has a higher train loss compared to the SGD solution, it is centered in the region of low loss, and has a substantially better test error.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure4.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Train loss and test error along the line connecting the SWA solution (circle) and SGD solution (square). SWA solution is centered in a wide region of low train loss while the SGD solution lies near the boundary. Because of the shift between train loss and test error surfaces, SWA solution leads to much better generalization.&lt;/p&gt;

&lt;h2 id=&quot;examples-and-results&quot;&gt;Examples and Results&lt;/h2&gt;

&lt;p&gt;We released a GitHub repo &lt;a href=&quot;https://github.com/izmailovpavel/contrib_swa_examples&quot;&gt;here&lt;/a&gt; with examples of using the &lt;code class=&quot;highlighter-rouge&quot;&gt;torchcontrib&lt;/code&gt; implementation of SWA for training DNNs. For example, these examples can be used to achieve the following results on CIFAR-100:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;DNN (Budget)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SGD&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SWA 1 Budget&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SWA 1.25 Budgets&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SWA 1.5 Budgets&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;VGG16 (200)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;72.55 ± 0.10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;73.91 ± 0.12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;74.17 ± 0.15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;74.27 ± 0.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PreResNet110 (150)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;76.77 ± 0.38&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;78.75 ± 0.16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;78.91 ± 0.29&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;79.10 ± 0.21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PreResNet164 (150)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;78.49 ± 0.36&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;79.77 ± 0.17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80.18 ± 0.23&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80.35 ± 0.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;WideResNet28x10 (200)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80.82 ± 0.23&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;81.46 ± 0.23&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;81.91 ± 0.27&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;82.15 ± 0.27&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;semi-supervised-learning&quot;&gt;Semi-Supervised Learning&lt;/h2&gt;

&lt;p&gt;In a follow-up &lt;a href=&quot;https://arxiv.org/abs/1806.05594&quot;&gt;paper&lt;/a&gt; SWA was applied to semi-supervised learning, where it illustrated  improvements beyond the best reported results in multiple settings. For example, with SWA you can get 95% accuracy on CIFAR-10 if you only have the training labels for 4k training data points (the previous best reported result on this problem was 93.7%). This paper also explores averaging multiple times within epochs, which can accelerate convergence and find still flatter solutions in a given time.&lt;/p&gt;
&lt;div class=&quot;text-center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure5.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; Performance of fast-SWA on semi-supervised learning with CIFAR-10. fast-SWA achieves record results in every setting considered.&lt;/p&gt;

&lt;h2 id=&quot;calibration-and-uncertainty-estimates&quot;&gt;Calibration and Uncertainty Estimates&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.02476&quot;&gt;SWA-Gaussian&lt;/a&gt; (SWAG) is a simple, scalable and convenient approach to uncertainty estimation and calibration in Bayesian deep learning. Similarly to SWA, which maintains a running average of SGD iterates, SWAG estimates the first and second moments of the iterates to construct a Gaussian distribution over weights. SWAG distribution approximates the shape of the true posterior: Figure 6 below shows the SWAG distribution on top of the posterior log-density for PreResNet-164 on CIFAR-100.&lt;/p&gt;
&lt;div class=&quot;text-center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure6.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; SWAG distribution on top of posterior log-density for PreResNet-164 on CIFAR-100. The shape of SWAG distribution is aligned with the posterior.&lt;/p&gt;

&lt;p&gt;Empirically, SWAG performs on par or better than popular alternatives including MC dropout, KFAC Laplace, and temperature scaling on uncertainty quantification, out-of-distribution detection, calibration and transfer learning in computer vision tasks. Code for SWAG is available &lt;a href=&quot;https://github.com/wjmaddox/swa_gaussian&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;In another follow-up &lt;a href=&quot;http://www.gatsby.ucl.ac.uk/~balaji/udl-camera-ready/UDL-24.pdf&quot;&gt;paper&lt;/a&gt; SWA was shown to improve the performance of policy gradient methods A2C and DDPG on several Atari games and MuJoCo environments.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Environment&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;A2C&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;A2C + SWA&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Breakout&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;522 ± 34&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;703 ± 60&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Qbert&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18777 ± 778&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21272 ± 655&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SpaceInvaders&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7727 ± 1121&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21676 ± 8897&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Seaquest&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1779 ± 4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1795 ± 4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CrazyClimber&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;147030 ± 10239&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;139752 ± 11618&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BeamRider&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9999 ± 402&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11321 ± 1065&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;low-precision-training&quot;&gt;Low Precision Training&lt;/h2&gt;
&lt;p&gt;We can filter through quantization noise by combining weights that have been rounded down with weights that have been rounded up. Moreover, by averaging weights to find a flat region of the loss surface, large perturbations of the weights will not affect the quality of the solution (Figures 7 and 8). Recent work shows that by adapting SWA to the low precision setting, in a method called SWALP, one can &lt;em&gt;match the performance of full-precision SGD even with all training in 8 bits&lt;/em&gt; [5]. This is quite a practically important result, given that (1) SGD training in 8 bits performs notably worse than full precision SGD, and (2) low precision training is significantly harder than predictions in low precision after training (the usual setting). For example, a ResNet-164 trained on CIFAR-100 with float (16-bit) SGD achieves 22.2% error, while 8-bit SGD achieves 24.0% error. By contrast, SWALP with 8 bit training achieves 21.8% error.&lt;/p&gt;
&lt;div class=&quot;text-center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure7.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 7.&lt;/strong&gt; Quantizing in a flat region can still provide solutions with low loss.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure8.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 8.&lt;/strong&gt; Low precision SGD training (with a modified learning rate schedule) and SWALP.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;One of the greatest open questions in deep learning is why SGD manages to find good solutions, given that the training objectives are highly multimodal, and there are in principle many settings of parameters that achieve no training loss but poor generalization. By understanding geometric features such as flatness, which relate to generalization, we can begin to resolve these questions and build optimizers that provide even better generalization, and many other useful features, such as uncertainty representation. We have presented SWA, a simple drop-in replacement for standard SGD, which can in principle benefit anyone training a deep neural network. SWA has been demonstrated to have strong performance in a number of areas, including computer vision, semi-supervised learning, reinforcement learning, uncertainty representation, calibration, Bayesian model averaging, and low precision training.&lt;/p&gt;

&lt;p&gt;We encourage you try out SWA! Using SWA is now as easy as using any other optimizer in PyTorch. And even if you have already trained your model with SGD (or any other optimizer), it’s very easy to realize the benefits of SWA by running SWA for a small number of epochs starting with a pre-trained model.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[1] Averaging Weights Leads to Wider Optima and Better Generalization; Pavel Izmailov, Dmitry Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson; Uncertainty in Artificial Intelligence (UAI), 2018&lt;/li&gt;
  &lt;li&gt;[2] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average; Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson; International Conference on Learning Representations (ICLR), 2019&lt;/li&gt;
  &lt;li&gt;[3] Improving Stability in Deep Reinforcement Learning with Weight Averaging; Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur Garipov, Pavel Shvechikov, Dmitry Vetrov, Andrew Gordon Wilson, UAI 2018 Workshop: Uncertainty in Deep Learning, 2018&lt;/li&gt;
  &lt;li&gt;[4]  A Simple Baseline for Bayesian Uncertainty in Deep Learning, Wesley Maddox, Timur Garipov, Pavel Izmailov, Andrew Gordon Wilson, arXiv pre-print, 2019: &lt;a href=&quot;https://arxiv.org/abs/1902.02476&quot;&gt;https://arxiv.org/abs/1902.02476&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[5] SWALP : Stochastic Weight Averaging in Low Precision Training, Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, Christopher De Sa, To appear at the International Conference on Machine Learning  (ICML), 2019.&lt;/li&gt;
  &lt;li&gt;[6] David Ruppert. Efficient estimations from a slowly convergent Robbins-Monro process. Technical report, Cornell University Operations Research and Industrial Engineering, 1988.&lt;/li&gt;
  &lt;li&gt;[7] Acceleration of stochastic approximation by averaging. Boris T Polyak and Anatoli B Juditsky. SIAM Journal on Control and Optimization, 30(4):838–855, 1992.&lt;/li&gt;
  &lt;li&gt;[8] Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, Andrew Gordon Wilson. Neural Information Processing Systems (NeurIPS), 2018&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pavel Izmailov and Andrew Gordon Wilson</name></author><summary type="html">In this blogpost we describe the recently proposed Stochastic Weight Averaging (SWA) technique [1, 2], and its new implementation in torchcontrib. SWA is a simple procedure that improves generalization in deep learning over Stochastic Gradient Descent (SGD) at no additional cost, and can be used as a drop-in replacement for any other optimizer in PyTorch. SWA has a wide range of applications and features:</summary></entry><entry><title type="html">The road to 1.0: production ready PyTorch</title><link href="https://pytorch.org/blog/the-road-to-1_0/" rel="alternate" type="text/html" title="The road to 1.0: production ready PyTorch" /><published>2018-05-02T00:00:00-07:00</published><updated>2018-05-02T00:00:00-07:00</updated><id>https://pytorch.org/blog/the-road-to-1_0</id><content type="html" xml:base="https://pytorch.org/blog/the-road-to-1_0/">&lt;p&gt;We would like to give you a preview of the roadmap for PyTorch 1.0 , the next release of PyTorch. Over the last year, we’ve had 0.2, 0.3 and 0.4 transform PyTorch from a [Torch+Chainer]-like interface into something cleaner, adding double-backwards, numpy-like functions, advanced indexing and removing Variable boilerplate. At this time, we’re confident that the API is in a reasonable and stable state to confidently release a 1.0.&lt;/p&gt;

&lt;p&gt;However, 1.0 isn’t just about stability of the interface.&lt;/p&gt;

&lt;p&gt;One of PyTorch’s biggest strengths is its first-class Python integration, imperative style, simplicity of the API and options. These are aspects that make PyTorch good for research and hackability.&lt;/p&gt;

&lt;p&gt;One of its biggest downsides has been production-support. What we mean by production-support is the countless things one has to do to models to run them efficiently at massive scale:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;exporting to C++-only runtimes for use in larger projects&lt;/li&gt;
  &lt;li&gt;optimizing mobile systems on iPhone, Android, Qualcomm and other systems&lt;/li&gt;
  &lt;li&gt;using more efficient data layouts and performing kernel fusion to do faster inference (saving 10% of speed or memory at scale is a big win)&lt;/li&gt;
  &lt;li&gt;quantized inference (such as 8-bit inference)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Startups, large companies and anyone who wants to build a product around PyTorch have asked for production support. At Facebook (the largest stakeholder for PyTorch) we have Caffe2, which has been the production-ready platform, running in our datacenters and shipping to more than 1 billion phones spanning eight generations of iPhones and six generations of Android CPU architectures. It has server-optimized inference on Intel / ARM, TensorRT support, and all the necessary bits for production. Considering all this value locked-in to a platform that the PyTorch team works quite closely with, &lt;strong&gt;we decided to marry PyTorch and Caffe2 which gives the production-level readiness for PyTorch&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Supporting production features without adding usability issues for our researchers and end-users needs creative solutions.&lt;/p&gt;

&lt;h2 id=&quot;production--pain-for-researchers&quot;&gt;Production != Pain for researchers&lt;/h2&gt;

&lt;p&gt;Adding production capabilities involves increasing the API complexity and number of configurable options for models. One configures memory-layouts (NCHW vs NHWC vs N,C/32,H,W,32, each providing different performance characteristics), quantization (8-bit? 3-bit?), fusion of low-level kernels (you used a Conv + BatchNorm + ReLU, let’s fuse them into a single kernel), separate backend options (MKLDNN backend for a few layers and NNPACK backend for other layers), etc.&lt;/p&gt;

&lt;p&gt;PyTorch’s central goal is to provide a great platform for research and hackability. So, while we add all these optimizations, we’ve been working with a hard design constraint to never trade these off against usability.&lt;/p&gt;

&lt;p&gt;To pull this off, we are introducing &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.jit&lt;/code&gt;, a just-in-time (JIT) compiler that at runtime takes your PyTorch models and rewrites them to run at production-efficiency. The JIT compiler can also export your model to run in a C++-only runtime based on Caffe2 bits.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In 1.0, your code continues to work as-is, we’re not making any big changes to the existing API.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Making your model production-ready is an opt-in annotation, which uses the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.jit&lt;/code&gt; compiler to export your model to a Python-less environment, and improving its performance. Let’s walk through the JIT compiler in detail.&lt;/p&gt;

&lt;h2 id=&quot;torchjit-a-jit-compiler-for-your-models&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.jit&lt;/code&gt;: A JIT-compiler for your models&lt;/h2&gt;

&lt;p&gt;We strongly believe that it’s hard to match the productivity you get from specifying your models directly as idiomatic Python code. This is what makes PyTorch so flexible, but it also means that PyTorch pretty much never knows the operation you’ll run next. This however is a big blocker for export/productionization and heavyweight automatic performance optimizations because they need full upfront knowledge of how the computation will look before it even gets executed.&lt;/p&gt;

&lt;p&gt;We provide two opt-in ways of recovering this information from your code, one based on tracing native python code and one based on compiling a subset of the python language annotated into a python-free intermediate representation. After thorough discussions we concluded that they’re both going to be useful in different contexts, and as such you will be able to mix and match them freely.&lt;/p&gt;

&lt;h2 id=&quot;tracing-mode&quot;&gt;Tracing Mode&lt;/h2&gt;

&lt;p&gt;The PyTorch tracer, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.jit.trace&lt;/code&gt;, is a function that records all the native PyTorch operations performed in a code region, along with the data dependencies between them. In fact, PyTorch has had a tracer since 0.3, which has been used for exporting models through ONNX. What changes now, is that you no longer necessarily need to take the trace and run it elsewhere - PyTorch can re-execute it for you, using a carefully designed high-performance C++ runtime. As we develop PyTorch 1.0 this runtime will integrate all the optimizations and hardware integrations that Caffe2 provides.&lt;/p&gt;

&lt;p&gt;The biggest benefit of this approach is that it doesn’t really care how your Python code is structured — you can trace through generators or coroutines, modules or pure functions. Since we only record native PyTorch operators, these details have no effect on the trace recorded. This behavior, however, is a double-edged sword. For example, if you have a loop in your model, it will get unrolled in the trace, inserting a copy of the loop body for as many times as the loop ran. This opens up opportunities for zero-cost abstraction (e.g. you can loop over modules, and the actual trace will be loop-overhead free!), but on the other hand this will also affect data dependent loops (think of e.g. processing sequences of varying lengths), effectively hard-coding a single length into the trace.&lt;/p&gt;

&lt;p&gt;For networks that do not contain loops and if statements, tracing is non-invasive and is robust enough to handle a wide variety of coding styles. This code example illustrates what tracing looks like:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# This will run your nn.Module or regular Python function with the example&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# input that you provided. The returned callable can be used to re-execute&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# all operations that happened during the example run, but it will no longer&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# use the Python interpreter.&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.jit&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;traced_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example_input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;traced_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example_input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# The training loop doesn't change. Traced model behaves exactly like an&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# nn.Module, except that you can't edit what it does or change its attributes.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Think of it as a &quot;frozen module&quot;.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;traced_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;script-mode&quot;&gt;Script Mode&lt;/h2&gt;

&lt;p&gt;Tracing mode is a great way to minimize the impact on your code, but we’re also very excited about the models that fundamentally make use of control flow such as RNNs. Our solution to this is a scripting mode.&lt;/p&gt;

&lt;p&gt;In this case you write out a regular Python function, except that you can no longer use certain more complicated language features. Once you isolated the desired functionality, you let us know that you’d like the function to get compiled by decorating it with an &lt;code class=&quot;highlighter-rouge&quot;&gt;@script&lt;/code&gt; decorator. This annotation will transform your python function directly into our high-performance C++ runtime. This lets us recover all the PyTorch operations along with loops and conditionals. They will be embedded into our internal representation of this function, and will be accounted for every time this function is run.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.jit&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@script&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rnn_loop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;optimization-and-export&quot;&gt;Optimization and Export&lt;/h2&gt;

&lt;p&gt;Regardless of whether you use tracing or &lt;code class=&quot;highlighter-rouge&quot;&gt;@script&lt;/code&gt;, the result is a python-free representation of your model, which can be used to optimize the model or to export the model from python for use in production environments.&lt;/p&gt;

&lt;p&gt;Extracting bigger segments of the model into an intermediate representation makes it possible to do sophisticated whole-program optimizations and to offload computation to specialized AI accelerators which operate on graphs of computation. We have already been developing the beginnings of these optimizations, including passes that fuse GPU operations together to improve the performance of smaller RNN models.&lt;/p&gt;

&lt;p&gt;It also allows us to use existing high-performance backends available in Caffe2 today to run the model efficiently. Additionally, @script functions (and modules!) can be fully exported to ONNX in a way that retains their dynamic nature, such that you can easily run them in a Python-free environment using the model executors from Caffe2 or by transferring the model to any other framework supporting ONNX.&lt;/p&gt;

&lt;h2 id=&quot;usability&quot;&gt;Usability&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;We care deeply about maintaining our current level of usability and we know that execution of the code not directly in Python leads to harder debugging, but this is something that we think about a lot, and we’re making sure that you’re not getting locked in to a completely different programming language.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First, we follow the principle of pay for what you use — if you don’t need to optimize or export your model, you do not have to use these new features and won’t see any downsides. Furthermore, use of traced or @script modules/functions can be done incrementally. For instance, all of these behaviors are allowed: You can trace part of your model and use the trace in a larger non-traced model. You can use tracing for 90% of your model, and use @script for the one sub-module that actually has some control flow in it. You can write a function using @script and have it call a native python function. If something appears incorrect in an @script function, you can remove the annotation and the code will execute in native python where it is easy to debug using your favorite tools and methods. Think of tracing and @script like type annotations using MyPy or TypeScript — each additional annotation can be tested incrementally, and none are required until you want to optimize or productionize.&lt;/p&gt;

&lt;p&gt;Most importantly, these modes will be built into the core of PyTorch so that mixing and matching them with your existing code can happen seamlessly.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: The name JIT for these components is a bit of a misnomer and comes from historical reasons. The tracing/function execution in PyTorch started out as an optimizing JIT compiler that generated fused CUDA kernels but then grew to encompass optimization, @script, and export. When it is ready for release we will likely rename this functionality to the hybrid frontend, but we wanted to present it here as it is named in the code so that you can follow along as we develop it.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;other-changes-and-improvements&quot;&gt;Other changes and improvements&lt;/h2&gt;

&lt;p&gt;Production support is the big feature for 1.0, but we will continue optimizing and fixing other parts of PyTorch as course of the standard release process.&lt;/p&gt;

&lt;p&gt;On the backend side of things, PyTorch will see some changes, which might affect user-written C and C++ extensions. We are replacing (or refactoring) the backend ATen library to incorporate features and optimizations from Caffe2.&lt;/p&gt;

&lt;h2 id=&quot;last-words&quot;&gt;Last Words&lt;/h2&gt;

&lt;p&gt;We aim to release 1.0 some time during the summer. You can follow-along our progress on the &lt;a href=&quot;https://github.com/pytorch/pytorch/pulls&quot;&gt;Pull Requests&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;You can read this from the perspective of the Caffe2 project at: &lt;a href=&quot;https://caffe2.ai/blog/2018/05/02/Caffe2_PyTorch_1_0.html&quot;&gt;https://caffe2.ai/blog/2018/05/02/Caffe2_PyTorch_1_0.html&lt;/a&gt;&lt;/p&gt;</content><author><name>The PyTorch Team</name></author><summary type="html">We would like to give you a preview of the roadmap for PyTorch 1.0 , the next release of PyTorch. Over the last year, we’ve had 0.2, 0.3 and 0.4 transform PyTorch from a [Torch+Chainer]-like interface into something cleaner, adding double-backwards, numpy-like functions, advanced indexing and removing Variable boilerplate. At this time, we’re confident that the API is in a reasonable and stable state to confidently release a 1.0.</summary></entry><entry><title type="html">PyTorch 0.4.0 Migration Guide</title><link href="https://pytorch.org/blog/pytorch-0_4_0-migration-guide/" rel="alternate" type="text/html" title="PyTorch 0.4.0 Migration Guide" /><published>2018-04-22T00:00:00-07:00</published><updated>2018-04-22T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-0_4_0-migration-guide</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-0_4_0-migration-guide/">&lt;p&gt;Welcome to the migration guide for PyTorch 0.4.0. In this release we introduced &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v0.4.0&quot;&gt;many exciting new features and critical bug fixes&lt;/a&gt;, with the goal of providing users a better and cleaner interface. In this guide, we will cover the most important changes in migrating existing code from previous versions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Tensors&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Variables&lt;/code&gt; have merged&lt;/li&gt;
  &lt;li&gt;Support for 0-dimensional (scalar) &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensors&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Deprecation of the &lt;code class=&quot;highlighter-rouge&quot;&gt;volatile&lt;/code&gt; flag&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dtypes&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;devices&lt;/code&gt;, and Numpy-style &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; creation functions&lt;/li&gt;
  &lt;li&gt;Writing device-agnostic code&lt;/li&gt;
  &lt;li&gt;New edge-case constraints on names of submodules, parameters, and buffers in &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;merging-tensor-and-variable-and-classes&quot;&gt;Merging &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensors.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt;&lt;/a&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Variable&lt;/code&gt; and classes&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensors.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.autograd.Variable&lt;/code&gt; are now the same class. More precisely, &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensors.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; is capable of tracking history and behaves like the old &lt;code class=&quot;highlighter-rouge&quot;&gt;Variable&lt;/code&gt;; &lt;code class=&quot;highlighter-rouge&quot;&gt;Variable&lt;/code&gt; wrapping continues to work as before but returns an object of type &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensors.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;. This means that you don’t need the &lt;code class=&quot;highlighter-rouge&quot;&gt;Variable&lt;/code&gt; wrapper everywhere in your code anymore.&lt;/p&gt;

&lt;h3 id=&quot;the-type-of-a-tensor-has-changed&quot;&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;type()&lt;/code&gt; of a &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensors.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt;&lt;/a&gt; has changed&lt;/h3&gt;

&lt;p&gt;Note also that the &lt;code class=&quot;highlighter-rouge&quot;&gt;type()&lt;/code&gt; of a Tensor no longer reflects the data type. Use &lt;code class=&quot;highlighter-rouge&quot;&gt;isinstance()&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;x.type()&lt;/code&gt;instead:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DoubleTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# was torch.DoubleTensor&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;class 'torch.Tensor'&amp;gt;&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# OK: 'torch.DoubleTensor'&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'torch.DoubleTensor'&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DoubleTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# OK: True&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;when-does-autograd-start-tracking-history-now&quot;&gt;When does &lt;a href=&quot;https://pytorch.org/docs/0.4.0/autograd.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;autograd&lt;/code&gt;&lt;/a&gt; start tracking history now?&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;requires_grad&lt;/code&gt;, the central flag for &lt;a href=&quot;https://pytorch.org/docs/0.4.0/autograd.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;autograd&lt;/code&gt;&lt;/a&gt;, is now an attribute on &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensors&lt;/code&gt;. The same rules previously used for &lt;code class=&quot;highlighter-rouge&quot;&gt;Variables&lt;/code&gt; applies to &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensors&lt;/code&gt;; &lt;a href=&quot;https://pytorch.org/docs/0.4.0/autograd.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;autograd&lt;/code&gt;&lt;/a&gt; starts tracking history when any input &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; of an operation has &lt;code class=&quot;highlighter-rouge&quot;&gt;requires_grad=True&lt;/code&gt;. For example,&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# create a tensor with requires_grad=False (default)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# another tensor with requires_grad=False&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# both inputs have requires_grad=False. so does the output&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# then autograd won't track this computation. let's verify!&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;RuntimeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensors&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;does&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;does&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;have&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_fn&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# now create a tensor with requires_grad=True&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# add to the previous result that has require_grad=False&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# the total sum now requires grad!&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# autograd can compute the gradients as well&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# and no computation is wasted to compute gradients for x, y and z, which don't require grad&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;manipulating-requires_grad-flag&quot;&gt;Manipulating &lt;code class=&quot;highlighter-rouge&quot;&gt;requires_grad&lt;/code&gt; flag&lt;/h4&gt;

&lt;p&gt;Other than directly setting the attribute, you can change this flag &lt;code class=&quot;highlighter-rouge&quot;&gt;in-place&lt;/code&gt; using &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensors.html#torch.Tensor.requires_grad_&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;my_tensor.requires_grad_()&lt;/code&gt;&lt;/a&gt;, or, as in the above example, at creation time by passing it in as an argument (default is &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;), e.g.,&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;existing_tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;existing_tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;what-about-data&quot;&gt;What about &lt;code class=&quot;highlighter-rouge&quot;&gt;.data?&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;.data&lt;/code&gt; was the primary way to get the underlying &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; from a &lt;code class=&quot;highlighter-rouge&quot;&gt;Variable&lt;/code&gt;. After this merge, calling &lt;code class=&quot;highlighter-rouge&quot;&gt;y = x.data&lt;/code&gt; still has similar semantics. So &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt; will be a &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; that shares the same data with &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;, is unrelated with the computation history of &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;, and has &lt;code class=&quot;highlighter-rouge&quot;&gt;requires_grad=False&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;However, &lt;code class=&quot;highlighter-rouge&quot;&gt;.data&lt;/code&gt; can be unsafe in some cases. Any changes on &lt;code class=&quot;highlighter-rouge&quot;&gt;x.data&lt;/code&gt; wouldn’t be tracked by &lt;code class=&quot;highlighter-rouge&quot;&gt;autograd&lt;/code&gt;, and the computed gradients would be incorrect if &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; is needed in a backward pass. A safer alternative is to use &lt;a href=&quot;https://pytorch.org/docs/master/autograd.html#torch.Tensor.detach&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;x.detach()&lt;/code&gt;&lt;/a&gt;, which also returns a &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; that shares data with &lt;code class=&quot;highlighter-rouge&quot;&gt;requires_grad=False&lt;/code&gt;, but will have its in-place changes reported by &lt;code class=&quot;highlighter-rouge&quot;&gt;autograd&lt;/code&gt; if &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; is needed in backward.&lt;/p&gt;

&lt;p&gt;Here is an example of the difference between &lt;code class=&quot;highlighter-rouge&quot;&gt;.data&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;x.detach()&lt;/code&gt; (and why we recommend using &lt;code class=&quot;highlighter-rouge&quot;&gt;detach&lt;/code&gt; in general).&lt;/p&gt;

&lt;p&gt;If you use &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor.detach()&lt;/code&gt;, the gradient computation is guaranteed to be correct.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# modified by c.zero_() !!&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# Requires the original value of out, but that was overwritten by c.zero_()&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;RuntimeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variables&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;needed&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;computation&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;been&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modified&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, using &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor.data&lt;/code&gt; can be unsafe and can easily result in incorrect gradients when a tensor is required for gradient computation but modified in-place.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# out  was modified by c.zero_()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# The result is very, very wrong because `out` changed!&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;support-for-0-dimensional-scalar-tensors&quot;&gt;Support for 0-dimensional (scalar) Tensors&lt;/h2&gt;

&lt;p&gt;Previously, indexing into a &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; vector (1-dimensional tensor) gave a Python number but indexing into a &lt;code class=&quot;highlighter-rouge&quot;&gt;Variable&lt;/code&gt; vector gave (inconsistently!) a vector of size &lt;code class=&quot;highlighter-rouge&quot;&gt;(1,)&lt;/code&gt;! Similar behavior existed with reduction functions, e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;tensor.sum()&lt;/code&gt; would return a Python number, but &lt;code class=&quot;highlighter-rouge&quot;&gt;variable.sum()&lt;/code&gt; would return a vector of size &lt;code class=&quot;highlighter-rouge&quot;&gt;(1,)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Fortunately, this release introduces proper scalar (0-dimensional tensor) support in PyTorch! Scalars can be created using the new &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.tensor&lt;/code&gt; function (which will be explained in more detail later; for now just think of it as the PyTorch equivalent of &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy.array&lt;/code&gt;). Now you can do things like:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.1416&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# create a scalar directly&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.1416&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.1416&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# scalar is 0-dimensional&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# compare to a vector of size 1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# this is a vector&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;# indexing into a vector gives a scalar&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;             &lt;span class=&quot;c&quot;&gt;# .item() gives the value as a Python number&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mysum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mysum&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mysum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;accumulating-losses&quot;&gt;Accumulating losses&lt;/h3&gt;

&lt;p&gt;Consider the widely used pattern &lt;code class=&quot;highlighter-rouge&quot;&gt;total_loss += loss.data[0]&lt;/code&gt;. Before 0.4.0. &lt;code class=&quot;highlighter-rouge&quot;&gt;loss&lt;/code&gt; was a &lt;code class=&quot;highlighter-rouge&quot;&gt;Variable&lt;/code&gt; wrapping a tensor of size &lt;code class=&quot;highlighter-rouge&quot;&gt;(1,)&lt;/code&gt;, but in 0.4.0 &lt;code class=&quot;highlighter-rouge&quot;&gt;loss&lt;/code&gt; is now a scalar and has &lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt; dimensions. Indexing into a scalar doesn’t make sense (it gives a warning now, but will be a hard error in 0.5.0). Use &lt;code class=&quot;highlighter-rouge&quot;&gt;loss.item()&lt;/code&gt; to get the Python number from a scalar.&lt;/p&gt;

&lt;p&gt;Note that if you don’t convert to a Python number when accumulating losses, you may find increased memory usage in your program. This is because the right-hand-side of the above expression used to be a Python float, while it is now a zero-dim Tensor. The total loss is thus accumulating Tensors and their gradient history, which may keep around large autograd graphs for much longer than necessary.&lt;/p&gt;

&lt;h2 id=&quot;deprecation-of-volatile-flag&quot;&gt;Deprecation of volatile flag&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;volatile&lt;/code&gt; flag is now deprecated and has no effect. Previously, any computation that involves a &lt;code class=&quot;highlighter-rouge&quot;&gt;Variable&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;volatile=True&lt;/code&gt; wouldn’t be tracked by &lt;code class=&quot;highlighter-rouge&quot;&gt;autograd&lt;/code&gt;. This has now been replaced by a &lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation&quot;&gt;set of more flexible context managers&lt;/a&gt; including &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.no_grad()&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.set_grad_enabled(grad_mode)&lt;/code&gt;, and others.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_grad_enabled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_grad_enabled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# this can also be used as a function&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_grad_enabled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;dtypes-devices-and-numpy-style-creation-functions&quot;&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dtypes&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;devices&lt;/code&gt;&lt;/a&gt; and NumPy-style creation functions&lt;/h2&gt;

&lt;p&gt;In previous versions of PyTorch, we used to specify data type (e.g. float vs double), device type (cpu vs cuda) and layout (dense vs sparse) together as a “tensor type”. For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.sparse.DoubleTensor&lt;/code&gt; was the &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; type representing the &lt;code class=&quot;highlighter-rouge&quot;&gt;double&lt;/code&gt; data type, living on CUDA devices, and with &lt;a href=&quot;https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)&quot;&gt;COO sparse tensor&lt;/a&gt; layout.&lt;/p&gt;

&lt;p&gt;In this release, we introduce &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.device&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.layout&lt;/code&gt;&lt;/a&gt; classes to allow better management of these properties via NumPy-style creation functions.&lt;/p&gt;

&lt;h3 id=&quot;torchdtype&quot;&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Below is a complete list of available &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;s (data types) and their corresponding tensor types.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Data&lt;/th&gt;
      &lt;th&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;type torch.dtype&lt;/code&gt;&lt;/th&gt;
      &lt;th&gt;Tensor types&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;32-bit floating point&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.float32&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.float&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*.FloatTensor&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;64-bit floating point&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.float64&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.double&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*.DoubleTensor&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;16-bit floating point&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.float16&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.half&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*.HalfTensor&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8-bit integer (unsigned)&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.uint8&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*.ByteTensor&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8-bit integer (signed)&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.int8&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*.CharTensor&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;16-bit integer (signed)&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.int16&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.short&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*.ShortTensor&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;32-bit integer (signed)&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.int32&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.int&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*.IntTensor&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;64-bit integer (signed)&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.int64&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.long&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*.LongTensor&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The dtype of a tensor can be access via its &lt;code class=&quot;highlighter-rouge&quot;&gt;dtype&lt;/code&gt; attribute.&lt;/p&gt;

&lt;h3 id=&quot;torchdevice&quot;&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.device&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;A &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.device&lt;/code&gt;&lt;/a&gt; contains a device type (&lt;code class=&quot;highlighter-rouge&quot;&gt;'cpu'&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;'cuda'&lt;/code&gt;) and optional device ordinal (id) for the device type. It can be initialized with &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.device('{device_type}')&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.device('{device_type}:{device_ordinal}')&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If the device ordinal is not present, this represents the current device for the device type; e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.device('cuda')&lt;/code&gt; is equivalent to &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.device('cuda:X')&lt;/code&gt; where &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; is the result of &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.current_device()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The device of a tensor can be accessed via its &lt;code class=&quot;highlighter-rouge&quot;&gt;device&lt;/code&gt; attribute.&lt;/p&gt;

&lt;h3 id=&quot;torchlayout&quot;&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.layout&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.layout&lt;/code&gt;&lt;/a&gt; represents the data layout of a &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensors.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt;&lt;/a&gt;. Currently &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.strided&lt;/code&gt; (dense tensors, the default) and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.sparse_coo&lt;/code&gt; (sparse tensors with COO format) are supported.&lt;/p&gt;

&lt;p&gt;The layout of a tensor can be access via its &lt;code class=&quot;highlighter-rouge&quot;&gt;layout&lt;/code&gt; attribute.&lt;/p&gt;

&lt;h3 id=&quot;creating-tensors&quot;&gt;Creating Tensors&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#creation-ops&quot;&gt;Methods that create a&lt;/a&gt; &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensors.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt;&lt;/a&gt; now also take in &lt;code class=&quot;highlighter-rouge&quot;&gt;dtype&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;device&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;layout&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;requires_grad&lt;/code&gt; options to specify the desired attributes on the returned &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt;. For example,&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda:1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6344&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.8562&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.2758&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8414&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.7962&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.0589&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1369&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0462&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4373&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda:1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# default is False&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;torchtensordata-&quot;&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.tensor&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.tensor(data, ...)&lt;/code&gt;&lt;/a&gt;&lt;/h5&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.tensor&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.tensor&lt;/code&gt;&lt;/a&gt; is one of the newly added &lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#creation-ops&quot;&gt;tensor creation methods&lt;/a&gt;. It takes in array-like data of all kinds and copies the contained values into a new &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt;. As mentioned earlier, &lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.tensor&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.tensor&lt;/code&gt;&lt;/a&gt; is the PyTorch equivalent of NumPy’s &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy.array&lt;/code&gt;constructor. Unlike the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*Tensor&lt;/code&gt; methods, you can also create zero-dimensional &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt;s (aka scalars) this way (a single python number is treated as a Size in the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*Tensor methods&lt;/code&gt;). Moreover, if a &lt;code class=&quot;highlighter-rouge&quot;&gt;dtype&lt;/code&gt; argument isn’t given, it will infer the suitable &lt;code class=&quot;highlighter-rouge&quot;&gt;dtype&lt;/code&gt; given the data. It is the recommended way to create a tensor from existing data like a Python list. For example,&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda:0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;               &lt;span class=&quot;c&quot;&gt;# scalar&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# type inferece&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# type inferece&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int64&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We’ve also added more tensor creation methods. Some of them have &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*_like&lt;/code&gt; and/or &lt;code class=&quot;highlighter-rouge&quot;&gt;tensor.new_*&lt;/code&gt; variants.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*_like&lt;/code&gt; takes in an input &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; instead of a shape. It returns a &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; with same attributes as the input &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; by default unless otherwise specified:&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tensor.new_*&lt;/code&gt; can also create &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensors&lt;/code&gt; with same attributes as &lt;code class=&quot;highlighter-rouge&quot;&gt;tensor&lt;/code&gt;, but it always takes in a shape argument:&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To specify the desired shape, you can either use a tuple (e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.zeros((2, 3))&lt;/code&gt;) or variable arguments (e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.zeros(2, 3)&lt;/code&gt;) in most cases.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Returned &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor&lt;/code&gt;&lt;/th&gt;
      &lt;th&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.*_like&lt;/code&gt; variant&lt;/th&gt;
      &lt;th&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tensor.new_*&lt;/code&gt; variant&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.empty&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.empty&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;uninitialized memory&lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.zeros&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.zeros&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;all zeros&lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.ones&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.ones&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;all ones&lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.full&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.full&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;filled with a given value&lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.rand&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.rand&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;i.i.d. continuous Uniform[0, 1)&lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.randn&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.randn&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;i.i.d. &lt;code class=&quot;highlighter-rouge&quot;&gt;Normal(0, 1)&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.randint&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.randint&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;i.i.d. discrete Uniform in given range&lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.randperm&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.randperm&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;random permutation of &lt;code class=&quot;highlighter-rouge&quot;&gt;{0, 1, ..., n - 1}&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.tensor&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.tensor&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;copied from existing data (list, NumPy ndarray, etc.)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;✔&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.from_numpy&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.from_numpy&lt;/code&gt;*&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;from NumPy &lt;code class=&quot;highlighter-rouge&quot;&gt;ndarray&lt;/code&gt; (sharing storage without copying)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.arange&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.arange&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.range&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.range&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.linspace&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.linspace&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;uniformly spaced values in a given range&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.logspace&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.logspace&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;logarithmically spaced values in a given range&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.eye&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.eye&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;identity matrix&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;*: &lt;a href=&quot;https://pytorch.org/docs/0.4.0/torch.html#torch.from_numpy&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.from_numpy&lt;/code&gt;&lt;/a&gt; only takes in a NumPy &lt;code class=&quot;highlighter-rouge&quot;&gt;ndarray&lt;/code&gt; as its input argument.&lt;/p&gt;

&lt;h2 id=&quot;writing-device-agnostic-code&quot;&gt;Writing device-agnostic code&lt;/h2&gt;

&lt;p&gt;Previous versions of PyTorch made it difficult to write code that was device agnostic (i.e. that could run on both CUDA-enabled and CPU-only machines without modification).&lt;/p&gt;

&lt;p&gt;PyTorch 0.4.0 makes this easier in two ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;device&lt;/code&gt; attribute of a Tensor gives the &lt;a href=&quot;https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device&quot;&gt;torch.device&lt;/a&gt; for all Tensors (&lt;code class=&quot;highlighter-rouge&quot;&gt;get_device&lt;/code&gt; only works for CUDA tensors)&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;to&lt;/code&gt; method of &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensors&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Modules&lt;/code&gt; can be used to easily move objects to different devices (instead of having to call &lt;code class=&quot;highlighter-rouge&quot;&gt;cpu()&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;cuda()&lt;/code&gt; based on the context)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We recommend the following pattern:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# at beginning of the script&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda:0&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cpu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# then whenever you get a new Tensor or Module&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# this won't copy if they are already on the desired device&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;new-edge-case-constraints-on-names-of-submodules-parameters-and-buffers-in-nnmodule&quot;&gt;New edge-case constraints on names of submodules, parameters, and buffers in &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt; that is an empty string or contains &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;.&quot;&lt;/code&gt; is no longer permitted in &lt;code class=&quot;highlighter-rouge&quot;&gt;module.add_module(name, value)&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;module.add_parameter(name, value)&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;module.add_buffer(name, value)&lt;/code&gt; because such names may cause lost data in the &lt;code class=&quot;highlighter-rouge&quot;&gt;state_dict&lt;/code&gt;. If you are loading a checkpoint for modules containing such names, please update the module definition and patch the &lt;code class=&quot;highlighter-rouge&quot;&gt;state_dict&lt;/code&gt; before loading it.&lt;/p&gt;

&lt;h2 id=&quot;code-samples-putting-it-all-together&quot;&gt;Code Samples (Putting it all together)&lt;/h2&gt;

&lt;p&gt;To get a flavor of the overall recommended changes in 0.4.0, let’s look at a quick example for a common code pattern in both 0.3.1 and 0.4.0:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;0.3.1 (old):
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyRNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;use_cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# train&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# init hidden&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;use_cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# get loss and optimize&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# evaluate&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;volatile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;use_cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;0.4.0 (new):
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# torch.device object used throughout this script&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;use_cuda&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cpu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyRNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# train&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# has the same device &amp;amp; dtype as `input`&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# get loss and optimize&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;           &lt;span class=&quot;c&quot;&gt;# get Python number from 1-element Tensor&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# evaluate&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;                   &lt;span class=&quot;c&quot;&gt;# operations inside don't track history&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for reading! Please refer to our &lt;a href=&quot;https://pytorch.org/docs/0.4.0/index.html&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v0.4.0&quot;&gt;release notes&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Happy PyTorch-ing!&lt;/p&gt;</content><author><name>Facebook</name></author><summary type="html">Welcome to the migration guide for PyTorch 0.4.0. In this release we introduced many exciting new features and critical bug fixes, with the goal of providing users a better and cleaner interface. In this guide, we will cover the most important changes in migrating existing code from previous versions:</summary></entry><entry><title type="html">Tensor Comprehensions in PyTorch</title><link href="https://pytorch.org/blog/tensor-comprehensions/" rel="alternate" type="text/html" title="Tensor Comprehensions in PyTorch" /><published>2018-03-05T00:00:00-08:00</published><updated>2018-03-05T00:00:00-08:00</updated><id>https://pytorch.org/blog/tensor-comprehensions</id><content type="html" xml:base="https://pytorch.org/blog/tensor-comprehensions/">&lt;p&gt;Tensor Comprehensions (TC) is a tool that lowers the barrier for writing high-performance code. It generates GPU code from a simple high-level language and autotunes the code for specific input sizes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We highly recommend reading the &lt;a href=&quot;https://research.fb.com/announcing-tensor-comprehensions/&quot;&gt;Tensor Comprehensions blogpost&lt;/a&gt; first.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you ran into any of the following scenarios, TC is a useful tool for you.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Your PyTorch layer is large and slow, and you contemplated writing a dedicated C++ or CUDA code for it. But you don’t know how to program in CUDA or write low-level code.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You wrote a CUDA layer, but it took a week to write, debug, optimize for speed. You wished you could do this in an hour.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You want to fuse multiple layers like Conv-ReLU-BatchNorm or Linear-ReLU-Linear-ReLU in your network for speed, but it was quite difficult to comprehend&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Your research involves weird Tensor shapes that CuDNN and MKL are not optimized for. For example, you do convolutions of 13 x 24 with an input image of 143 x 55. You tried running it with CuDNN and it was slower than you wished.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Your code is slowed-down by transposing Tensors constantly to fit a particular memory layout. You wish it was easy to write custom code that operates efficiently on your input layout.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tensor Comprehensions are seamless to use in PyTorch, interoperating with PyTorch Tensors and &lt;code class=&quot;highlighter-rouge&quot;&gt;nn&lt;/code&gt; Variables.&lt;/p&gt;

&lt;p&gt;Let us run through using TC with PyTorch.&lt;/p&gt;

&lt;h4 id=&quot;1-install-the-package&quot;&gt;1. Install the package&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; pytorch &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; tensorcomp tensor_comprehensions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this time we only provide Linux-64 binaries which have been tested on Ubuntu 16.04 and CentOS7.&lt;/p&gt;

&lt;p&gt;TC depends on heavyweight C++ projects such as &lt;a href=&quot;http://halide-lang.org/&quot;&gt;Halide&lt;/a&gt;, &lt;a href=&quot;https://github.com/wsmoses/Tapir-LLVM&quot;&gt;Tapir-LLVM&lt;/a&gt; and &lt;a href=&quot;http://isl.gforge.inria.fr/&quot;&gt;ISL&lt;/a&gt;. Hence, we rely on Anaconda to distribute these dependencies reliably. For the same reason, TC is not available via PyPI.&lt;/p&gt;

&lt;h4 id=&quot;2-import-the-python-package&quot;&gt;2. Import the python package&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensor_comprehensions&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3-define-the-tc-expression-and-create-a-python-function&quot;&gt;3. Define the TC expression and create a python function&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;lang&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
def fcrelu(float(B,M) I, float(N,M) W1, float(N) B1) -&amp;gt; (O1) {
    O1(b, n) +=! I(b, m) * W1(n, m)
    O1(b, n) = O1(b, n) + B1(n)
    O1(b, n) = fmax(O1(b, n), 0)
}
&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fcrelu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;define&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lang&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fcrelu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This &lt;code class=&quot;highlighter-rouge&quot;&gt;fcrelu&lt;/code&gt; function takes PyTorch Tensors as input and returns a PyTorch Tensor. It takes input &lt;code class=&quot;highlighter-rouge&quot;&gt;I&lt;/code&gt;, weight &lt;code class=&quot;highlighter-rouge&quot;&gt;W1&lt;/code&gt;, bias &lt;code class=&quot;highlighter-rouge&quot;&gt;B1&lt;/code&gt; and returns output &lt;code class=&quot;highlighter-rouge&quot;&gt;O1&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;4-lets-create-some-dummy-input-tensors&quot;&gt;4. Let’s create some dummy input tensors&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;5-now-autotune-the-function-for-your-input-sizes&quot;&gt;5. Now autotune the function for your input sizes&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fcrelu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autotune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fcrelu_100_128_100.tc&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The autotuner is your biggest friend. You generally do not want to use a &lt;code class=&quot;highlighter-rouge&quot;&gt;tc&lt;/code&gt; function without autotuning it first.&lt;/p&gt;

&lt;p&gt;When the autotuning is running, the current best performance is displayed. If you are satisfied with the current result or you are out of time, stop the tuning procedure by pressing &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl+C&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pytorch.org/static/img/tc_autotuner.gif&quot; alt=&quot;tc-autotuner&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cache&lt;/code&gt; saves the results of the autotuned kernel search and saves it to the file &lt;code class=&quot;highlighter-rouge&quot;&gt;fcrelu_100_128_100.tc&lt;/code&gt;. The next time you call the same line of code, it loads the results of the autotuning without recomputing it.&lt;/p&gt;

&lt;p&gt;The autotuner has a few hyperparameters (just like your ConvNet has learning rate, number of layers, etc.). We pick reasonable defaults, but you can read about using advanced options &lt;a href=&quot;https://facebookresearch.github.io/TensorComprehensions/framework/pytorch_integration/writing_layers.html#specifying-mapping-options&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;6-call-the-function-with-the-inputs-to-get-your-result&quot;&gt;6. Call the function with the inputs, to get your result&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fcrelu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, let’s look at how to write TC expressions.&lt;/p&gt;

&lt;h2 id=&quot;a-quick-primer-on-the-tc-language&quot;&gt;A quick primer on the TC language&lt;/h2&gt;

&lt;p&gt;The TC notation focuses on the mathematical nature of the layer, leaving performance considerations to it’s backend code that uses Halide and polyhedral compilation techniques which accumulate decades of cutting edge Loop Nest Optimization (LNO) research.&lt;/p&gt;

&lt;p&gt;TC is close to &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html&quot;&gt;np.einsum&lt;/a&gt;. We shall quickly learn TC by example&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;lang&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
def matmul(float(M,N) A, float(N,K) B) -&amp;gt; (output) {
  output(i, j) +=! A(i, kk) * B(kk, j)
}
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this example, we define a function &lt;code class=&quot;highlighter-rouge&quot;&gt;matmul&lt;/code&gt; which takes two input &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;B&lt;/code&gt; of shapes &lt;code class=&quot;highlighter-rouge&quot;&gt;M x N&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;N x K&lt;/code&gt; and returns a single &lt;code class=&quot;highlighter-rouge&quot;&gt;output&lt;/code&gt;. The shape of &lt;code class=&quot;highlighter-rouge&quot;&gt;output&lt;/code&gt; is automatically inferred by the TC language (discussed below).&lt;/p&gt;

&lt;p&gt;Let’s look at this line:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;!&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It says:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;output(i, j)&lt;/code&gt; means output is 2D.&lt;/li&gt;
  &lt;li&gt;for each location &lt;code class=&quot;highlighter-rouge&quot;&gt;output(i, j)&lt;/code&gt;, we add (&lt;code class=&quot;highlighter-rouge&quot;&gt;+=&lt;/code&gt;) &lt;code class=&quot;highlighter-rouge&quot;&gt;A(i, kk) * B(kk, j)&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; is well-defined as all locations in &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; dim=0, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;i in range(0, M)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;j&lt;/code&gt; is well-defined as all locations in &lt;code class=&quot;highlighter-rouge&quot;&gt;B&lt;/code&gt; dim=1, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;j in range(0, K)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kk&lt;/code&gt; is inferred as all locations from &lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The shape of output is inferred from the maximum values &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;j&lt;/code&gt; can take, which is &lt;code class=&quot;highlighter-rouge&quot;&gt;M&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;K&lt;/code&gt;, so output is of size &lt;code class=&quot;highlighter-rouge&quot;&gt;M x K&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;!&lt;/code&gt; symbol initializes output with &lt;code class=&quot;highlighter-rouge&quot;&gt;0.0&lt;/code&gt;. It is equivalent to:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Scalar inputs and range constraints: implement AvgPool2d&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;

def avgpool(float(B, C, H, W) input) -&amp;gt; (output) {{
  output(b, c, h, w) += input(b, c, h * {sH} + kh, w * {sW} + kw) where kh in 0:{kH}, kw in 0:{kW}
}}

&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;avgpool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;define&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LANG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;avgpool&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constants&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sH&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sW&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;kH&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;kW&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;here the &lt;code class=&quot;highlighter-rouge&quot;&gt;where&lt;/code&gt; keyword can take ranges of values to operate on. &lt;code class=&quot;highlighter-rouge&quot;&gt;0:{kH}&lt;/code&gt; is equivalent &lt;code class=&quot;highlighter-rouge&quot;&gt;range(kH)&lt;/code&gt; in Python.&lt;/p&gt;

&lt;p&gt;Note: the syntax for passing in scalars is subject to change in the next release.&lt;/p&gt;

&lt;h2 id=&quot;torchnn-layers&quot;&gt;torch.nn layers&lt;/h2&gt;

&lt;p&gt;We added some sugar-coating around the basic PyTorch integration of TC to make it easy to integrate TC into larger &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn&lt;/code&gt; models by defining the forward and backward TC expressions and taking &lt;code class=&quot;highlighter-rouge&quot;&gt;Variable&lt;/code&gt; inputs / outputs. Here is an &lt;a href=&quot;https://github.com/facebookresearch/TensorComprehensions/blob/master/test_python/layers/test_convolution_train.py&quot;&gt;example&lt;/a&gt; of defining a convolution layer with TC.&lt;/p&gt;

&lt;h2 id=&quot;some-essentials-that-you-will-miss-were-working-on-them&quot;&gt;Some essentials that you will miss (we’re working on them)&lt;/h2&gt;

&lt;h3 id=&quot;autotuning-for-variable-length-sequences&quot;&gt;Autotuning for variable-length sequences&lt;/h3&gt;

&lt;p&gt;The TC auto-tuner requires all input sizes to be specified before-hand. For example, if you have input &lt;code class=&quot;highlighter-rouge&quot;&gt;I1&lt;/code&gt; which is an image batch, the autotuner wants to know the exact shape of &lt;code class=&quot;highlighter-rouge&quot;&gt;I1&lt;/code&gt; to generate an optimized kernel. You cannot specify: &lt;code class=&quot;highlighter-rouge&quot;&gt;image with height between 200 and 300&lt;/code&gt;. This is more essential in sequence data such as NLP, where each sentence can have a different length.&lt;/p&gt;

&lt;p&gt;The reason why the autotuner is non-parametric is because it’s harder and harder to auto-tune parametric constraints, this is active research. Hence, for the first release, we made a conscious decision to give you the tool in a form where we know it works well.&lt;/p&gt;

&lt;p&gt;As a work-around, if you know that you have a few specific shapes of interest, you can run the autotuner with these multiple shapes.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;define&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LANG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autotune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# image of size 32 x 32&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autotune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# image of size 48 x 48&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autotune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# image of size 64 x 64&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now the autotuner is tuned for these three specific image sizes &lt;code class=&quot;highlighter-rouge&quot;&gt;32x32&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;48x48&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;64x64&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;lack-of-loops&quot;&gt;Lack of loops&lt;/h3&gt;

&lt;p&gt;If you want to write an RNN, it’s easy to see it as a &lt;code class=&quot;highlighter-rouge&quot;&gt;for&lt;/code&gt; loop over time. However, the TC language does not have loops yet. If you really want to write RNNs, you can write unrolled loops.&lt;/p&gt;

&lt;h3 id=&quot;strided-tensors&quot;&gt;Strided-Tensors&lt;/h3&gt;

&lt;p&gt;The TC backend does not support non-contiguous Tensors yet. If the inputs you give are not contiguous, they are made contiguous before passing to the TC backend.&lt;/p&gt;

&lt;h3 id=&quot;reshaping-tensors-within-a-tc-expression&quot;&gt;Reshaping Tensors within a TC expression&lt;/h3&gt;

&lt;p&gt;You cannot write this operation in TC: &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.matmul(...).view(...).mean(...)&lt;/code&gt;. Whenever there is need for a &lt;code class=&quot;highlighter-rouge&quot;&gt;view&lt;/code&gt; to change the shape of an input, you have to get the output, &lt;code class=&quot;highlighter-rouge&quot;&gt;view&lt;/code&gt; it at the PyTorch level.&lt;/p&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting Started&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://facebookresearch.github.io/TensorComprehensions/framework/pytorch_integration/writing_layers.html&quot;&gt;Walk through Tutorial&lt;/a&gt; to quickly get started with understanding and using Tensor Comprehensions PyTorch package.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/facebookresearch/TensorComprehensions/tree/master/test_python/layers&quot;&gt;Over 20 examples&lt;/a&gt; of various ML layers with TC, including &lt;code class=&quot;highlighter-rouge&quot;&gt;avgpool&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;maxpool&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;matmul&lt;/code&gt;, matmul - give output buffers and &lt;code class=&quot;highlighter-rouge&quot;&gt;batch-matmul&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;convolution&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;strided-convolution&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;batchnorm&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;copy&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;cosine similarity&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Linear&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Linear + ReLU&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;group-convolutions&lt;/code&gt;, strided &lt;code class=&quot;highlighter-rouge&quot;&gt;group-convolutions&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;indexing&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Embedding&lt;/code&gt; (lookup table), small-mobilenet, &lt;code class=&quot;highlighter-rouge&quot;&gt;softmax&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;tensordot&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;transpose&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://facebookresearch.github.io/TensorComprehensions/framework/pytorch_integration/getting_started.html&quot;&gt;Detailed docs&lt;/a&gt; on Tensor Comprehensions and integration with PyTorch.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;communication&quot;&gt;Communication&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://tensorcomprehensions.herokuapp.com/&quot;&gt;Slack&lt;/a&gt;: For discussion around framework integration, build support, collaboration, etc. join our slack channel.&lt;/li&gt;
  &lt;li&gt;Email: tensorcomp@fb.com&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/facebookresearch/TensorComprehensions&quot;&gt;GitHub&lt;/a&gt;: bug reports, feature requests, install issues, RFCs, thoughts, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We would like to thank Soumith Chintala, &lt;a href=&quot;https://github.com/ezyang&quot;&gt;Edward Yang&lt;/a&gt; and &lt;a href=&quot;https://github.com/colesbury&quot;&gt;Sam Gross&lt;/a&gt; for their immense guidance and help in making the integration API nice and smooth. We would also like to thank rest of the PyTorch team and our pre-release users for their helpful feedback that guided us in making the integration better.&lt;/p&gt;</content><author><name>Priya Goyal (FAIR), Nicolas Vasilache (FAIR), Oleksandr Zinenko (Inria &amp; DI ENS), Theodoros Theodoridis (ETH Zürich), Zachary DeVito (FAIR), William S. Moses (MIT CSAIL), Sven Verdoolaege (FAIR), Andrew Adams (FAIR), Albert Cohen (Inria &amp; DI ENS &amp; FAIR)</name></author><summary type="html">Tensor Comprehensions (TC) is a tool that lowers the barrier for writing high-performance code. It generates GPU code from a simple high-level language and autotunes the code for specific input sizes.</summary></entry><entry><title type="html">PyTorch, a year in….</title><link href="https://pytorch.org/blog/a-year-in/" rel="alternate" type="text/html" title="PyTorch, a year in...." /><published>2018-01-19T09:00:00-08:00</published><updated>2018-01-19T09:00:00-08:00</updated><id>https://pytorch.org/blog/a-year-in</id><content type="html" xml:base="https://pytorch.org/blog/a-year-in/">&lt;p&gt;Today marks 1 year since PyTorch was released publicly. It’s been a wild ride — our quest to build a flexible deep learning research platform. Over the last year, we’ve seen an amazing community of people using, contributing to and evangelizing PyTorch — thank you for the love.&lt;/p&gt;

&lt;p&gt;Looking back, we wanted to summarize PyTorch over the past year: the progress, the news and highlights from the community.&lt;/p&gt;

&lt;h2 id=&quot;community&quot;&gt;Community&lt;/h2&gt;

&lt;p&gt;We’ve been blessed with a strong organic community of researchers and engineers who fell in love with PyTorch. The core team has engineers and researchers from multiple countries, companies and universities, and we couldn’t have made PyTorch what it is without each contribution.&lt;/p&gt;

&lt;h3 id=&quot;research-papers-packages-and-github&quot;&gt;Research papers, packages and Github&lt;/h3&gt;

&lt;p&gt;Within days of release, users from the community started to implement their favorite research papers in PyTorch and release the code on Github. Open-source code is a primary and essential tool for researchers today.&lt;/p&gt;

&lt;p&gt;Folks came together to create &lt;a href=&quot;https://github.com/pytorch/text&quot;&gt;torchtext&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/vision&quot;&gt;torchvision&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/audio&quot;&gt;torchaudio&lt;/a&gt; packages to help facilitate and democratize research in different domains.&lt;/p&gt;

&lt;p&gt;The first community package based on PyTorch came from Brandon Amos, &lt;a href=&quot;https://twitter.com/brandondamos/status/828652480573607937&quot;&gt;titled Block&lt;/a&gt;, and helped with easier manipulation of block matrices. The Locus Lab at &lt;strong&gt;CMU&lt;/strong&gt; subsequently went on to &lt;a href=&quot;https://github.com/locuslab&quot;&gt;publish PyTorch packages&lt;/a&gt; and implementations for most of their research. The first research paper code came from Sergey Zagoruyko titled &lt;a href=&quot;https://twitter.com/PyTorch/status/822561885744726016&quot;&gt;Paying more attention to attention&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jun-Yan Zhu, Taesung Park, Phillip Isola, Alyosha Efros and team from &lt;strong&gt;U.C.Berkeley&lt;/strong&gt; released the hugely popular &lt;a href=&quot;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&quot;&gt;Cycle-GAN and pix2pix&lt;/a&gt; which does image to image transforms.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/horse2zebra.gif&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The researchers at &lt;strong&gt;HarvardNLP&lt;/strong&gt; and &lt;strong&gt;Systran&lt;/strong&gt; started developing and improving &lt;a href=&quot;https://github.com/OpenNMT/OpenNMT-py&quot;&gt;OpenNMT in PyTorch&lt;/a&gt;, seeded by initial reimplementation of the [Lua]Torch code from Adam Lerer at Facebook.&lt;/p&gt;

&lt;p&gt;The MagicPony team at &lt;strong&gt;Twitter&lt;/strong&gt; contributed implementations of their &lt;a href=&quot;https://twitter.com/Rob_Bishop/status/821793080877588480&quot;&gt;Super-resolution work early on into PyTorch’s examples&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Salesforce Research&lt;/strong&gt; released several packages, including their highlight release of &lt;a href=&quot;https://twitter.com/Smerity/status/917472260851560448&quot;&gt;PyTorch-QRNN&lt;/a&gt;, a type of RNN that is 2x to 17x faster than standard LSTMs optimized by CuDNN. James Bradbury and team form one of the most active and engaging forces in the PyTorch community.&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;We&amp;#39;re releasing &lt;a href=&quot;https://twitter.com/PyTorch?ref_src=twsrc%5Etfw&quot;&gt;@PyTorch&lt;/a&gt;-QRNN, 2-17x faster than NVIDIA&amp;#39;s cuDNN LSTM.&lt;br /&gt;Speed thanks to 50 lines of CUDA via CuPy.&lt;a href=&quot;https://t.co/KaWhN4yDZd&quot;&gt;https://t.co/KaWhN4yDZd&lt;/a&gt; &lt;a href=&quot;https://t.co/yoLYj3pMI0&quot;&gt;pic.twitter.com/yoLYj3pMI0&lt;/a&gt;&lt;/p&gt;&amp;mdash; Smerity (@Smerity) &lt;a href=&quot;https://twitter.com/Smerity/status/917472260851560448?ref_src=twsrc%5Etfw&quot;&gt;October 9, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;Researchers from &lt;strong&gt;Uber&lt;/strong&gt;, &lt;strong&gt;Northeastern&lt;/strong&gt; and &lt;strong&gt;Stanford&lt;/strong&gt; came together to form an active probabilistic programming community around their packages &lt;a href=&quot;http://pyro.ai/&quot;&gt;Pyro&lt;/a&gt; and &lt;a href=&quot;https://github.com/probtorch/probtorch&quot;&gt;ProbTorch&lt;/a&gt;. They are actively developing the torch.distributions core package. This community is so active and fast-moving, we had our first pytorch-probabilistic-programming meetup at NIPS 2017 with Fritz Obermeyer, Noah Goodman, Jan-Willem van de Meent, Brooks Paige, Dustin Tran and 22 additional attendees discussing how to make the world bayesian.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/probpackages.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;NVIDIA&lt;/strong&gt; Researchers released three high-quality repositories that implemented &lt;a href=&quot;https://github.com/NVIDIA/pix2pixHD&quot;&gt;pix2pix-HD&lt;/a&gt;, &lt;a href=&quot;https://github.com/NVIDIA/sentiment-discovery&quot;&gt;Sentiment Neuron&lt;/a&gt; and &lt;a href=&quot;https://github.com/NVIDIA/flownet2-pytorch&quot;&gt;FlowNet2&lt;/a&gt; papers. Their analysis of scalability of different &lt;a href=&quot;https://github.com/NVIDIA/sentiment-discovery/blob/master/analysis/scale.md&quot;&gt;Data Parallel models in PyTorch&lt;/a&gt; was helpful to the community.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/sentiment.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The Allen Institute for AI released &lt;a href=&quot;http://allennlp.org/&quot;&gt;AllenNLP&lt;/a&gt; which includes several state-of-the-art models in NLP — reference implementations and easy to use &lt;a href=&quot;http://demo.allennlp.org/machine-comprehension&quot;&gt;web demos&lt;/a&gt; for standard NLP tasks.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/allennlp.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We also had our first Kaggle winning team grt123 in July. They won the DataScience Bowl 2017 on Lung Cancer detection and &lt;a href=&quot;https://twitter.com/PyTorch/status/881573658166267904&quot;&gt;subsequently released their PyTorch implementations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On the visualization front, Tzu-Wei Huang implemented a &lt;a href=&quot;https://github.com/lanpa/tensorboard-pytorch&quot;&gt;TensorBoard-PyTorch plugin&lt;/a&gt; and Facebook AI Research released PyTorch compatibility for their &lt;a href=&quot;https://github.com/facebookresearch/visdom&quot;&gt;visdom&lt;/a&gt; visualization package.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/tensorboard_model.png&quot; width=&quot;40%&quot; /&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/visdom.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Lastly, &lt;strong&gt;Facebook AI Research&lt;/strong&gt; released several projects such as &lt;a href=&quot;https://github.com/facebookresearch/&quot;&gt;ParlAI, fairseq-py, VoiceLoop and FaderNetworks&lt;/a&gt; that implemented cutting-edge models and interfaced datasets in multiple domains.&lt;/p&gt;

&lt;p&gt;There are countless good projects that we haven’t highlighted for the lack of space, you can find a curated list &lt;a href=&quot;https://github.com/soumith?tab=stars&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We would also like to give a huge shout-out to folks who actively help others out on the Forums, especially &lt;a href=&quot;https://discuss.pytorch.org/u/ptrblck/summary&quot;&gt;ptrblck&lt;/a&gt;, &lt;a href=&quot;https://discuss.pytorch.org/u/jpeg729/summary&quot;&gt;jpeg729&lt;/a&gt;, &lt;a href=&quot;https://discuss.pytorch.org/u/quantscientist/summary&quot;&gt;QuantScientist&lt;/a&gt;, &lt;a href=&quot;https://discuss.pytorch.org/u/alband/summary&quot;&gt;albanD&lt;/a&gt;, &lt;a href=&quot;https://discuss.pytorch.org/u/tom/summary&quot;&gt;Thomas Viehmann&lt;/a&gt; and &lt;a href=&quot;https://discuss.pytorch.org/u/chenyuntc/summary&quot;&gt;chenyuntc&lt;/a&gt;. You are providing an invaluable service, thank you so much!&lt;/p&gt;

&lt;h2 id=&quot;metrics&quot;&gt;Metrics&lt;/h2&gt;

&lt;p&gt;In terms of sheer numbers,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;87,769 lines of Python code on github that &lt;a href=&quot;https://github.com/search?l=Python&amp;amp;q=import+torch&amp;amp;type=Code&quot;&gt;import torch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/search?q=pytorch&amp;amp;type=Repositories&quot;&gt;3,983 repositories on Github that mention PyTorch in their name or description&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;More than half a million downloads of PyTorch binaries. 651,916 to be precise.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;5,400 users&lt;/strong&gt; wrote &lt;strong&gt;21,500 posts&lt;/strong&gt; discussing 5,200 topics on our forums discuss.pytorch.org (http://discuss.pytorch.org/)&lt;/li&gt;
  &lt;li&gt;131 mentions of PyTorch on Reddit’s /r/machinelearning since the day of release. In the same period, TensorFlow was mentioned 255 times.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;research-metrics&quot;&gt;Research Metrics&lt;/h3&gt;

&lt;p&gt;PyTorch is a research-focused framework. So one of the metrics of interest is to see the usage of PyTorch in machine learning research papers.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the recent ICLR2018 conference submissions, PyTorch was mentioned in &lt;strong&gt;87 papers&lt;/strong&gt;, compared to TensorFlow at 228 papers, Keras at 42 papers, Theano and Matlab at 32 papers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/fchollet/status/951828914103402497&quot;&gt;Monthly arxiv.org mentions for frameworks&lt;/a&gt; had PyTorch at 72 mentions, with TensorFlow at 273 mentions, Keras at 100 mentions, Caffe at 94 mentions and Theano at 53 mentions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;courses-tutorials-and-books&quot;&gt;Courses, Tutorials and Books&lt;/h2&gt;

&lt;p&gt;When we released PyTorch, we had good API documentation, but our tutorials were limited to a few ipython notebooks — helpful, but not good enough.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/chsasank&quot;&gt;Sasank Chilamkurthy&lt;/a&gt; took it upon himself to revamp the tutorials into the &lt;a href=&quot;https://pytorch.org/tutorials/&quot;&gt;beautiful website&lt;/a&gt; that it is today.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/blog_combined_tutorials.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/spro/practical-pytorch&quot;&gt;Sean Robertson&lt;/a&gt; and &lt;a href=&quot;https://github.com/jcjohnson/pytorch-examples&quot;&gt;Justin Johnson&lt;/a&gt; wrote great new tutorials — in NLP, and to learn by example. &lt;a href=&quot;https://github.com/yunjey/pytorch-tutorial&quot;&gt;Yunjey Choi&lt;/a&gt; wrote a beautiful tutorial where most models were implemented in 30 lines or less.
Each new tutorial helped users find their way faster, with different approaches to learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/PyTorch/status/888500355943641088&quot;&gt;Goku Mohandas and Delip Rao&lt;/a&gt; switched the code content of their book-in-progress to use PyTorch.&lt;/p&gt;

&lt;p&gt;We’ve seen quite a few university machine learning courses being taught with PyTorch as the primary tool, such as Harvard’s &lt;a href=&quot;https://harvard-ml-courses.github.io/cs287-web/&quot;&gt;CS287&lt;/a&gt;. Taking it one step further and democratizing learning, we had three online courses pop up that teach using PyTorch.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Fast.ai’s&lt;/strong&gt; “Deep Learning for Coders” is a popular online course. In September, Jeremy and Rachel &lt;a href=&quot;http://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/&quot;&gt;announced that the next fast.ai courses will be nearly entirely based on PyTorch&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Ritchie Ng, a researcher with ties to NUS Singapore and Tsinghua released &lt;a href=&quot;https://www.udemy.com/practical-deep-learning-with-pytorch/&quot;&gt;a Udemy course&lt;/a&gt; titled Practical Deep Learning with PyTorch.&lt;/li&gt;
  &lt;li&gt;Sung Kim from HKUST released an &lt;a href=&quot;https://www.youtube.com/playlist?list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m&quot;&gt;online course on Youtube&lt;/a&gt; that was aimed towards a general audience, titled: “PyTorch Zero to All”.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;engineering&quot;&gt;Engineering&lt;/h2&gt;

&lt;p&gt;Over the last year we implemented multiple features, improved performance across the board and fixed lots of bugs. A full list of the work we’ve done is found in our &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;release notes&lt;/a&gt;.
Here are highlights from our work over the last year:&lt;/p&gt;

&lt;h2 id=&quot;higher-order-gradients&quot;&gt;Higher-order gradients&lt;/h2&gt;

&lt;p&gt;With the release of several papers that implement penalties of gradients and with ongoing research in 2nd order gradient methods, this was an essential and sought-after feature. In August, we implemented a generalized interface that can take n-th order derivatives and increased the coverage of functions that support higher-order gradients over time, such that at the moment of writing almost all ops support this.&lt;/p&gt;

&lt;h2 id=&quot;distributed-pytorch&quot;&gt;Distributed PyTorch&lt;/h2&gt;

&lt;p&gt;In August, we released a small distributed package that followed the highly popular MPI-collective approach. The package has multiple backends such as TCP, MPI, Gloo and NCCL2 to support various types of CPU/GPU collective operations and use-cases, and integrates distributed technologies such as Infiniband and RoCE. Distributed is hard, and we had bugs in the initial iteration. Over subsequent releases, we made the package more stable and improved performance.&lt;/p&gt;

&lt;h2 id=&quot;closer-to-numpy&quot;&gt;Closer to NumPy&lt;/h2&gt;

&lt;p&gt;One of the biggest demands from users were NumPy features that they were familiar with. Features such as Broadcasting and Advanced Indexing are convenient and save users a lot of verbosity. We implemented these features and started to align our API to be closer to NumPy. Over time, we expect to get closer and closer to NumPy’s API where appropriate.&lt;/p&gt;

&lt;h2 id=&quot;sparse-tensors&quot;&gt;Sparse Tensors&lt;/h2&gt;

&lt;p&gt;In March, we released a small package supporting sparse Tensors and in May we released CUDA support for the sparse package. The package is small and limited in functionality, and is used for implementing Sparse Embeddings and commonly used sparse paradigms in deep learning. This package is still small in scope and there’s demand to expand it — if you are interested in working on expanding the sparse package, reach out to us on our &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;Discussion Boards&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;Performance is always an ongoing battle, especially for PyTorch which is a dynamic framework that wants to maximize flexibility. Over the last year, we’ve improved performance across board, from our core Tensor library to the neural network operators, writing faster micro-optimized across board.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We’ve added specialized AVX and AVX2 intrinsics for Tensor operations&lt;/li&gt;
  &lt;li&gt;Wrote faster GPU kernels for frequent workloads like concatenation and Softmax (among many other things)&lt;/li&gt;
  &lt;li&gt;Rewrote the code for several neural network operators (too many to list), but notably nn.Embedding and group convolutions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Reducing framework overhead by 10x across board&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since PyTorch is a dynamic graph framework, we create a new graph on the fly at every iteration of a training loop. Hence, the framework overhead has to be low, or the workload has to be large enough that the framework overhead is hidden. In August, the authors of DyNet (Graham Neubig and team) showcased that it’s much faster than PyTorch on small NLP models. This was an interesting challenge, we didn’t realize that models of those sizes were being trained. In a multi-month (and ongoing) effort, we embarked upon a significant rewrite of PyTorch internals that reduced the framework overhead from more than 10 microseconds per operator execution to as little as 1 microsecond.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ATen&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As we embarked upon a redesign of the PyTorch internals, we built the &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/master/aten&quot;&gt;ATen C++11&lt;/a&gt; library that now powers all of the PyTorch backend. ATen has an API that mirrors PyTorch’s Python API, which makes it a convenient C++ library for Tensor computation. ATen can be built and used independently of PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;exporting-models-to-production--onnx-support-and-the-jit-compiler&quot;&gt;Exporting models to production — ONNX Support and the JIT compiler&lt;/h2&gt;

&lt;p&gt;One of the common requests we’ve received was to export PyTorch models to another framework. Users engaged in a rapid research cycle in PyTorch and when they were done, they wanted to ship it to larger projects with C++ only requirements.&lt;/p&gt;

&lt;p&gt;With this in mind, we built a tracer for PyTorch — which can export PyTorch models into an intermediate representation.
The subsequent trace can be either used to run the current PyTorch model more efficiently (by running optimization passes on it), or be converted to the &lt;a href=&quot;http://onnx.ai/&quot;&gt;ONNX&lt;/a&gt; format to be shipped to other frameworks such as Caffe2, MXNet, TensorFlow and others or directly to the hardware accelerated libraries like CoreML or TensorRT. Over the next year, you will hear more about the JIT compiler for performance improvements.&lt;/p&gt;

&lt;h2 id=&quot;users-being-funny-&quot;&gt;Users being funny :)&lt;/h2&gt;

&lt;p&gt;Our users express their support in funny ways, made us laugh, thanks for this :)&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;I&amp;#39;ve been using PyTorch a few months now and I&amp;#39;ve never felt better. I have more energy. My skin is clearer. My eye sight has improved.&lt;/p&gt;&amp;mdash; Andrej Karpathy (@karpathy) &lt;a href=&quot;https://twitter.com/karpathy/status/868178954032513024?ref_src=twsrc%5Etfw&quot;&gt;May 26, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Talk to your doctor to find out if PyTorch is right for you.&lt;/p&gt;&amp;mdash; Sean Robertson (@sprobertson) &lt;a href=&quot;https://twitter.com/sprobertson/status/868180795000750080?ref_src=twsrc%5Etfw&quot;&gt;May 26, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;PyTorch gave me so much life that my skin got cleared, my grades are up, my bills are paid and my crops are watered.&lt;/p&gt;&amp;mdash; Adam Will ð️‍ð (@adam_will_do_it) &lt;a href=&quot;https://twitter.com/adam_will_do_it/status/868179679483764736?ref_src=twsrc%5Etfw&quot;&gt;May 26, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;So have I! But my hair is also shiner and I&amp;#39;ve lost weight. &lt;a href=&quot;https://twitter.com/PyTorch?ref_src=twsrc%5Etfw&quot;&gt;@PyTorch&lt;/a&gt; for the win. &lt;a href=&quot;https://t.co/qgU4oIOB4K&quot;&gt;https://t.co/qgU4oIOB4K&lt;/a&gt;&lt;/p&gt;&amp;mdash; Mariya (@thinkmariya) &lt;a href=&quot;https://twitter.com/thinkmariya/status/868181991212044288?ref_src=twsrc%5Etfw&quot;&gt;May 26, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;</content><author><name>The PyTorch Team</name></author><summary type="html">Today marks 1 year since PyTorch was released publicly. It’s been a wild ride — our quest to build a flexible deep learning research platform. Over the last year, we’ve seen an amazing community of people using, contributing to and evangelizing PyTorch — thank you for the love.</summary></entry></feed>