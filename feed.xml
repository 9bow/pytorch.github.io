<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pytorch.org/" rel="alternate" type="text/html" /><updated>2021-06-08T01:12:05-07:00</updated><id>https://pytorch.org/feed.xml</id><title type="html">PyTorch Website</title><subtitle>Scientific Computing...</subtitle><author><name>Facebook</name></author><entry><title type="html">Everything you need to know about TorchVision’s MobileNetV3 implementation</title><link href="https://pytorch.org/blog/torchvision-mobilenet-v3-implementation/" rel="alternate" type="text/html" title="Everything you need to know about TorchVision’s MobileNetV3 implementation" /><published>2021-05-26T00:00:00-07:00</published><updated>2021-05-26T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchvision-mobilenet-v3-implementation</id><content type="html" xml:base="https://pytorch.org/blog/torchvision-mobilenet-v3-implementation/">&lt;p&gt;In TorchVision v0.9, we released a series of &lt;a href=&quot;https://pytorch.org/blog/ml-models-torchvision-v0.9/&quot;&gt;new mobile-friendly models&lt;/a&gt; that can be used for Classification, Object Detection and Semantic Segmentation. In this article, we will dig deep into the code of the models, share notable implementation details, explain how we configured and trained them, and highlight important tradeoffs we made during their tuning. Our goal is to disclose technical details that typically remain undocumented in the original papers and repos of the models.&lt;/p&gt;

&lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt;

&lt;p&gt;The implementation of the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py&quot;&gt;MobileNetV3 architecture&lt;/a&gt; follows closely the &lt;a href=&quot;https://arxiv.org/abs/1905.02244&quot;&gt;original paper&lt;/a&gt;. It is customizable and offers different configurations for building Classification, Object Detection and Semantic Segmentation backbones. It was designed to follow a similar structure to MobileNetV2 and the two share &lt;a href=&quot;https://github.com/pytorch/vision/blob/cac8a97b0bd14eddeff56f87a890d5cc85776e18/torchvision/models/mobilenetv2.py#L32&quot;&gt;common building blocks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Off-the-shelf, we offer the two variants described on the paper: the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L196-L214&quot;&gt;Large&lt;/a&gt; and the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L215-L229&quot;&gt;Small&lt;/a&gt;. Both are constructed using the same code with the only difference being their configuration which describes the number of blocks, their sizes, their activation functions etc.&lt;/p&gt;

&lt;h3 id=&quot;configuration-parameters&quot;&gt;Configuration parameters&lt;/h3&gt;

&lt;p&gt;Even though one can write a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L105&quot;&gt;custom InvertedResidual setting&lt;/a&gt; and pass it to the MobileNetV3 class directly, for the majority of applications we can adapt the existing configs by passing parameters to the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L253&quot;&gt;model building methods&lt;/a&gt;. Some of the key configuration parameters are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;width_mult&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L188&quot;&gt;parameter&lt;/a&gt; is a multiplier that affects the number of channels of the model. The default value is 1 and by increasing or decreasing it one can change the number of filters of all convolutions, including the ones of the first and last layers. The implementation ensures that the number of filters is always a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L56-L57&quot;&gt;multiple of 8&lt;/a&gt;. This is a hardware optimization trick which allows for faster vectorization of operations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduced_tail&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L188&quot;&gt;parameter&lt;/a&gt; halves the number of channels on the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L210-L214&quot;&gt;last blocks&lt;/a&gt; of the network. This version is used by some Object Detection and Semantic Segmentation models. It’s a speed optimization which is described on the &lt;a href=&quot;https://arxiv.org/abs/1905.02244&quot;&gt;MobileNetV3 paper&lt;/a&gt; and reportedly leads to a 15% latency reduction without a significant negative effect on accuracy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dilated&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L188&quot;&gt;parameter&lt;/a&gt; affects the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L210-L212&quot;&gt;last 3&lt;/a&gt; InvertedResidual blocks of the model and turns their normal depthwise Convolutions to Atrous Convolutions. This is used to control the output stride of these blocks and has a &lt;a href=&quot;https://arxiv.org/abs/1706.05587&quot;&gt;significant positive effect&lt;/a&gt; on the accuracy of Semantic Segmentation models.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation details&lt;/h3&gt;

&lt;p&gt;Below we provide additional information on some notable implementation details of the architecture.
The &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L101&quot;&gt;MobileNetV3 class&lt;/a&gt; is responsible for building a network out of the provided configuration. Here are some implementation details of the class:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The last convolution block expands the output of the last InvertedResidual block by a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L149&quot;&gt;factor of 6&lt;/a&gt;. The implementation is aligned with the Large and Small configurations described on the paper and can adapt to different values of the multiplier parameter.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly to other models such as MobileNetV2, a dropout layer is placed just before the final Linear layer of the classifier.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L60&quot;&gt;InvertedResidual class&lt;/a&gt; is the main building block of the network. Here are some notable implementation details of the block along with its visualization which comes from Figure 4 of the paper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There is no &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L73-L76&quot;&gt;expansion step&lt;/a&gt; if the input channels and the expanded channels are the same. This happens on the first convolution block of the network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is always a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L86-L88&quot;&gt;projection step&lt;/a&gt; even when the expanded channels are the same as the output channels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The activation method of the depthwise block is placed &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L82-L84&quot;&gt;before&lt;/a&gt; the Squeeze-and-Excite layer as this improves marginally the accuracy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/mobilenet-v3-block.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;

&lt;p&gt;In this section we provide benchmarks of the pre-trained models and details on how they were configured, trained and quantized.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here is how to initialize the pre-trained models:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;large = torchvision.models.mobilenet_v3_large(pretrained=True, width_mult=1.0,  reduced_tail=False, dilated=False)
small = torchvision.models.mobilenet_v3_small(pretrained=True)
quantized = torchvision.models.quantization.mobilenet_v3_large(pretrained=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below we have the detailed benchmarks between new and selected previous models. As we can see MobileNetV3-Large is a viable replacement of ResNet50 for users who are willing to sacrifice a bit of accuracy for a roughly 6x speed-up:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@5&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# Params (M)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;74.042&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.340&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0411&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MobileNetV3-Small&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;67.668&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;87.402&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0165&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.54&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Quantized MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;73.004&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.858&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0162&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.96&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MobileNetV2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.880&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.290&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0608&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet50&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;76.150&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92.870&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2545&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;25.56&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet18&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;69.760&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89.080&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1032&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11.69&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Note that the inference times are measured on CPU. They are not absolute benchmarks, but they allow for relative comparisons between models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All pre-trained models are configured with a width multiplier of 1, have full tails, are non-dilated, and were fitted on ImageNet. Both the Large and Small variants were trained using the same hyper-parameters and scripts which can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/c2ab0c59f42babf9ad01aa616cd8a901daac86dd/references/classification#mobilenetv3-large--small&quot;&gt;references&lt;/a&gt; folder. Below we provide details on the most notable aspects of the training process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Achieving fast and stable training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/vision/blob/c2ab0c59f42babf9ad01aa616cd8a901daac86dd/references/classification/train.py#L172-L173&quot;&gt;Configuring RMSProp&lt;/a&gt; correctly was crucial to achieve fast training with numerical stability. The authors of the paper used TensorFlow in their experiments and in their runs they reported using &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet#v3&quot;&gt;quite high&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rmsprop_epsilon&lt;/code&gt; comparing to the default. Typically this hyper-parameter takes small values as it’s used to avoid zero denominators, but in this specific model choosing the right value seems important to avoid numerical instabilities in the loss.&lt;/p&gt;

&lt;p&gt;Another important detail is that though PyTorch’s and TensorFlow’s RMSProp implementations typically behave similarly, there are &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/32545&quot;&gt;a few differences&lt;/a&gt; with the most notable in our setup being how the epsilon hyperparameter is handled. More specifically, PyTorch adds the epsilon &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/training/rmsprop.py#L25&quot;&gt;outside of the square root calculation&lt;/a&gt; while TensorFlow &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/training/rmsprop.py#L25&quot;&gt;adds it inside&lt;/a&gt;. The result of this implementation detail is that one needs to adjust the epsilon value while porting the hyper parameter of the paper. A reasonable approximation can be taken with the formula &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PyTorch_eps = sqrt(TF_eps)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Increasing our accuracy by tuning hyperparameters &amp;amp; improving our training recipe&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After configuring the optimizer to achieve fast and stable training, we turned into optimizing the accuracy of the model. There are a few techniques that helped us achieve this. First of all, to avoid overfitting we augmented out data using the AutoAugment algorithm, followed by RandomErasing. Additionally we tuned parameters such as the weight decay using cross validation. We also found beneficial to perform &lt;a href=&quot;https://github.com/pytorch/vision/blob/674e8140042c2a3cbb1eb9ebad1fa49501599130/references/classification/utils.py#L259&quot;&gt;weight averaging&lt;/a&gt; across different epoch checkpoints after the end of the training. Finally, though not used in our published training recipe, we found that using Label Smoothing, Stochastic Depth and LR noise injection improve the overall accuracy by over &lt;a href=&quot;https://rwightman.github.io/pytorch-image-models/training_hparam_examples/#mobilenetv3-large-100-75766-top-1-92542-top-5&quot;&gt;1.5 points&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The graph and table depict a simplified summary of the most important iterations for improving the accuracy of the MobileNetV3 Large variant. Note that the actual number of iterations done while training the model was significantly larger and that the progress in accuracy was not always monotonically increasing. Also note that the Y-axis of the graph starts from 70% instead from 0% to make the difference between iterations more visible:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/key-iterations-for-improving-the-accuracyof-mobilenetV3.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Iteration&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@5&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Baseline with “MobileNetV2-style” Hyperparams&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.542&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.068&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ RMSProp with default eps&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;70.684&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ RMSProp with adjusted eps &amp;amp; LR scheme&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.764&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.178&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Data Augmentation &amp;amp; Tuned Hyperparams&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;73.86&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.292&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Checkpoint Averaging&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;74.028&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.382&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Label Smoothing &amp;amp; Stochastic Depth &amp;amp; LR noise&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;75.536&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92.368&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Note that once we’ve achieved an acceptable accuracy, we verified the model performance on the hold-out test dataset which hasn’t been used before for training or hyper-parameter tuning. This process helps us detect overfitting and is always performed for all pre-trained models prior their release.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Quantization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We currently offer quantized weights for the QNNPACK backend of the &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/quantization/mobilenetv3.py#L115&quot;&gt;MobileNetV3-Large variant&lt;/a&gt; which provides a speed-up of 2.5x. To quantize the model, Quantized Aware Training (QAT) was used. The hyper parameters and the scripts used to train the model can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/c2ab0c59f42babf9ad01aa616cd8a901daac86dd/references/classification#quantized&quot;&gt;references&lt;/a&gt; folder.&lt;/p&gt;

&lt;p&gt;Note that QAT allows us to model the effects of quantization and adjust the weights so that we can improve the model accuracy. This translates to an accuracy increase of 1.8 points comparing to simple post-training quantization:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Quantization Status&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@5&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Non-quantized&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;74.042&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.340&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Quantized Aware Training&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;73.004&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.858&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Post-training Quantization&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.160&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89.834&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h3&gt;

&lt;p&gt;In this section, we will first provide benchmarks of the released models, and then discuss how the MobileNetV3-Large backbone was used in a Feature Pyramid Network along with the FasterRCNN detector to perform Object Detection. We will also explain how the network was trained and tuned alongside with any tradeoffs we had to make. We will not cover details about how it was used with &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/detection/ssdlite.py&quot;&gt;SSDlite&lt;/a&gt; as this will be discussed on a future article.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here is how the models are initialized:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;high_res = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) 
low_res = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are some benchmarks between new and selected previous models. As we can see the high resolution Faster R-CNN with MobileNetV3-Large FPN backbone seems a viable replacement of the equivalent ResNet50 model for those users who are willing to sacrifice few accuracy points for a 5x speed-up:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mAP&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# Params (M)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN MobileNetV3-Large FPN (High-Res)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;32.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.8409&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;19.39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN MobileNetV3-Large 320 FPN (Low-Res)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;22.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1679&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;19.39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;37.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1514&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;41.76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RetinaNet ResNet-50 FPN&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;36.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.8825&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;34.01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Implementation details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Detector uses a FPN-style backbone which extracts features from different convolutions of the MobileNetV3 model. &lt;a href=&quot;https://github.com/pytorch/vision/blob/eca37cf735064702189ff5d5b1428cbe25ab2bcf/torchvision/models/detection/backbone_utils.py#L165-L166&quot;&gt;By default&lt;/a&gt; the pre-trained model uses the output of the 13th InvertedResidual block and the output of the Convolution prior to the pooling layer but the implementation supports using the outputs of &lt;a href=&quot;https://github.com/pytorch/vision/blob/eca37cf735064702189ff5d5b1428cbe25ab2bcf/torchvision/models/detection/backbone_utils.py#L147-L150&quot;&gt;more stages&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All feature maps extracted from the network have their output projected down to &lt;a href=&quot;https://github.com/pytorch/vision/blob/eca37cf735064702189ff5d5b1428cbe25ab2bcf/torchvision/models/detection/backbone_utils.py#L160&quot;&gt;256 channels&lt;/a&gt; by the FPN block as this greatly improves the speed of the network. These feature maps provided by the FPN backbone are used by the FasterRCNN detector to provide box and class predictions at &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L382-L389&quot;&gt;different scales&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training &amp;amp; Tuning process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We currently offer two pre-trained models capable of doing object detection at different resolutions. Both models were trained on the COCO dataset using the same hyper-parameters and scripts which can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/e35793a1a4000db1f9f99673437c514e24e65451/references/detection#faster-r-cnn-mobilenetv3-large-fpn&quot;&gt;references&lt;/a&gt; folder.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L398-L399&quot;&gt;High Resolution detector&lt;/a&gt; was trained with images of 800-1333px, while the mobile-friendly &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L398-L399&quot;&gt;Low Resolution detector&lt;/a&gt; was trained with images of 320-640px. The reason why we provide two separate sets of pre-trained weights is because training a detector directly on the smaller images leads to a 5 mAP increase in precision comparing to passing small images to the pre-trained high-res model. Both backbones were initialized with weights fitted on ImageNet and the &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L377-L378&quot;&gt;3 last stages&lt;/a&gt; of their weights where fined-tuned during the training process.&lt;/p&gt;

&lt;p&gt;An additional speed optimization can be applied on the mobile-friendly model by &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L423-L424&quot;&gt;tuning the RPN NMS thresholds&lt;/a&gt;. By sacrificing only 0.2 mAP of precision we were able to improve the CPU speed of the model by roughly 45%. The details of the optimization can be seen below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Tuning Status&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mAP&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Before&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;23.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2904&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;After&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;22.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1679&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Below we provide some examples of visualizing the predictions of the Faster R-CNN MobileNetV3-Large FPN model:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/detection.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;semantic-segmentation&quot;&gt;Semantic Segmentation&lt;/h3&gt;

&lt;p&gt;In this section we will start by providing some benchmarks of the released pre-trained models. Then we will discuss how a MobileNetV3-Large backbone was combined with segmentation heads such as &lt;a href=&quot;https://arxiv.org/abs/1905.02244&quot;&gt;LR-ASPP&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1706.05587&quot;&gt;DeepLabV3&lt;/a&gt; and the &lt;a href=&quot;https://arxiv.org/abs/1411.4038&quot;&gt;FCN&lt;/a&gt; to conduct Semantic Segmentation. We will also explain how the network was trained and propose a few optional optimization techniques for speed critical applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is how to initialize the pre-trained models:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(pretrained=True) 
deeplabv3 = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are the detailed benchmarks between new and selected existing models. As we can see, the DeepLabV3 with a MobileNetV3-Large backbone is a viable replacement of FCN with ResNet50 for the majority of applications as it achieves similar accuracy with a 8.5x speed-up. We also observe that the LR-ASPP network supersedes the equivalent FCN in all metrics:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mIoU&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Global Pixel Acc&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# Params (M)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;LR-ASPP MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;57.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.3278&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;60.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5869&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11.03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN MobileNetV3-Large (not released)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;57.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.3702&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 ResNet50&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;66.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.3531&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;39.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN ResNet50&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;60.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.0146&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;32.96&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;implementation-details-1&quot;&gt;Implementation details&lt;/h3&gt;

&lt;p&gt;In this section we will discuss important implementation details of tested segmentation heads. Note that all models described in this section use a dilated MobileNetV3-Large backbone.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LR-ASPP&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The LR-ASPP is the Lite variant of the Reduced Atrous Spatial Pyramid Pooling model proposed by the authors of the MobileNetV3 paper. Unlike the other segmentation models in TorchVision, it does not make use of an &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L185-L186&quot;&gt;auxiliary loss&lt;/a&gt;. Instead it uses &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L92-L100&quot;&gt;low and high-level features&lt;/a&gt; with output strides of 8 and 16 respectively.&lt;/p&gt;

&lt;p&gt;Unlike the paper where a 49x49 AveragePooling layer with variable strides is used, &lt;a href=&quot;https://github.com/pytorch/vision/blob/e2db2eddbb1699a59fbb5ccbec912979048ef3bf/torchvision/models/segmentation/lraspp.py#L53&quot;&gt;our implementation&lt;/a&gt; uses an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AdaptiveAvgPool2d&lt;/code&gt; layer to process the global features. This is because the authors of the paper tailored the head to the Cityscapes dataset while our focus is to provide a general purpose implementation that can work on multiple datasets. Finally our implementation always has a bilinear interpolation &lt;a href=&quot;https://github.com/pytorch/vision/blob/e2db2eddbb1699a59fbb5ccbec912979048ef3bf/torchvision/models/segmentation/lraspp.py#L35&quot;&gt;before returning the output&lt;/a&gt; to ensure that the sizes of the input and output images match exactly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DeepLabV3 &amp;amp; FCN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The combination of MobileNetV3 with DeepLabV3 and FCN follows closely the ones of other models and the stage estimation for these methods is identical to LR-ASPP. The only notable difference is that instead of using high and low level features, &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L37-L45&quot;&gt;we attach&lt;/a&gt; the normal loss to the feature map with output stride 16 and an auxiliary loss on the feature map with output stride 8.&lt;/p&gt;

&lt;p&gt;Finally we should note that the FCN version of the model was not released because it was completely superseded by the LR-ASPP both in terms of speed and accuracy. The &lt;a href=&quot;https://github.com/pytorch/vision/pull/3276/commits/1641d5f4c7d41f534444fab340c598d61a91bd12#diff-ccff7af514d99eeb40416c8b9ec30f032d1a3f450aaa4057958ca39ab174452eL17&quot;&gt;pre-trained weights&lt;/a&gt; are still available and can be used with minimal changes to the code.&lt;/p&gt;

&lt;h3 id=&quot;training--tuning-process&quot;&gt;Training &amp;amp; Tuning process&lt;/h3&gt;

&lt;p&gt;We currently offer two MobileNetV3 pre-trained models capable of doing semantic segmentation: the LR-ASPP and the DeepLabV3. The backbones of the models were &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L89-L90&quot;&gt;initialized with ImageNet weights&lt;/a&gt; and trained end-to-end. Both architectures were trained on the COCO dataset using the same scripts with similar hyper-parameters. Their details can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/a78d0d83d0a499fe8480d7a9f493676e746c4699/references/segmentation#deeplabv3_mobilenet_v3_large&quot;&gt;references&lt;/a&gt; folder.&lt;/p&gt;

&lt;p&gt;Normally, during inference the images are &lt;a href=&quot;https://github.com/pytorch/vision/blob/a78d0d83d0a499fe8480d7a9f493676e746c4699/references/segmentation/train.py#L30-L33&quot;&gt;resized to 520 pixels&lt;/a&gt;. An optional speed optimization is to construct a Low Res configuration of the model by using the High-Res pre-trained weights and reducing the inference resizing to 320 pixels. This will improve the CPU execution times by roughly 60% while sacrificing a couple of mIoU points. The detailed numbers of this optimization can be found on the table below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Low-Res Configuration&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mIoU Difference&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Speed Improvement&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mIoU&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Global Pixel Acc&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;LR-ASPP MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-2.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;65.26%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;55.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1139&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-3.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;63.86%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;56.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2121&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN MobileNetV3-Large (not released)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;57.57%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;54.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1571&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here are some examples of visualizing the predictions of the LR-ASPP MobileNetV3-Large model:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/segmentation.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We hope that you found this article interesting. We are looking forward to your feedback to see if this is the type of content you would like us to publish more often. If the community finds that such posts are useful, we will be happy to publish more articles that cover the implementation details of newly introduced Machine Learning models.&lt;/p&gt;</content><author><name>Vasilis Vryniotis and Francisco Massa</name></author><summary type="html">In TorchVision v0.9, we released a series of new mobile-friendly models that can be used for Classification, Object Detection and Semantic Segmentation. In this article, we will dig deep into the code of the models, share notable implementation details, explain how we configured and trained them, and highlight important tradeoffs we made during their tuning. Our goal is to disclose technical details that typically remain undocumented in the original papers and repos of the models.</summary></entry><entry><title type="html">Announcing the PyTorch Enterprise Support Program</title><link href="https://pytorch.org/blog/announcing-pytorch-enterprise/" rel="alternate" type="text/html" title="Announcing the PyTorch Enterprise Support Program" /><published>2021-05-25T00:00:00-07:00</published><updated>2021-05-25T00:00:00-07:00</updated><id>https://pytorch.org/blog/announcing-pytorch-enterprise</id><content type="html" xml:base="https://pytorch.org/blog/announcing-pytorch-enterprise/">&lt;p&gt;Today, we are excited to announce the &lt;a href=&quot;http://pytorch.org/enterprise-support-program&quot;&gt;PyTorch Enterprise Support Program&lt;/a&gt;, a participatory program that enables service providers to develop and offer tailored enterprise-grade support to their customers. This new offering, built in collaboration between Facebook and Microsoft, was created in direct response to feedback from PyTorch enterprise users who are developing models in production at scale for mission-critical applications.&lt;/p&gt;

&lt;p&gt;The PyTorch Enterprise Support Program is available to any service provider. It is designed to mutually benefit all program Participants by sharing and improving PyTorch long-term support (LTS), including contributions of hotfixes and other improvements found while working closely with customers and on their systems.&lt;/p&gt;

&lt;p&gt;To benefit the open source community, all hotfixes developed by Participants will be tested and fed back to the LTS releases of PyTorch regularly through PyTorch’s standard pull request process. To participate in the program, a service provider must apply and meet a set of program terms and certification requirements. Once accepted, the service provider becomes a program Participant and can offer a packaged PyTorch Enterprise support service with LTS, prioritized troubleshooting, useful integrations, and more.&lt;/p&gt;

&lt;div class=&quot;col-md-5 enterprise-azure-logo-container&quot;&gt;
  &lt;img src=&quot;/assets/images/microsoft-azure.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;As one of the founding members and an inaugural member of the PyTorch Enterprise Support Program, Microsoft is launching &lt;a href=&quot;https://Aka.ms/PyTorchEnterpriseHeroBlog&quot;&gt;PyTorch Enterprise on Microsoft Azure&lt;/a&gt; to deliver a reliable production experience for PyTorch users. Microsoft will support each PyTorch release for as long as it is current. In addition, it will support selected releases for two years, enabling a stable production experience. Microsoft Premier and Unified Support customers can access prioritized troubleshooting for hotfixes, bugs, and security patches at no additional cost. Microsoft will extensively test PyTorch releases for performance regression. The latest release of PyTorch will be integrated with &lt;a href=&quot;https://azure.microsoft.com/en-us/services/machine-learning/&quot;&gt;Azure Machine Learning&lt;/a&gt; and other PyTorch add-ons including &lt;a href=&quot;https://www.onnxruntime.ai/&quot;&gt;ONNX Runtime&lt;/a&gt; for faster inference.&lt;/p&gt;

&lt;p&gt;PyTorch Enterprise on Microsoft Azure not only benefits its customers, but also the PyTorch community users. All improvements will be tested and fed back to the future release for PyTorch so everyone in the community can use them.&lt;/p&gt;

&lt;p&gt;As an organization or PyTorch user, the standard way of researching and deploying with different release versions of PyTorch does not change. If your organization is looking for the managed long-term support, prioritized patches, bug fixes, and additional enterprise-grade support, then you should reach out to service providers participating in the program.&lt;/p&gt;

&lt;p&gt;To learn more and participate in the program as a service provider, visit the &lt;a href=&quot;http://pytorch.org/enterprise-support-program&quot;&gt;PyTorch Enterprise Support Program&lt;/a&gt;. If you want to learn more about Microsoft’s offering, visit &lt;a href=&quot;https://Aka.ms/PyTorchEnterpriseHeroBlog&quot;&gt;PyTorch Enterprise on Microsoft Azure&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thank you,&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we are excited to announce the PyTorch Enterprise Support Program, a participatory program that enables service providers to develop and offer tailored enterprise-grade support to their customers. This new offering, built in collaboration between Facebook and Microsoft, was created in direct response to feedback from PyTorch enterprise users who are developing models in production at scale for mission-critical applications.</summary></entry><entry><title type="html">PyTorch Ecosystem Day 2021 Recap and New Contributor Resources</title><link href="https://pytorch.org/blog/ecosystem-day-2021-recap/" rel="alternate" type="text/html" title="PyTorch Ecosystem Day 2021 Recap and New Contributor Resources" /><published>2021-05-10T00:00:00-07:00</published><updated>2021-05-10T00:00:00-07:00</updated><id>https://pytorch.org/blog/ecosystem-day-2021-recap</id><content type="html" xml:base="https://pytorch.org/blog/ecosystem-day-2021-recap/">&lt;p&gt;Thank you to our incredible community for making the first ever PyTorch Ecosystem Day a success! The day was filled with discussions on new developments, trends and challenges showcased through 71 posters, 32 breakout sessions and 6 keynote speakers.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/ecosystem-day-thank-you.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Special thanks to our keynote speakers: Piotr Bialecki, Ritchie Ng, Miquel Farré, Joe Spisak, Geeta Chauhan, and Suraj Subramanian who shared updates from the latest release of PyTorch, exciting work being done with partners, use case example from Disney, the growth and development of the PyTorch community in Asia Pacific, and latest contributor highlights.&lt;/p&gt;

&lt;p&gt;If you missed the opening talks, you rewatch them here:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=MYE01-XaSZA&quot;&gt;Morning/EMEA Opening Talks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=CjU_6OaYKpw&quot;&gt;Evening/APAC Opening Talks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to the talks, we had 71 posters covering various topics such as multimodal, NLP, compiler, distributed training, researcher productivity tools, AI accelerators, and more. From the event, it was clear that an underlying thread that ties all of these different projects together is the cross-collaboration of the PyTorch community. Thank you for continuing to push the state of the art with PyTorch!&lt;/p&gt;

&lt;p&gt;To view the full catalogue of poster, please visit &lt;strong&gt;&lt;a href=&quot;https://pytorch.org/ecosystem/pted/2021&quot;&gt;PyTorch Ecosystem Day 2021 Event Page&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;new-contributor-resources&quot;&gt;New Contributor Resources&lt;/h3&gt;
&lt;p&gt;Today, we are also sharing new contributor resources that we are trying out to give you the most access to up-to-date news, networking opportunities and more.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/resources/contributors/&quot;&gt;Contributor Newsletter&lt;/a&gt; - Includes curated news including RFCs, feature roadmaps, notable PRs, editorials from developers, and more to support keeping track of everything that’s happening in our community.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev-discuss.pytorch.org/&quot;&gt;Contributors Discussion Forum&lt;/a&gt; - Designed for contributors to learn and collaborate on the latest development across PyTorch.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch-dev-podcast.simplecast.com/&quot;&gt;PyTorch Developer Podcast (Beta)&lt;/a&gt; - Edward Yang, PyTorch Research Scientist, at Facebook AI shares bite-sized (10 to 20 mins) podcast episodes discussing topics about all sorts of internal development topics in PyTorch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you,&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Thank you to our incredible community for making the first ever PyTorch Ecosystem Day a success! The day was filled with discussions on new developments, trends and challenges showcased through 71 posters, 32 breakout sessions and 6 keynote speakers.</summary></entry><entry><title type="html">An overview of the ML models introduced in TorchVision v0.9</title><link href="https://pytorch.org/blog/ml-models-torchvision-v0.9/" rel="alternate" type="text/html" title="An overview of the ML models introduced in TorchVision v0.9" /><published>2021-04-16T00:00:00-07:00</published><updated>2021-04-16T00:00:00-07:00</updated><id>https://pytorch.org/blog/ml-models-torchvision-v0.9</id><content type="html" xml:base="https://pytorch.org/blog/ml-models-torchvision-v0.9/">&lt;p&gt;TorchVision v0.9 has been &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;released&lt;/a&gt; and it is packed with numerous new Machine Learning models and features, speed improvements and bug fixes. In this blog post, we provide a quick overview of the newly introduced ML models and discuss their key features and characteristics.&lt;/p&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;MobileNetV3 Large &amp;amp; Small:&lt;/strong&gt; These two classification models are optimized for Mobile use-cases and are used as backbones on other Computer Vision tasks. The implementation of the new &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenetv3.py&quot;&gt;MobileNetV3 architecture&lt;/a&gt; supports the Large &amp;amp; Small variants and the depth multiplier parameter as described in the &lt;a href=&quot;https://arxiv.org/pdf/1905.02244.pdf&quot;&gt;original paper&lt;/a&gt;. We offer pre-trained weights on ImageNet for both Large and Small networks with depth multiplier 1.0 and resolution 224x224. Our previous &lt;a href=&quot;https://github.com/pytorch/vision/tree/master/references/classification#mobilenetv3-large--small&quot;&gt;training recipes&lt;/a&gt; have been updated and can be used to easily train the models from scratch (shoutout to Ross Wightman for inspiring some of our &lt;a href=&quot;https://rwightman.github.io/pytorch-image-models/training_hparam_examples/#mobilenetv3-large-100-75766-top-1-92542-top-5&quot;&gt;training configuration&lt;/a&gt;). The Large variant offers a &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/docs/source/models.rst#classification&quot;&gt;competitive accuracy&lt;/a&gt; comparing to ResNet50 while being over 6x faster on CPU, meaning that it is a good candidate for applications where speed is important. For applications where speed is critical, one can sacrifice further accuracy for speed and use the Small variant which is 15x faster than ResNet50.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Quantized MobileNetV3 Large:&lt;/strong&gt; The quantized version of MobilNetV3 Large reduces the number of parameters by 45% and it is roughly 2.5x faster than the non-quantized version while remaining competitive in &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/docs/source/models.rst#quantized-models&quot;&gt;terms of accuracy&lt;/a&gt;. It was fitted on ImageNet using Quantization Aware Training by iterating on the non-quantized version and it can be trained from scratch using the existing &lt;a href=&quot;https://github.com/pytorch/vision/tree/master/references/classification#quantized&quot;&gt;reference scripts&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torchvision.models.mobilenet_v3_large(pretrained=True)
# model = torchvision.models.mobilenet_v3_small(pretrained=True)
# model = torchvision.models.quantization.mobilenet_v3_large(pretrained=True)
model.eval()
predictions = model(img)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Faster R-CNN MobileNetV3-Large FPN:&lt;/strong&gt; Combining the MobileNetV3 Large backbone with a Faster R-CNN detector and a Feature Pyramid Network leads to a highly accurate and fast object detector. The pre-trained weights are fitted on COCO 2017 using the provided reference &lt;a href=&quot;https://github.com/pytorch/vision/tree/master/references/detection#faster-r-cnn-mobilenetv3-large-fpn&quot;&gt;scripts&lt;/a&gt; and the model is 5x faster on CPU than the equivalent ResNet50 detector while remaining competitive in &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/docs/source/models.rst#object-detection-instance-segmentation-and-person-keypoint-detection&quot;&gt;terms of accuracy&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Faster R-CNN MobileNetV3-Large 320 FPN:&lt;/strong&gt; This is an iteration of the previous model that uses reduced resolution (min_size=320 pixel) and sacrifices accuracy for speed. It is 25x faster on CPU than the equivalent ResNet50 detector and thus it is good for real mobile use-cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)
# model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)
model.eval()
predictions = model(img)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;semantic-segmentation&quot;&gt;Semantic Segmentation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;DeepLabV3 with Dilated MobileNetV3 Large Backbone:&lt;/strong&gt; A dilated version of the MobileNetV3 Large backbone combined with DeepLabV3 helps us build a highly accurate and fast semantic segmentation model. The pre-trained weights are fitted on COCO 2017 using our &lt;a href=&quot;https://github.com/pytorch/vision/tree/master/references/segmentation#deeplabv3_mobilenet_v3_large&quot;&gt;standard training recipes&lt;/a&gt;. The final model has the &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/docs/source/models.rst#semantic-segmentation&quot;&gt;same accuracy&lt;/a&gt; as the FCN ResNet50 but it is 8.5x faster on CPU and thus making it an excellent replacement for the majority of applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lite R-ASPP with Dilated MobileNetV3 Large Backbone:&lt;/strong&gt; We introduce the implementation of a new segmentation head called Lite R-ASPP and combine it with the dilated MobileNetV3 Large backbone to build a very fast segmentation model. The new model sacrifices some accuracy to achieve a 15x speed improvement comparing to the previously most lightweight segmentation model which was the FCN ResNet50.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)
# model = torchvision.models.segmentation.lraspp_mobilenet_v3_large(pretrained=True)
model.eval()
predictions = model(img)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In the near future we plan to publish an article that covers the details of how the above models were trained and discuss their tradeoffs and design choices. Until then we encourage you to try out the new models and provide your feedback.&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">TorchVision v0.9 has been released and it is packed with numerous new Machine Learning models and features, speed improvements and bug fixes. In this blog post, we provide a quick overview of the newly introduced ML models and discuss their key features and characteristics.</summary></entry><entry><title type="html">Introducing PyTorch Profiler - the new and improved performance tool</title><link href="https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/" rel="alternate" type="text/html" title="Introducing PyTorch Profiler - the new and improved performance tool" /><published>2021-03-25T00:00:00-07:00</published><updated>2021-03-25T00:00:00-07:00</updated><id>https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool</id><content type="html" xml:base="https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/">&lt;p&gt;Along with &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v1.8.1&quot;&gt;PyTorch 1.8.1 release&lt;/a&gt;, we are excited to announce PyTorch Profiler – the new and improved performance debugging profiler for PyTorch. Developed as part of a collaboration between Microsoft and Facebook, the PyTorch Profiler is an open-source tool that enables accurate and efficient performance analysis and troubleshooting for large-scale deep learning models.&lt;/p&gt;

&lt;p&gt;Analyzing and improving large-scale deep learning model performance is an ongoing challenge that grows in importance as the model sizes increase. For a long time, PyTorch users had a hard time solving this challenge due to the lack of available tools. There were standard performance debugging tools that provide GPU hardware level information but missed PyTorch-specific context of operations. In order to recover missed information, users needed to combine multiple tools together or manually add minimum correlation information to make sense of the data. There was also the autograd profiler (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.autograd.profiler&lt;/code&gt;) which can capture information about PyTorch operations but does not capture detailed GPU hardware-level information and cannot provide support for visualization.&lt;/p&gt;

&lt;p&gt;The new PyTorch Profiler (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.profiler&lt;/code&gt;) is a tool that brings both types of information together and then builds experience that realizes the full potential of that information. This new profiler collects both GPU hardware and PyTorch related information, correlates them, performs automatic detection of bottlenecks in the model, and generates recommendations on how to resolve these bottlenecks. All of this information from the profiler is visualized for the user in TensorBoard. The new Profiler API is natively supported in PyTorch and delivers the simplest experience available to date where users can profile their models without installing any additional packages and see results immediately in TensorBoard with the new PyTorch Profiler plugin. Below is the screenshot of PyTorch Profiler - automatic bottleneck detection.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/pytorch-profiler-bottleneck.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting started&lt;/h2&gt;

&lt;p&gt;PyTorch Profiler is the next version of the PyTorch autograd profiler. It has a new module namespace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.profiler&lt;/code&gt; but maintains compatibility with autograd profiler APIs. The Profiler uses a new GPU profiling engine, built using Nvidia CUPTI APIs, and is able to capture GPU kernel events with high fidelity. To profile your model training loop, wrap the code in the profiler context manager as shown below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;schedule&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;warmup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;repeat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;on_trace_ready&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensorboard_trace_handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;with_stack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;step:{}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;schedule&lt;/code&gt; parameter allows you to limit the number of training steps included in the profile to reduce the amount of data collected and simplify visual analysis by focusing on what’s important. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensorboard_trace_handler&lt;/code&gt; automatically saves profiling results to disk for analysis in TensorBoard.&lt;/p&gt;

&lt;p&gt;To view results of the profiling session in TensorBoard, install PyTorch Profiler TensorBoard Plugin package.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch_tb_profiler&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;visual-studio-code-integration&quot;&gt;Visual Studio Code Integration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;Microsoft Visual Studio Code&lt;/a&gt; is one of the most popular code editors for Python developers and data scientists. The &lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=ms-python.python&quot;&gt;Python extension&lt;/a&gt;  for VS Code recently added the integration of TensorBoard into the code editor, including support for the PyTorch Profiler. Once you have VS Code and the Python extension installed, you can quickly open the TensorBoard Profiler plugin by launching the Command Palette using the keyboard shortcut CTRL + SHIFT + P (CMD + SHIFT + P on a Mac) and typing the “Launch TensorBoard” command.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/pytorch-profiler-vscode-launch.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;This integration comes with a built-in lifecycle   management feature. VS Code will install the TensorBoard package and the PyTorch Profiler plugin package (coming in mid-April) automatically if you don’t have them on your system. VS Code will also launch TensorBoard process for you and automatically look for any TensorBoard log files within your current directory. When you’re done, just close the tab and VS Code will automatically close the process.  No more Terminal windows running on your system to provide a backend for the TensorBoard UI! Below is PyTorch Profiler Trace View running in TensorBoard.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/pytorch-profiler-vscode.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Learn more about TensorBoard support in VS Code in &lt;a href=&quot;https://devblogs.microsoft.com/python/python-in-visual-studio-code-february-2021-release/&quot;&gt;this blog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;feedback&quot;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;Review &lt;a href=&quot;https://pytorch.org/docs/stable/profiler.html&quot;&gt;PyTorch Profiler documentation&lt;/a&gt;, give Profiler a try and let us know about your experience. Provide your feedback on &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;PyTorch Discussion Forum&lt;/a&gt; or file issues on &lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;PyTorch GitHub&lt;/a&gt;.&lt;/p&gt;</content><author><name>Maxim Lukiyanov - Principal PM at Microsoft, Guoliang Hua - Principal Engineering Manager at Microsoft, Geeta Chauhan - Partner Engineering Lead at Facebook, Gisle Dankel - Tech Lead at Facebook</name></author><summary type="html">Along with PyTorch 1.8.1 release, we are excited to announce PyTorch Profiler – the new and improved performance debugging profiler for PyTorch. Developed as part of a collaboration between Microsoft and Facebook, the PyTorch Profiler is an open-source tool that enables accurate and efficient performance analysis and troubleshooting for large-scale deep learning models.</summary></entry><entry><title type="html">PyTorch for AMD ROCm™ Platform now available as Python package</title><link href="https://pytorch.org/blog/pytorch-for-amd-rocm-platform-now-available-as-python-package/" rel="alternate" type="text/html" title="PyTorch for AMD ROCm™ Platform now available as Python package" /><published>2021-03-24T00:00:00-07:00</published><updated>2021-03-24T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-for-amd-rocm-platform-now-available-as-python-package</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-for-amd-rocm-platform-now-available-as-python-package/">&lt;p&gt;With the PyTorch 1.8 release, we are delighted to announce a new installation option for users of
PyTorch on the ROCm™ open software platform. An installable Python package is now hosted on
pytorch.org, along with instructions for local installation in the same simple, selectable format as
PyTorch packages for CPU-only configurations and other GPU platforms. PyTorch on ROCm includes full
capability for mixed-precision and large-scale training using AMD’s MIOpen &amp;amp; RCCL libraries. This
provides a new option for data scientists, researchers, students, and others in the community to get
started with accelerated PyTorch using AMD GPUs.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/amd_rocm_blog.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;the-rocm-ecosystem&quot;&gt;The ROCm Ecosystem&lt;/h2&gt;

&lt;p&gt;ROCm is AMD’s open source software platform for GPU-accelerated high performance computing and
machine learning. Since the original ROCm release in 2016, the ROCm platform has evolved to support
additional libraries and tools, a wider set of Linux® distributions, and a range of new GPUs. This includes
the AMD Instinct™ MI100, the first GPU based on AMD CDNA™ architecture.&lt;/p&gt;

&lt;p&gt;The ROCm ecosystem has an established history of support for PyTorch, which was initially implemented
as a fork of the PyTorch project, and more recently through ROCm support in the upstream PyTorch
code. PyTorch users can install PyTorch for ROCm using AMD’s public PyTorch docker image, and can of
course build PyTorch for ROCm from source. With PyTorch 1.8, these existing installation options are
now complemented by the availability of an installable Python package.&lt;/p&gt;

&lt;p&gt;The primary focus of ROCm has always been high performance computing at scale. The combined
capabilities of ROCm and AMD’s Instinct family of data center GPUs are particularly suited to the
challenges of HPC at data center scale. PyTorch is a natural fit for this environment, as HPC and ML
workflows become more intertwined.&lt;/p&gt;

&lt;h3 id=&quot;getting-started-with-pytorch-for-rocm&quot;&gt;Getting started with PyTorch for ROCm&lt;/h3&gt;

&lt;p&gt;The scope for this build of PyTorch is AMD GPUs with ROCm support, running on Linux. The GPUs
supported by ROCm include all of AMD’s Instinct family of compute-focused data center GPUs, along
with some other select GPUs. A current list of supported GPUs can be found in the &lt;a href=&quot;https://github.com/RadeonOpenCompute/ROCm#supported-gpus&quot;&gt;ROCm Github
repository&lt;/a&gt;. After confirming that the target system includes supported GPUs and the current 4.0.1
release of ROCm, installation of PyTorch follows the same simple Pip-based installation as any other
Python package. As with PyTorch builds for other platforms, the configurator at &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt; provides the specific command line to be run.&lt;/p&gt;

&lt;p&gt;PyTorch for ROCm is built from the upstream PyTorch repository, and is a full featured implementation.
Notably, it includes support for distributed training across multiple GPUs and supports accelerated
mixed precision training.&lt;/p&gt;

&lt;h3 id=&quot;more-information&quot;&gt;More information&lt;/h3&gt;

&lt;p&gt;A list of ROCm supported GPUs and operating systems can be found at
&lt;a href=&quot;https://github.com/RadeonOpenCompute/ROCm&quot;&gt;https://github.com/RadeonOpenCompute/ROCm&lt;/a&gt;
General documentation on the ROCm platform is available at &lt;a href=&quot;https://rocmdocs.amd.com/en/latest/&quot;&gt;https://rocmdocs.amd.com/en/latest/&lt;/a&gt;
ROCm Learning Center at &lt;a href=&quot;https://developer.amd.com/resources/rocm-resources/rocm-learning-center/&quot;&gt;https://developer.amd.com/resources/rocm-resources/rocm-learning-center/&lt;/a&gt; General information on AMD’s offerings for HPC and ML can be found at &lt;a href=&quot;https://amd.com/hpc&quot;&gt;https://amd.com/hpc&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;feedback&quot;&gt;Feedback&lt;/h3&gt;
&lt;p&gt;An engaged user base is a tremendously important part of the PyTorch ecosystem. We would be deeply
appreciative of feedback on the PyTorch for ROCm experience in the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;PyTorch discussion forum&lt;/a&gt; and, where appropriate, reporting any issues via &lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;</content><author><name>Niles Burbank – Director PM at AMD, Mayank Daga – Director, Deep Learning Software at AMD</name></author><summary type="html">With the PyTorch 1.8 release, we are delighted to announce a new installation option for users of PyTorch on the ROCm™ open software platform. An installable Python package is now hosted on pytorch.org, along with instructions for local installation in the same simple, selectable format as PyTorch packages for CPU-only configurations and other GPU platforms. PyTorch on ROCm includes full capability for mixed-precision and large-scale training using AMD’s MIOpen &amp;amp; RCCL libraries. This provides a new option for data scientists, researchers, students, and others in the community to get started with accelerated PyTorch using AMD GPUs.</summary></entry><entry><title type="html">Announcing PyTorch Ecosystem Day</title><link href="https://pytorch.org/blog/ecosystem_day_2021/" rel="alternate" type="text/html" title="Announcing PyTorch Ecosystem Day" /><published>2021-03-09T00:00:00-08:00</published><updated>2021-03-09T00:00:00-08:00</updated><id>https://pytorch.org/blog/ecosystem_day_2021</id><content type="html" xml:base="https://pytorch.org/blog/ecosystem_day_2021/">&lt;p&gt;We’re proud to announce our first PyTorch Ecosystem Day. The virtual, one-day event will focus completely on our Ecosystem and Industry PyTorch communities!&lt;/p&gt;

&lt;p&gt;PyTorch is a deep learning framework of choice for academics and companies, all thanks to its rich ecosystem of tools and strong community. As with our developers, our ecosystem partners play a pivotal role in the development and growth of the community.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/ecosystem_day.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We will be hosting our first PyTorch Ecosystem Day, a virtual event designed for our ecosystem and industry communities to showcase their work and discover new opportunities to collaborate.&lt;/p&gt;

&lt;p&gt;PyTorch Ecosystem Day will be held on April 21, with both a morning and evening session, to ensure we reach our global community. Join us virtually for a day filled with discussions on new developments, trends, challenges, and best practices through keynotes, breakout sessions, and a unique networking opportunity hosted through Gather.Town .&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;Event Details&lt;/h2&gt;
&lt;p&gt;April 21, 2021 (Pacific Time)
Fully digital experience&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Morning Session: (EMEA)
Opening Talks - 8:00 am-9:00 am PT
Poster Exhibition &amp;amp; Breakout Sessions - 9:00 am-12:00 pm PT&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Evening Session (APAC/US)
Opening Talks - 3:00 pm-4:00 pm PT
Poster Exhibition &amp;amp; Breakout Sessions - 3:00 pm-6:00 pm PT&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Networking - 9:00 am-7:00 pm PT&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;there-are-two-ways-to-participate-in-pytorch-ecosystem-day&quot;&gt;There are two ways to participate in PyTorch Ecosystem Day:&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Poster Exhibition&lt;/strong&gt; from the PyTorch ecosystem and industry communities covering a variety of topics. Posters are available for viewing throughout the duration of the event. To be part of the poster exhibition, please see below for submission details. If your poster is accepted, we highly recommend tending your poster during one of the morning or evening sessions or both!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Breakout Sessions&lt;/strong&gt; are 40-min sessions freely designed by the community. The breakouts can be talks, demos, tutorials or discussions. Note: you must have an accepted poster to apply for the breakout sessions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Call for posters now open! &lt;a href=&quot;https://pytorchecosystemday.fbreg.com/posters&quot;&gt;Submit your proposal&lt;/a&gt; today! Please send us the &lt;strong&gt;title&lt;/strong&gt; and &lt;strong&gt;summary&lt;/strong&gt; of your projects, tools, and libraries that could benefit PyTorch researchers in academia and industry, application developers, and ML engineers for consideration. The focus must be on academic papers, machine learning research, or open-source projects. Please no sales pitches. &lt;strong&gt;Deadline for submission is March 18, 2021.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Visit &lt;a href=&quot;http://pytorchecosystemday.fbreg.com&quot;&gt;pytorchecosystemday.fbreg.com&lt;/a&gt; for more information and we look forward to welcoming you to PyTorch Ecosystem Day on April 21st!&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">We’re proud to announce our first PyTorch Ecosystem Day. The virtual, one-day event will focus completely on our Ecosystem and Industry PyTorch communities!</summary></entry><entry><title type="html">New PyTorch library releases including TorchVision Mobile, TorchAudio I/O, and more</title><link href="https://pytorch.org/blog/pytorch-1.8-new-library-releases/" rel="alternate" type="text/html" title="New PyTorch library releases including TorchVision Mobile, TorchAudio I/O, and more" /><published>2021-03-04T00:00:00-08:00</published><updated>2021-03-04T00:00:00-08:00</updated><id>https://pytorch.org/blog/pytorch-1.8-new-library-releases</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.8-new-library-releases/">&lt;p&gt;Today, we are announcing updates to a number of PyTorch libraries, alongside the &lt;a href=&quot;https://pytorch.org/blog/pytorch-1.8-released&quot;&gt;PyTorch 1.8 release&lt;/a&gt;. The updates include new releases for the domain libraries including TorchVision, TorchText and TorchAudio as well as new version of TorchCSPRNG. These releases include a number of new features and improvements and, along with the PyTorch 1.8 release, provide a broad set of updates for the PyTorch community to build on and leverage.&lt;/p&gt;

&lt;p&gt;Some highlights include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;TorchVision&lt;/strong&gt; - Added support for PyTorch Mobile including &lt;a href=&quot;https://ai.facebook.com/blog/d2go-brings-detectron2-to-mobile&quot;&gt;Detectron2Go&lt;/a&gt; (D2Go), auto-augmentation of data during training, on the fly type conversion, and &lt;a href=&quot;https://pytorch.org/docs/stable/amp.html&quot;&gt;AMP autocasting&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchAudio&lt;/strong&gt; - Major improvements to I/O, including defaulting to sox_io backend and file-like object support. Added Kaldi Pitch feature and support for CMake based build allowing TorchAudio to better support no-Python environments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchText&lt;/strong&gt; - Updated the dataset loading API to be compatible with standard PyTorch data loading utilities.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchCSPRNG&lt;/strong&gt; - Support for cryptographically secure pseudorandom number generators for PyTorch is now stable with new APIs for AES128 ECB/CTR and CUDA support on Windows.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please note that, starting in PyTorch 1.6, features are classified as Stable, Beta, and Prototype. Prototype features are not included as part of the binary distribution and are instead available through either building from source, using nightlies or via compiler flag. You can see the detailed announcement &lt;a href=&quot;https://pytorch.org/blog/pytorch-feature-classification-changes/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;torchvision-090&quot;&gt;TorchVision 0.9.0&lt;/h1&gt;
&lt;h3 id=&quot;stable-torchvision-mobile-operators-android-binaries-and-tutorial&quot;&gt;[Stable] TorchVision Mobile: Operators, Android Binaries, and Tutorial&lt;/h3&gt;
&lt;p&gt;We are excited to announce the first on-device support and binaries for a PyTorch domain library. We have seen significant appetite in both research and industry for on-device vision support to allow low latency, privacy friendly, and resource efficient mobile vision experiences. You can follow this &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/D2Go&quot;&gt;new tutorial&lt;/a&gt; to build your own Android object detection app using TorchVision operators, D2Go, or your own custom operators and model.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/tochvisionmobile.png&quot; width=&quot;30%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;stable-new-mobile-models-for-classification-object-detection-and-semantic-segmentation&quot;&gt;[Stable] New Mobile models for Classification, Object Detection and Semantic Segmentation&lt;/h3&gt;
&lt;p&gt;We have added support for the MobileNetV3 architecture and provided pre-trained weights for Classification, Object Detection and Segmentation. It is easy to get up and running with these models, just import and load them as you would any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision&lt;/code&gt; model:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Classification
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mobilenet_v3_large&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Quantized Classification
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mobilenet_v3_large&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Object Detection: Highly Accurate High Resolution Mobile Model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fasterrcnn_mobilenet_v3_large_fpn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Semantic Segmentation: Highly Accurate Mobile Model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;520&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;520&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_segmenter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;segmentation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deeplabv3_mobilenet_v3_large&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_segmenter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_segmenter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;These models are highly competitive with TorchVision’s existing models on resource efficiency, speed, and accuracy. See our &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;release notes&lt;/a&gt; for detailed performance metrics.&lt;/p&gt;

&lt;h3 id=&quot;stable-autoaugment&quot;&gt;[Stable] AutoAugment&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1805.09501.pdf&quot;&gt;AutoAugment&lt;/a&gt; is a common Data Augmentation technique that can increase the accuracy of Scene Classification models. Though the data augmentation policies are directly linked to their trained dataset, empirical studies show that ImageNet policies provide significant improvements when applied to other datasets. We’ve implemented 3 policies learned on the following datasets: ImageNet, CIFA10 and SVHN. These can be used standalone or mixed-and-matched with existing transforms:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AutoAugment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Compose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AutoAugment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;other-new-features-for-torchvision&quot;&gt;Other New Features for TorchVision&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;[Stable] All read and decode methods in the io.image package now support:
    &lt;ul&gt;
      &lt;li&gt;Palette, Grayscale Alpha and RBG Alpha image types during PNG decoding&lt;/li&gt;
      &lt;li&gt;On-the-fly conversion of image from one type to the other during read&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;[Stable] WiderFace dataset&lt;/li&gt;
  &lt;li&gt;[Stable] Improved FasterRCNN speed and accuracy by introducing a score threshold on RPN&lt;/li&gt;
  &lt;li&gt;[Stable] Modulation input for DeformConv2D&lt;/li&gt;
  &lt;li&gt;[Stable] Option to write audio to a video file&lt;/li&gt;
  &lt;li&gt;[Stable] Utility to draw bounding boxes&lt;/li&gt;
  &lt;li&gt;[Beta] Autocast support in all Operators
Find the full TorchVision release notes &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;torchaudio-080&quot;&gt;TorchAudio 0.8.0&lt;/h1&gt;
&lt;h3 id=&quot;io-improvements&quot;&gt;I/O Improvements&lt;/h3&gt;
&lt;p&gt;We have continued our work from the &lt;a href=&quot;https://github.com/pytorch/audio/releases/tag/v0.7.0&quot;&gt;previous release&lt;/a&gt; to improve TorchAudio’s I/O support, including:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;[Stable] Changing the default backend to “sox_io” (for Linux/macOS), and updating the “soundfile” backend’s interface to align with that of “sox_io”. The legacy backend and interface are still accessible, though it is strongly discouraged to use them.&lt;/li&gt;
  &lt;li&gt;[Stable] File-like object support in both “sox_io” backend, “soundfile” backend and sox_effects.&lt;/li&gt;
  &lt;li&gt;[Stable] New options to change the format, encoding, and bits_per_sample when saving.&lt;/li&gt;
  &lt;li&gt;[Stable] Added GSM, HTK, AMB, AMR-NB and AMR-WB format support to the “sox_io” backend.&lt;/li&gt;
  &lt;li&gt;[Beta] A new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;functional.apply_codec&lt;/code&gt; function which can degrade audio data by applying audio codecs supported by “sox_io” backend in an in-memory fashion.
Here are some examples of features landed in this release:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Load audio over HTTP
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;waveform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;c1&quot;&gt;# Saving to Bytes buffer as 32-bit floating-point PCM
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buffer_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BytesIO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;buffer_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;waveform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;wav&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;PCM_S&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits_per_sample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;c1&quot;&gt;# Apply effects while loading audio from S3
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boto3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Bucket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S3_BUCKET&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S3_KEY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;waveform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sox_effects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply_effect_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Body'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lowpass&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;-1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;300&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;8000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
 
&lt;span class=&quot;c1&quot;&gt;# Apply GSM codec to Tensor
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply_codec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;waveform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gsm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check out the revamped audio preprocessing tutorial, &lt;a href=&quot;https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html&quot;&gt;Audio Manipulation with TorchAudio&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stable-switch-to-cmake-based-build&quot;&gt;[Stable] Switch to CMake-based build&lt;/h3&gt;
&lt;p&gt;In the previous version of TorchAudio, it was utilizing CMake to build third party dependencies. Starting in 0.8.0, TorchaAudio uses CMake to build its C++ extension. This will open the door to integrate TorchAudio in non-Python environments (such as C++ applications and mobile). We will continue working on adding example applications and mobile integrations.&lt;/p&gt;

&lt;h3 id=&quot;beta-improved-and-new-audio-transforms&quot;&gt;[Beta] Improved and New Audio Transforms&lt;/h3&gt;
&lt;p&gt;We have added two widely requested operators in this release: the SpectralCentroid transform and the Kaldi Pitch feature extraction (detailed in &lt;a href=&quot;https://ieeexplore.ieee.org/document/6854049&quot;&gt;“A pitch extraction algorithm tuned for automatic speech recognition”&lt;/a&gt;). We’ve also exposed a normalization method to Mel transforms, and additional STFT arguments to Spectrogram. We would like to ask  our community to continue to &lt;a href=&quot;https://github.com/pytorch/audio/issues/new?assignees=&amp;amp;labels=&amp;amp;template=feature-request.md&quot;&gt;raise feature requests&lt;/a&gt; for core audio processing features like these!&lt;/p&gt;

&lt;h3 id=&quot;community-contributions&quot;&gt;Community Contributions&lt;/h3&gt;
&lt;p&gt;We had more contributions from the open source community in this release than ever before, including several completely new features. We would like to extend our sincere thanks to the community. Please check out the newly added &lt;a href=&quot;https://github.com/pytorch/audio/blob/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for ways to contribute code, and remember that reporting bugs and requesting features are just as valuable. We will continue posting well-scoped work items as issues labeled “help-wanted” and “contributions-welcome” for anyone who would like to contribute code, and are happy to coach new contributors through the contribution process.&lt;/p&gt;

&lt;p&gt;Find the full TorchAudio release notes &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;torchtext-090&quot;&gt;TorchText 0.9.0&lt;/h1&gt;
&lt;h3 id=&quot;beta-dataset-api-updates&quot;&gt;[Beta] Dataset API Updates&lt;/h3&gt;
&lt;p&gt;In this release, we are updating TorchText’s dataset API to be compatible with PyTorch data utilities, such as DataLoader, and are deprecating TorchText’s custom data abstractions such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Field&lt;/code&gt;. The updated datasets are simple string-by-string iterators over the data. For guidance about migrating from the legacy abstractions to use modern PyTorch data utilities, please refer to our &lt;a href=&quot;https://github.com/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb&quot;&gt;migration guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The text datasets listed below have been updated as part of this work. For examples of how to use these datasets, please refer to our &lt;a href=&quot;https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;end-to-end text classification tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Language modeling:&lt;/strong&gt; WikiText2, WikiText103, PennTreebank, EnWik9&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Text classification:&lt;/strong&gt; AG_NEWS, SogouNews, DBpedia, YelpReviewPolarity, YelpReviewFull, YahooAnswers, AmazonReviewPolarity, AmazonReviewFull, IMDB&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sequence tagging:&lt;/strong&gt; UDPOS, CoNLL2000Chunking&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Translation:&lt;/strong&gt; IWSLT2016, IWSLT2017&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Question answer:&lt;/strong&gt; SQuAD1, SQuAD2&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Find the full TorchText release notes &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;stable-torchcsprng-020&quot;&gt;[Stable] TorchCSPRNG 0.2.0&lt;/h1&gt;
&lt;p&gt;We &lt;a href=&quot;https://pytorch.org/blog/torchcsprng-release-blog/&quot;&gt;released TorchCSPRNG in August 2020&lt;/a&gt;, a PyTorch C++/CUDA extension that provides cryptographically secure pseudorandom number generators for PyTorch. Today, we are releasing the 0.2.0 version and designating the library as stable. This release includes a new API for encrypt/decrypt with AES128 ECB/CTR as well as CUDA 11 and Windows CUDA support.&lt;/p&gt;

&lt;p&gt;Find the full TorchCSPRNG release notes &lt;a href=&quot;https://github.com/pytorch/csprng/releases/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks for reading, and if you are excited about these updates and want to participate in the future of PyTorch, we encourage you to join the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;discussion forums&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch&quot;&gt;open GitHub issues&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Team PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we are announcing updates to a number of PyTorch libraries, alongside the PyTorch 1.8 release. The updates include new releases for the domain libraries including TorchVision, TorchText and TorchAudio as well as new version of TorchCSPRNG. These releases include a number of new features and improvements and, along with the PyTorch 1.8 release, provide a broad set of updates for the PyTorch community to build on and leverage.</summary></entry><entry><title type="html">PyTorch 1.8 Release, including Compiler and Distributed Training updates, and New Mobile Tutorials</title><link href="https://pytorch.org/blog/pytorch-1.8-released/" rel="alternate" type="text/html" title="PyTorch 1.8 Release, including Compiler and Distributed Training updates, and New Mobile Tutorials" /><published>2021-03-04T00:00:00-08:00</published><updated>2021-03-04T00:00:00-08:00</updated><id>https://pytorch.org/blog/pytorch-1.8-released</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.8-released/">&lt;p&gt;We are excited to announce the availability of PyTorch 1.8. This release is composed of more than 3,000 commits since 1.7. It includes major updates and new features for compilation, code optimization, frontend APIs for scientific computing, and AMD ROCm support through binaries that are available via pytorch.org. It also provides improved features for large-scale training for pipeline and model parallelism, and gradient compression. A few of the highlights include:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Support for doing python to python functional transformations via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fx&lt;/code&gt;;&lt;/li&gt;
  &lt;li&gt;Added or stabilized APIs to support FFTs (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt;), Linear Algebra functions (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt;), added support for autograd for complex tensors and updates to improve performance for calculating hessians and jacobians; and&lt;/li&gt;
  &lt;li&gt;Significant updates and improvements to distributed training including: Improved NCCL reliability; Pipeline parallelism support; RPC profiling; and support for communication hooks adding gradient compression.
See the full release notes &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Along with 1.8, we are also releasing major updates to PyTorch libraries including &lt;a href=&quot;https://github.com/pytorch/csprng&quot;&gt;TorchCSPRNG&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/vision&quot;&gt;TorchVision&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/text&quot;&gt;TorchText&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/audio&quot;&gt;TorchAudio&lt;/a&gt;. For more on the library releases, see the post &lt;a href=&quot;http://pytorch.org/blog/pytorch-1.8-new-library-releases&quot;&gt;here&lt;/a&gt;. As previously noted, features in PyTorch releases are classified as Stable, Beta and Prototype. You can learn more about the definitions in the post &lt;a href=&quot;https://pytorch.org/blog/pytorch-feature-classification-changes/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;new-and-updated-apis&quot;&gt;New and Updated APIs&lt;/h1&gt;
&lt;p&gt;The PyTorch 1.8 release brings a host of new and updated API surfaces ranging from additional APIs for NumPy compatibility, also support for ways to improve and scale your code for performance at both inference and training time. Here is a brief summary of the major features coming in this release:&lt;/p&gt;

&lt;h3 id=&quot;stable-torchfft-support-for-high-performance-numpy-style-ffts&quot;&gt;[Stable] &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Torch.fft&lt;/code&gt; support for high performance NumPy style FFTs&lt;/h3&gt;
&lt;p&gt;As part of PyTorch’s goal to support scientific computing, we have invested in improving our FFT support and with PyTorch 1.8, we are releasing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module. This module implements the same functions as NumPy’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module, but with support for hardware acceleration and autograd.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;See this &lt;a href=&quot;https://pytorch.org/blog/the-torch.fft-module-accelerated-fast-fourier-transforms-with-autograd-in-pyTorch/&quot;&gt;blog post&lt;/a&gt; for more details&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/1.8.0/fft.html&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;beta-support-for-numpy-style-linear-algebra-functions-via-torchlinalg&quot;&gt;[Beta] Support for NumPy style linear algebra functions via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; module, modeled after NumPy’s &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.linalg.html?highlight=linalg#module-numpy.linalg&quot;&gt;np.linalg&lt;/a&gt; module, brings NumPy-style support for common linear algebra operations including Cholesky decompositions, determinants, eigenvalues and many others.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/1.8.0/linalg.html&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-python-code-transformations-with-fx&quot;&gt;[Beta] Python code Transformations with FX&lt;/h2&gt;
&lt;p&gt;FX allows you to write transformations of the form &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transform(input_module : nn.Module)&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt;, where you can feed in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Module&lt;/code&gt; instance and get a transformed &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Module&lt;/code&gt; instance out of it.&lt;/p&gt;

&lt;p&gt;This kind of functionality is applicable in many scenarios. For example, the FX-based Graph Mode Quantization product is releasing as a prototype contemporaneously with FX. Graph Mode Quantization automates the process of quantizing a neural net and does so by leveraging FX’s program capture, analysis and transformation facilities. We are also developing many other transformation products with FX and we are excited to share this powerful toolkit with the community.&lt;/p&gt;

&lt;p&gt;Because FX transforms consume and produce nn.Module instances, they can be used within many existing PyTorch workflows. This includes workflows that, for example, train in Python then deploy via TorchScript.&lt;/p&gt;

&lt;p&gt;You can read more about FX in the official &lt;a href=&quot;https://pytorch.org/docs/master/fx.html&quot;&gt;documentation&lt;/a&gt;. You can also find several examples of program transformations implemented using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fx&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/fx&quot;&gt;here&lt;/a&gt;. We are constantly improving FX and invite you to share any feedback you have about the toolkit on the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;forums&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/pytorch/issues&quot;&gt;issue tracker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We’d like to acknowledge &lt;a href=&quot;https://pytorch.org/docs/stable/jit.html&quot;&gt;TorchScript&lt;/a&gt; tracing, &lt;a href=&quot;https://mxnet.apache.org/versions/1.7.0/&quot;&gt;Apache MXNet&lt;/a&gt; hybridize, and more recently &lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; as influences for program acquisition via tracing. We’d also like to acknowledge &lt;a href=&quot;https://caffe2.ai/&quot;&gt;Caffe2&lt;/a&gt;, &lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt;, and &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt; as inspiration for the value of simple, directed dataflow graph program representations and transformations over those representations.&lt;/p&gt;

&lt;h1 id=&quot;distributed-training&quot;&gt;Distributed Training&lt;/h1&gt;
&lt;p&gt;The PyTorch 1.8 release added a number of new features as well as improvements to reliability and usability. Concretely, support for: &lt;a href=&quot;https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group&quot;&gt;Stable level async error/timeout handling&lt;/a&gt; was added to improve NCCL reliability; and stable support for &lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html&quot;&gt;RPC based profiling&lt;/a&gt;. Additionally, we have added support for pipeline parallelism as well as gradient compression through the use of communication hooks in DDP. Details are below:&lt;/p&gt;

&lt;h3 id=&quot;beta-pipeline-parallelism&quot;&gt;[Beta] Pipeline Parallelism&lt;/h3&gt;
&lt;p&gt;As machine learning models continue to grow in size, traditional Distributed DataParallel (DDP) training no longer scales as these models don’t fit on a single GPU device. The new pipeline parallelism feature provides an easy to use PyTorch API to leverage pipeline parallelism as part of your training loop.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/44827&quot;&gt;RFC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/1.8.0/pipeline.html?highlight=pipeline#&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;beta-ddp-communication-hook&quot;&gt;[Beta] DDP Communication Hook&lt;/h3&gt;
&lt;p&gt;The DDP communication hook is a generic interface to control how to communicate gradients across workers by overriding the vanilla allreduce in DistributedDataParallel. A few built-in communication hooks are provided including PowerSGD, and users can easily apply any of these hooks to optimize communication. Additionally, the communication hook interface can also support user-defined communication strategies for more advanced use cases.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/39272&quot;&gt;RFC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/1.8.0/ddp_comm_hooks.html?highlight=powersgd&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;additional-prototype-features-for-distributed-training&quot;&gt;Additional Prototype Features for Distributed Training&lt;/h3&gt;
&lt;p&gt;In addition to the major stable and beta distributed training features in this release, we also have a number of prototype features available in our nightlies to try out and provide feedback. We have linked in the draft docs below for reference:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;(Prototype) ZeroRedundancyOptimizer&lt;/strong&gt; - Based on and in partnership with the Microsoft DeepSpeed team, this feature helps reduce per-process memory footprint by sharding optimizer states across all participating processes in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ProcessGroup&lt;/code&gt; gang. Refer to this &lt;a href=&quot;https://pytorch.org/docs/master/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer&quot;&gt;documentation&lt;/a&gt; for more details.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(Prototype) Process Group NCCL Send/Recv&lt;/strong&gt; - The NCCL send/recv API was introduced in v2.7 and this feature adds support for it in NCCL process groups. This feature will provide an option for users to implement collective operations at Python layer instead of C++ layer. Refer to this &lt;a href=&quot;https://pytorch.org/docs/master/distributed.html#distributed-communication-package-torch-distributed&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/distributed/distributed_c10d.py#L899&quot;&gt;code examples&lt;/a&gt; to learn more.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(Prototype) CUDA-support in RPC using TensorPipe&lt;/strong&gt; - This feature should bring consequent speed improvements for users of PyTorch RPC with multiple-GPU machines, as TensorPipe will automatically leverage NVLink when available, and avoid costly copies to and from host memory when exchanging GPU tensors between processes. When not on the same machine, TensorPipe will fall back to copying the tensor to host memory and sending it as a regular CPU tensor. This will also improve the user experience as users will be able to treat GPU tensors like regular CPU tensors in their code. Refer to this &lt;a href=&quot;https://pytorch.org/docs/1.8.0/rpc.html&quot;&gt;documentation&lt;/a&gt; for more details.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(Prototype) Remote Module&lt;/strong&gt; - This feature allows users to operate a module on a remote worker like using a local module, where the RPCs are transparent to the user. In the past, this functionality was implemented in an ad-hoc way and overall this feature will improve the usability of model parallelism on PyTorch. Refer to this &lt;a href=&quot;https://pytorch.org/docs/master/rpc.html#remotemodule&quot;&gt;documentation&lt;/a&gt; for more details.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;pytorch-mobile&quot;&gt;PyTorch Mobile&lt;/h1&gt;
&lt;p&gt;Support for PyTorch Mobile is expanding with a new set of tutorials to help new users launch models on-device quicker and give existing users a tool to get more out of our framework. These include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/deeplabv3_on_ios.html&quot;&gt;Image segmentation DeepLabV3 on iOS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/deeplabv3_on_android.html&quot;&gt;Image segmentation DeepLabV3 on Android&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our new demo apps also include examples of image segmentation, object detection, neural machine translation, question answering, and vision transformers. They are available on both iOS and Android:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/ios-demo-app&quot;&gt;iOS demo app&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/android-demo-app&quot;&gt;Android demo app&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to performance improvements on CPU for MobileNetV3 and other models, we also revamped our Android GPU backend prototype for broader models coverage and faster inferencing:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/prototype/vulkan_workflow.html&quot;&gt;Android tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lastly, we are launching the PyTorch Mobile Lite Interpreter as a prototype feature in this release. The Lite Interpreter allows users to reduce the runtime binary size. Please try these out and send us your feedback on the &lt;a href=&quot;https://discuss.pytorch.org/c/mobile/&quot;&gt;PyTorch Forums&lt;/a&gt;. All our latest updates can be found on the &lt;a href=&quot;https://pytorch.org/mobile/home/&quot;&gt;PyTorch Mobile page&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;prototype-pytorch-mobile-lite-interpreter&quot;&gt;[Prototype] PyTorch Mobile Lite Interpreter&lt;/h3&gt;
&lt;p&gt;PyTorch Lite Interpreter is a streamlined version of the PyTorch runtime that can execute PyTorch programs in resource constrained devices, with reduced binary size footprint. This prototype feature reduces binary sizes by up to 70% compared to the current on-device runtime in the current release.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/prototype/lite_interpreter.html&quot;&gt;iOS/Android Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;performance-optimization&quot;&gt;Performance Optimization&lt;/h1&gt;
&lt;p&gt;In 1.8, we are releasing the support for benchmark utils to enable users to better monitor performance. We are also opening up a new automated quantization API. See the details below:&lt;/p&gt;

&lt;h3 id=&quot;beta-benchmark-utils&quot;&gt;(Beta) Benchmark utils&lt;/h3&gt;
&lt;p&gt;Benchmark utils allows users to take accurate performance measurements, and provides composable tools to help with both benchmark formulation and post processing. This expected to be helpful for contributors to PyTorch to quickly understand how their contributions are impacting PyTorch performance.&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.benchmark&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;timer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;stmt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;torch.add(x, y, out=out)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;setup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
            n = 1024
            x = torch.ones((n, n))
            y = torch.ones((n, 1))
            out = torch.empty((n, n))
        &quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blocked_autorange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_run_time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; thread&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
        &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;median&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; us   &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;(&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;median&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x)&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;376&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;   
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threads&lt;/span&gt;    &lt;span class=&quot;mi&quot;&gt;189&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threads&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.8&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/1.8.0/benchmark_utils.html?highlight=benchmark#&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/benchmark.html&quot;&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prototype-fx-graph-mode-quantization&quot;&gt;(Prototype) FX Graph Mode Quantization&lt;/h3&gt;
&lt;p&gt;FX Graph Mode Quantization is the new automated quantization API in PyTorch. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fx&lt;/code&gt;).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tutorials:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html&quot;&gt;(Prototype) FX Graph Mode Post Training Dynamic Quantization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html&quot;&gt;(Prototype) FX Graph Mode Post Training Static Qunatization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html&quot;&gt;(Prototype) FX Graph Mode Quantization User Guide&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hardware-support&quot;&gt;Hardware Support&lt;/h1&gt;

&lt;h3 id=&quot;beta-ability-to-extend-the-pytorch-dispatcher-for-a-new-backend-in-c&quot;&gt;[Beta] Ability to Extend the PyTorch Dispatcher for a new backend in C++&lt;/h3&gt;
&lt;p&gt;In PyTorch 1.8, you can now create new out-of-tree devices that live outside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pytorch/pytorch&lt;/code&gt; repo. The tutorial linked below shows how to register your device and keep it in sync with native PyTorch devices.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/extend_dispatcher.html&quot;&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;beta-amd-gpu-binaries-now-available&quot;&gt;[Beta] AMD GPU Binaries Now Available&lt;/h3&gt;
&lt;p&gt;Starting in PyTorch 1.8, we have added support for ROCm wheels providing an easy onboarding to using AMD GPUs. You can simply go to the standard &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;PyTorch installation selector&lt;/a&gt; and choose ROCm as an installation option and execute the provided command.&lt;/p&gt;

&lt;p&gt;Thanks for reading, and if you are excited about these updates and want to participate in the future of PyTorch, we encourage you to join the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;discussion forums&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/issues&quot;&gt;open GitHub issues&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Team PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">We are excited to announce the availability of PyTorch 1.8. This release is composed of more than 3,000 commits since 1.7. It includes major updates and new features for compilation, code optimization, frontend APIs for scientific computing, and AMD ROCm support through binaries that are available via pytorch.org. It also provides improved features for large-scale training for pipeline and model parallelism, and gradient compression. A few of the highlights include: Support for doing python to python functional transformations via torch.fx; Added or stabilized APIs to support FFTs (torch.fft), Linear Algebra functions (torch.linalg), added support for autograd for complex tensors and updates to improve performance for calculating hessians and jacobians; and Significant updates and improvements to distributed training including: Improved NCCL reliability; Pipeline parallelism support; RPC profiling; and support for communication hooks adding gradient compression. See the full release notes here.</summary></entry><entry><title type="html">The torch.fft module: Accelerated Fast Fourier Transforms with Autograd in PyTorch</title><link href="https://pytorch.org/blog/the-torch.fft-module-accelerated-fast-fourier-transforms-with-autograd-in-pyTorch/" rel="alternate" type="text/html" title="The torch.fft module: Accelerated Fast Fourier Transforms with Autograd in PyTorch" /><published>2021-03-03T00:00:00-08:00</published><updated>2021-03-03T00:00:00-08:00</updated><id>https://pytorch.org/blog/the-torch.fft-module-accelerated-fast-fourier-transforms-with-autograd-in-pyTorch</id><content type="html" xml:base="https://pytorch.org/blog/the-torch.fft-module-accelerated-fast-fourier-transforms-with-autograd-in-pyTorch/">&lt;p&gt;The Fast Fourier Transform (FFT) calculates the Discrete Fourier Transform in O(n log n) time. It is foundational to a wide variety of numerical algorithms and signal processing techniques since it makes working in signals’ “frequency domains” as tractable as working in their spatial or temporal domains.&lt;/p&gt;

&lt;p&gt;As part of PyTorch’s goal to support hardware-accelerated deep learning and scientific computing, we have invested in improving our FFT support, and with PyTorch 1.8, we are releasing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module. This module implements the same functions as NumPy’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module, but with support for accelerators, like GPUs, and autograd.&lt;/p&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting started&lt;/h2&gt;

&lt;p&gt;Getting started with the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module is easy whether you are familiar with NumPy’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module or not. While complete documentation for each function in the module can be found &lt;a href=&quot;https://pytorch.org/docs/1.8.0/fft.html&quot;&gt;here&lt;/a&gt;, a breakdown of what it offers is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fft&lt;/code&gt;, which computes a complex FFT over a single dimension, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ifft&lt;/code&gt;, its inverse&lt;/li&gt;
  &lt;li&gt;the more general &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fftn&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ifftn&lt;/code&gt;, which support multiple dimensions&lt;/li&gt;
  &lt;li&gt;The “real” FFT functions, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rfft&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;irfft&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rfftn&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;irfftn&lt;/code&gt;,  designed to work with signals that are real-valued in their time domains&lt;/li&gt;
  &lt;li&gt;The “Hermitian” FFT functions, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hfft&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ihfft&lt;/code&gt;, designed to work with signals that are real-valued in their frequency domains&lt;/li&gt;
  &lt;li&gt;Helper functions, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fftfreq&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rfftfreq&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fftshift&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ifftshift&lt;/code&gt;, that make it easier to manipulate signals&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We think these functions provide a straightforward interface for FFT functionality, as vetted by the NumPy community, although we are always interested in feedback and suggestions!&lt;/p&gt;

&lt;p&gt;To better illustrate how easy it is to move from NumPy’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module to PyTorch’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module, let’s look at a NumPy implementation of a simple low-pass filter that removes high-frequency variance from a 2-dimensional image, a form of noise reduction or blurring:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.fft&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lowpass_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pass1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rfftfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pass2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fftfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pass2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pass1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;fft_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rfft2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;irfft2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now let’s see the same filter implemented in PyTorch:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.fft&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lowpass_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pass1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rfftfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pass2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fftfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pass2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pass1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;fft_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rfft2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;irfft2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Not only do current uses of NumPy’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module translate directly to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt;, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; operations also support tensors on accelerators, like GPUs and autograd. This makes it possible to (among other things) develop new neural network modules using the FFT.&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module is not only easy to use — it is also fast! PyTorch natively supports Intel’s MKL-FFT library on Intel CPUs, and NVIDIA’s cuFFT library on CUDA devices, and we have carefully optimized how we use those libraries to maximize performance. While your own results will depend on your CPU and CUDA hardware, computing Fast Fourier Transforms on CUDA devices can be many times faster than computing it on the CPU, especially for larger signals.&lt;/p&gt;

&lt;p&gt;In the future, we may add support for additional math libraries to support more hardware. See below for where you can request additional hardware support.&lt;/p&gt;

&lt;h2 id=&quot;updating-from-older-pytorch-versions&quot;&gt;Updating from older PyTorch versions&lt;/h2&gt;

&lt;p&gt;Some PyTorch users might know that older versions of PyTorch also offered FFT functionality with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fft()&lt;/code&gt; function. Unfortunately, this function had to be removed because its name conflicted with the new module’s name, and we think the new functionality is the best way to use the Fast Fourier Transform in PyTorch. In particular, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fft()&lt;/code&gt; was developed before PyTorch supported complex tensors, while the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module was designed to work with them.&lt;/p&gt;

&lt;p&gt;PyTorch also has a “Short Time Fourier Transform”, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.stft&lt;/code&gt;, and its inverse &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.istft&lt;/code&gt;. These functions are being kept but updated to support complex tensors.&lt;/p&gt;

&lt;h2 id=&quot;future&quot;&gt;Future&lt;/h2&gt;

&lt;p&gt;As mentioned, PyTorch 1.8 offers the torch.fft module, which makes it easy to use the Fast Fourier Transform (FFT) on accelerators and with support for autograd. We encourage you to try it out!&lt;/p&gt;

&lt;p&gt;While this module has been modeled after NumPy’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module so far, we are not stopping there. We are eager to hear from you, our community, on what FFT-related functionality you need, and we encourage you to create posts on our forums at &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;https://discuss.pytorch.org/&lt;/a&gt;, or &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/new?assignees=&amp;amp;labels=&amp;amp;template=feature-request.md&quot;&gt;file issues on our Github&lt;/a&gt; with your feedback and requests. Early adopters have already started asking about Discrete Cosine Transforms and support for more hardware platforms, for example, and we are investigating those features now.&lt;/p&gt;

&lt;p&gt;We look forward to hearing from you and seeing what the community does with PyTorch’s new FFT functionality!&lt;/p&gt;</content><author><name>Mike Ruberry, Peter Bell, and Joe Spisak</name></author><summary type="html">The Fast Fourier Transform (FFT) calculates the Discrete Fourier Transform in O(n log n) time. It is foundational to a wide variety of numerical algorithms and signal processing techniques since it makes working in signals’ “frequency domains” as tractable as working in their spatial or temporal domains.</summary></entry></feed>