<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pytorch.org/" rel="alternate" type="text/html" /><updated>2020-08-11T11:49:51-07:00</updated><id>https://pytorch.org/</id><title type="html">PyTorch Website</title><subtitle>Scientific Computing...</subtitle><author><name>Facebook</name></author><entry><title type="html">Efficient PyTorch I/O library for Large Datasets, Many Files, Many GPUs</title><link href="https://pytorch.org/blog/efficient-pytorch-io-library-for-large-datasets-many-files-many-gpus/" rel="alternate" type="text/html" title="Efficient PyTorch I/O library for Large Datasets, Many Files, Many GPUs" /><published>2020-08-11T00:00:00-07:00</published><updated>2020-08-11T00:00:00-07:00</updated><id>https://pytorch.org/blog/efficient-pytorch-io-library-for-large-datasets-many-files-many-gpus</id><content type="html" xml:base="https://pytorch.org/blog/efficient-pytorch-io-library-for-large-datasets-many-files-many-gpus/">&lt;p&gt;The size of data sets are growing bigger and faster every day. This means there are more data sets for deep learning researchers and engineers to train and validate their models.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Many datasets for research in still image recognition are becoming available with 10 million or more images, including OpenImages and Places.&lt;/li&gt;
  &lt;li&gt;million YouTube videos &lt;a href=&quot;https://research.google.com/youtube8m/&quot;&gt;(YouTube 8M)&lt;/a&gt; consume about 300 TB in 720p, used for research in object recognition, video analytics, and action recognition.&lt;/li&gt;
  &lt;li&gt;The Tobacco Corpus consists of about 20 million scanned HD pages, useful for OCR and text analytics research.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although the most commonly encountered big data sets right now involve images and videos, big datasets occur in many other domains and involve many other kinds of data types: web pages, financial transactions, network traces, brain scans, etc.&lt;/p&gt;

&lt;p&gt;However, working with the large amount of data sets presents a number of challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dataset Size:&lt;/strong&gt; datasets often exceed the capacity of node-local disk storage, requiring distributed storage systems and efficient network access.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of Files:&lt;/strong&gt; datasets often consist of billions of files with uniformly random access patterns, something that often overwhelms both local and network file systems.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Rates:&lt;/strong&gt; training jobs on large datasets often use many GPUs, requiring aggregate I/O bandwidths to the dataset of many GBytes/s; these can only be satisfied by massively parallel I/O systems.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Shuffling and Augmentation:&lt;/strong&gt; training data needs to be shuffled and augmented prior to training.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; users often want to develop and test on small datasets and then rapidly scale up to large datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Traditional local and network file systems, and even object storage servers, are not designed for these kinds of applications. &lt;a href=&quot;https://github.com/tmbdev/webdataset&quot;&gt;The WebDataset I/O library&lt;/a&gt; for PyTorch, together with the optional &lt;a href=&quot;https://github.com/NVIDIA/aistore&quot;&gt;AIStore server&lt;/a&gt; and &lt;a href=&quot;https://github.com/NVlabs/tensorcom&quot;&gt;Tensorcom&lt;/a&gt; RDMA libraries, provide an efficient, simple, and standards-based solution to all these problems. The library is simple enough for day-to-day use, is based on mature open source standards, and is easy to migrate to from existing file-based datasets.&lt;/p&gt;

&lt;p&gt;Using WebDataset is simple and requires little effort, and it will let you scale up the same code from running local experiments to using hundreds of GPUs on clusters or in the cloud with linearly scalable performance. Even on small problems and on your desktop, it can speed up I/O tenfold and simplifies data management and processing of large datasets. The rest of this blog post tells you how to get started with WebDataset and how it works.&lt;/p&gt;

&lt;h2 id=&quot;the-webdataset-library&quot;&gt;The WebDataset Library&lt;/h2&gt;

&lt;p&gt;The WebDataset library provides a simple solution to the challenges listed above. Currently, it is available as a separate library &lt;a href=&quot;https://github.com/tmbdev/webdataset&quot;&gt;(github.com/tmbdev/webdataset)&lt;/a&gt;, but it is on track for being incorporated into PyTorch (see &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/38419&quot;&gt;RFC 38419&lt;/a&gt;).  The WebDataset implementation is small (about 1500 LOC) and has no external dependencies.&lt;/p&gt;

&lt;p&gt;Instead of inventing a new format, WebDataset represents large datasets as collections of POSIX tar archive files consisting of the original data files. The WebDataset library can use such tar archives directly for training, without the need for unpacking or local storage.&lt;/p&gt;

&lt;p&gt;WebDataset scales perfectly from small, local datasets to petascale datasets and training on hundreds of GPUs and allows data to be stored on local disk, on web servers, or dedicated file servers. For container-based training, WebDataset eliminates the need for volume plugins or node-local storage. As an additional benefit, datasets need not be unpacked prior to training, simplifying the distribution and use of research data.&lt;/p&gt;

&lt;p&gt;WebDataset implements PyTorch’s &lt;a href=&quot;https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset&quot;&gt;IterableDataset&lt;/a&gt; interface and can be used like existing DataLoader-based code. Since data is stored as files inside an archive, existing loading and data augmentation code usually requires minimal modification.&lt;/p&gt;

&lt;p&gt;The WebDataset library is a complete solution for working with large datasets and distributed training in PyTorch (and also works with TensorFlow, Keras, and DALI via their Python APIs). Since POSIX tar archives are a standard, widely supported format, it is easy to write other tools for manipulating datasets in this format. E.g., the &lt;a href=&quot;https://github.com/tmbdev/tarp&quot;&gt;tarp&lt;/a&gt; command is written in Go and can shuffle and process training datasets.&lt;/p&gt;

&lt;h2 id=&quot;benefits&quot;&gt;Benefits&lt;/h2&gt;

&lt;p&gt;The use of sharded, sequentially readable formats is essential for very large datasets. In addition, it has benefits in many other environments. WebDataset provides a solution that scales well from small problems on a desktop machine to very large deep learning problems in clusters or in the cloud. The following table summarizes some of the benefits in different environments.&lt;/p&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Environment&lt;/th&gt;
      &lt;th&gt;Benefits of WebDataset&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Local Cluster with AIStore&lt;/td&gt;
      &lt;td&gt;AIStore can be deployed easily as K8s containers and offers linear scalability and near 100% utilization of network and I/O bandwidth. Suitable for petascale deep learning.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cloud Computing&lt;/td&gt;
      &lt;td&gt;WebDataset deep learning jobs can be trained directly against datasets stored in cloud buckets; no volume plugins required. Local and cloud jobs work identically. Suitable for petascale learning.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Local Cluster with existing distributed FS or object store&lt;/td&gt;
      &lt;td&gt;WebDataset’s large sequential reads improve performance with existing distributed stores and eliminate the need for dedicated volume plugins.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Educational Environments&lt;/td&gt;
      &lt;td&gt;WebDatasets can be stored on existing web servers and web caches, and can be accessed directly by students by URL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Training on Workstations from Local Drives&lt;/td&gt;
      &lt;td&gt;Jobs can start training as the data still downloads. Data doesn’t need to be unpacked for training. Ten-fold improvements in I/O performance on hard drives over random access file-based datasets.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;All Environments&lt;/td&gt;
      &lt;td&gt;Datasets are represented in an archival format and contain metadata such as file types. Data is compressed in native formats (JPEG, MP4, etc.). Data management, ETL-style jobs, and data transformations and I/O are simplified and easily parallelized.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We will be adding more examples giving benchmarks and showing how to use WebDataset in these environments over the coming months.&lt;/p&gt;

&lt;h2 id=&quot;high-performance&quot;&gt;High-Performance&lt;/h2&gt;
&lt;p&gt;For high-performance computation on local clusters, the companion open-source &lt;a href=&quot;https://github.com/NVIDIA/AIStore&quot;&gt;AIStore&lt;/a&gt; server provides full disk to GPU I/O bandwidth, subject only to hardware constraints. &lt;a href=&quot;https://arxiv.org/abs/2001.01858&quot;&gt;This Bigdata 2019 Paper&lt;/a&gt; contains detailed benchmarks and performance measurements. In addition to benchmarks, research projects at NVIDIA and Microsoft have used WebDataset for petascale datasets and billions of training samples.&lt;/p&gt;

&lt;p&gt;Below is a benchmark of AIStore with WebDataset clients using 10 server nodes and 120 rotational drives each.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/pytorchwebdataset1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The left axis shows the aggregate bandwidth from the cluster, while the right scale shows the measured per drive I/O bandwidth. WebDataset and AIStore scale linearly to about 300 clients, at which point they are increasingly limited by the maximum I/O bandwidth available from the rotational drives (about 150 MBytes/s per drive). For comparison, HDFS is shown. HDFS uses a similar approach to AIStore/WebDataset and also exhibits linear scaling up to about 192 clients; at that point, it hits a performance limit of about 120 MBytes/s per drive, and it failed when using more than 1024 clients. Unlike HDFS, the WebDataset-based code just uses standard URLs and HTTP to access data and works identically with local files, with files stored on web servers, and with AIStore. For comparison, NFS in similar experiments delivers about 10-20 MBytes/s per drive.&lt;/p&gt;

&lt;h2 id=&quot;storing-datasets-in-tar-archives&quot;&gt;Storing Datasets in Tar Archives&lt;/h2&gt;

&lt;p&gt;The format used for WebDataset is standard POSIX tar archives, the same archives used for backup and data distribution. In order to use the format to store training samples for deep learning, we adopt some simple naming conventions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;datasets are POSIX tar archives&lt;/li&gt;
  &lt;li&gt;each training sample consists of adjacent files with the same basename&lt;/li&gt;
  &lt;li&gt;shards are numbered consecutively&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, ImageNet is stored in 1282 separate 100 Mbyte shards with names &lt;code class=&quot;highlighter-rouge&quot;&gt;pythonimagenet-train-000000.tar to imagenet-train-001281.tar,&lt;/code&gt; the contents of the first shard are:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n03991062_24866&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cls&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;108611&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n03991062_24866&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jpg&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n07749582_9506&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cls&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;129044&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n07749582_9506&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jpg&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n03425413_23604&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cls&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;106255&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n03425413_23604&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jpg&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n02795169_27274&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cls&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;WebDataset datasets can be used directly from local disk, from web servers (hence the name), from cloud storage and object stores, just by changing a URL. WebDataset datasets can be used for training without unpacking, and training can even be carried out on streaming data, with no local storage.&lt;/p&gt;

&lt;p&gt;Shuffling during training is important for many deep learning applications, and WebDataset performs shuffling both at the shard level and at the sample level. Splitting of data across multiple workers is performed at the shard level using a user-provided &lt;code class=&quot;highlighter-rouge&quot;&gt;shard_selection&lt;/code&gt; function that defaults to a function that splits based on &lt;code class=&quot;highlighter-rouge&quot;&gt;get_worker_info.&lt;/code&gt; (WebDataset can be combined with the &lt;a href=&quot;https://github.com/NVLabs/tensorcom&quot;&gt;tensorcom&lt;/a&gt; library to offload decompression/data augmentation and provide RDMA and direct-to-GPU loading; see below.)&lt;/p&gt;

&lt;h2 id=&quot;code-sample&quot;&gt;Code Sample&lt;/h2&gt;
&lt;p&gt;Here are some code snippets illustrating the use of WebDataset in a typical PyTorch deep learning application (you can find a full example at &lt;a href=&quot;http://github.com/tmbdev/pytorch-imagenet-wds&quot;&gt;http://github.com/tmbdev/pytorch-imagenet-wds&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;webdataset&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wds&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sharedurl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/imagenet/imagenet-train-{000000..001281}.tar&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.485&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.456&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.406&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.229&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.225&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;preproc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Compose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomResizedCrop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomHorizontalFlip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;wds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sharedurl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pil&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;jpg;png&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preproc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This code is nearly identical to the file-based I/O pipeline found in the PyTorch Imagenet example: it creates a preprocessing/augmentation pipeline, instantiates a dataset using that pipeline and a data source location, and then constructs a DataLoader instance from the dataset.&lt;/p&gt;

&lt;p&gt;WebDataset uses a fluent API for a configuration that internally builds up a processing pipeline. Without any added processing stages, In this example, WebDataset is used with the PyTorch DataLoader class, which replicates DataSet instances across multiple threads and performs both parallel I/O and parallel data augmentation.&lt;/p&gt;

&lt;p&gt;WebDataset instances themselves just iterate through each training sample as a dictionary:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# load from a web server using a separate client process&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sharedurl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;pipe:curl -s http://server/imagenet/imagenet-train-{000000..001281}.tar&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sharedurl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# sample[&quot;jpg&quot;] contains the raw image data&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# sample[&quot;cls&quot;] contains the class&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For a general introduction to how we handle large scale training with WebDataset, see these &lt;a href=&quot;https://www.youtube.com/playlist?list=PL0dsKxFNMcX4XcB0w1Wm-pvSfQu-eWM26&quot;&gt;YouTube videos&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;related-software&quot;&gt;Related Software&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/NVIDIA/AIStore&quot;&gt;AIStore&lt;/a&gt; is an open-source object store capable of full-bandwidth disk-to-GPU data delivery (meaning that if you have 1000 rotational drives with 200 MB/s read speed, AIStore actually delivers an aggregate bandwidth of 200 GB/s to the GPUs). AIStore is fully compatible with WebDataset as a client, and in addition understands the WebDataset format, permitting it to perform shuffling, sorting, ETL, and some map-reduce operations directly in the storage system. AIStore can be thought of as a remix of a distributed object store, a network file system, a distributed database, and a GPU-accelerated map-reduce implementation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/tmbdev/tarp&quot;&gt;tarp&lt;/a&gt; is a small command-line program for splitting, merging, shuffling, and processing tar archives and WebDataset datasets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/NVLabs/tensorcom&quot;&gt;tensorcom&lt;/a&gt; is a library supporting distributed data augmentation and RDMA to GPU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/tmbdev/webdataset-examples&quot;&gt;webdataset-examples&lt;/a&gt; contains an example (and soon more examples) of how to use WebDataset in practice.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.01858&quot;&gt;Bigdata 2019 Paper with Benchmarks&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check out &lt;a href=&quot;https://github.com/tmbdev/webdataset&quot;&gt;the library&lt;/a&gt; and provide your feedback for &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/38419&quot;&gt;RFC 38419&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alex Aizman, Gavin Maltby, Thomas Breuel</name></author><summary type="html">The size of data sets are growing bigger and faster every day. This means there are more data sets for deep learning researchers and engineers to train and validate their models.</summary></entry><entry><title type="html">PyTorch 1.6 released w/ Native AMP Support, Microsoft joins as maintainers for Windows</title><link href="https://pytorch.org/blog/pytorch-1.6-released/" rel="alternate" type="text/html" title="PyTorch 1.6 released w/ Native AMP Support, Microsoft joins as maintainers for Windows" /><published>2020-07-28T00:00:00-07:00</published><updated>2020-07-28T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.6-released</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.6-released/">&lt;p&gt;Today, we’re announcing the availability of PyTorch 1.6, along with updated domain libraries. We are also excited to announce the team at &lt;a href=&quot;https://pytorch.org/blog/microsoft-becomes-maintainer-of-the-windows-version-of-pytorch&quot;&gt;Microsoft is now maintaining Windows builds and binaries&lt;/a&gt; and will also be supporting the community on GitHub as well as the PyTorch Windows discussion forums.&lt;/p&gt;

&lt;p&gt;The PyTorch 1.6 release includes a number of new APIs, tools for performance improvement and profiling, as well as major updates to both distributed data parallel (DDP) and remote procedure call (RPC) based distributed training. 
A few of the highlights include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Automatic mixed precision (AMP) training is now natively supported and a stable feature (See &lt;a href=&quot;https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/&quot;&gt;here&lt;/a&gt; for more details) - thanks for NVIDIA’s contributions;&lt;/li&gt;
  &lt;li&gt;Native TensorPipe support now added for tensor-aware, point-to-point communication primitives built specifically for machine learning;&lt;/li&gt;
  &lt;li&gt;Added support for complex tensors to the frontend API surface;&lt;/li&gt;
  &lt;li&gt;New profiling tools providing tensor-level memory consumption information;&lt;/li&gt;
  &lt;li&gt;Numerous improvements and new features for both distributed data parallel (DDP) training and the remote procedural call (RPC) packages.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Additionally, from this release onward, features will be classified as Stable, Beta and Prototype. Prototype features are not included as part of the binary distribution and are instead available through either building from source, using nightlies or via compiler flag. You can learn more about what this change means in the post &lt;a href=&quot;https://pytorch.org/blog/pytorch-feature-classification-changes/&quot;&gt;here&lt;/a&gt;. You can also find the full release notes &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;performance--profiling&quot;&gt;Performance &amp;amp; Profiling&lt;/h1&gt;

&lt;h2 id=&quot;stable-automatic-mixed-precision-amp-training&quot;&gt;[Stable] Automatic Mixed Precision (AMP) Training&lt;/h2&gt;

&lt;p&gt;AMP allows users to easily enable automatic mixed precision training enabling higher performance and memory savings of up to 50% on Tensor Core GPUs. Using the natively supported &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.amp&lt;/code&gt; API, AMP provides convenience methods for mixed precision, where some operations use the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.float32 (float)&lt;/code&gt; datatype and other operations use &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.float16 (half)&lt;/code&gt;. Some ops, like linear layers and convolutions, are much faster in &lt;code class=&quot;highlighter-rouge&quot;&gt;float16&lt;/code&gt;. Other ops, like reductions, often require the dynamic range of &lt;code class=&quot;highlighter-rouge&quot;&gt;float32&lt;/code&gt;. Mixed precision tries to match each op to its appropriate datatype.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Design doc (&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/25081&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/amp.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Usage examples (&lt;a href=&quot;https://pytorch.org/docs/stable/notes/amp_examples.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-forkjoin-parallelism&quot;&gt;[Beta] Fork/Join Parallelism&lt;/h2&gt;

&lt;p&gt;This release adds support for a language-level construct as well as runtime support for coarse-grained parallelism in TorchScript code. This support is useful for situations such as running models in an ensemble in parallel, or running bidirectional components of recurrent nets in parallel, and allows the ability to unlock the computational power of parallel architectures (e.g. many-core CPUs) for task level parallelism.&lt;/p&gt;

&lt;p&gt;Parallel execution of TorchScript programs is enabled through two primitives: &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.jit.fork&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.jit.wait&lt;/code&gt;. In the below example, we parallelize execution of &lt;code class=&quot;highlighter-rouge&quot;&gt;foo&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;neg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@torch.jit.script&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;futures&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;future&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;futures&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/jit.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-memory-profiler&quot;&gt;[Beta] Memory Profiler&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.autograd.profiler&lt;/code&gt; API now includes a memory profiler that lets you inspect the tensor memory cost of different operators inside your CPU and GPU models.&lt;/p&gt;

&lt;p&gt;Here is an example usage of the API:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.autograd.profiler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resnet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;profile_memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record_shapes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# NOTE: some columns were removed for brevity&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prof&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;self_cpu_memory_usage&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_limit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ---------------------------  ---------------  ---------------  ---------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Name                         CPU Mem          Self CPU Mem     Number of Calls&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ---------------------------  ---------------  ---------------  ---------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# empty                        94.79 Mb         94.79 Mb         123&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# resize_                      11.48 Mb         11.48 Mb         2&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# addmm                        19.53 Kb         19.53 Kb         1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# empty_strided                4 b              4 b              1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# conv2d                       47.37 Mb         0 b              20&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ---------------------------  ---------------  ---------------  ---------------&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;PR (&lt;a href=&quot;https://github.com/pytorch/pytorch/pull/37775&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#profiler&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;distributed-training--rpc&quot;&gt;Distributed Training &amp;amp; RPC&lt;/h1&gt;

&lt;h2 id=&quot;beta-tensorpipe-backend-for-rpc&quot;&gt;[Beta] TensorPipe backend for RPC&lt;/h2&gt;

&lt;p&gt;PyTorch 1.6 introduces a new backend for the RPC module which leverages the TensorPipe library, a tensor-aware point-to-point communication primitive targeted at machine learning, intended to complement the current primitives for distributed training in PyTorch (Gloo, MPI, …) which are collective and blocking. The pairwise and asynchronous nature of TensorPipe lends itself to new networking paradigms that go beyond data parallel: client-server approaches (e.g., parameter server for embeddings, actor-learner separation in Impala-style RL, …) and model and pipeline parallel training (think GPipe), gossip SGD, etc.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# One-line change needed to opt in&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_rpc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BackendType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TENSORPIPE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# No changes to the rest of the RPC API&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc_sync&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Design doc (&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/35251&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/rpc/index.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-ddprpc&quot;&gt;[Beta] DDP+RPC&lt;/h2&gt;

&lt;p&gt;PyTorch Distributed supports two powerful paradigms: DDP for full sync data parallel training of models and the RPC framework which allows for distributed model parallelism. Previously, these two features worked independently and users couldn’t mix and match these to try out hybrid parallelism paradigms.&lt;/p&gt;

&lt;p&gt;Starting in PyTorch 1.6, we’ve enabled DDP and RPC to work together seamlessly so that users can combine these two techniques to achieve both data parallelism and model parallelism. An example is where users would like to place large embedding tables on parameter servers and use the RPC framework for embedding lookups, but store smaller dense parameters on trainers and use DDP to synchronize the dense parameters. Below is a simple code snippet.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;On&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;remote_emb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_emb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ps&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ddp_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DDP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dense_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remote_emb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ddp_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;DDP+RPC Tutorial (&lt;a href=&quot;https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/rpc/index.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Usage Examples (&lt;a href=&quot;https://github.com/pytorch/examples/pull/800&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-rpc---asynchronous-user-functions&quot;&gt;[Beta] RPC - Asynchronous User Functions&lt;/h2&gt;

&lt;p&gt;RPC Asynchronous User Functions supports the ability to yield and resume on the server side when executing a user-defined function. Prior to this feature, when a callee processes a request, one RPC thread waits until the user function returns. If the user function contains IO (e.g., nested RPC) or signaling (e.g., waiting for another request to unblock), the corresponding RPC thread would sit idle waiting for these events. As a result, some applications have to use a very large number of threads and send additional RPC requests, which can potentially lead to performance degradation. To make a user function yield on such events, applications need to: 1) Decorate the function with the &lt;code class=&quot;highlighter-rouge&quot;&gt;@rpc.functions.async_execution&lt;/code&gt; decorator; and 2) Let the function return a &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.futures.Future&lt;/code&gt; and install the resume logic as callbacks on the &lt;code class=&quot;highlighter-rouge&quot;&gt;Future&lt;/code&gt; object. See below for an example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nd&quot;&gt;@rpc.functions.async_execution&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;async_add_chained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc_async&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fut&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc_sync&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;worker1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;async_add_chained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;worker2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# prints tensor([3., 3.])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Tutorial for performant batch RPC using Asynchronous User Functions (&lt;a href=&quot;https://github.com/pytorch/tutorials/blob/release/1.6/intermediate_source/rpc_async_execution.rst&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.functions.async_execution&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Usage examples (&lt;a href=&quot;https://github.com/pytorch/examples/tree/master/distributed/rpc/batch&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;frontend-api-updates&quot;&gt;Frontend API Updates&lt;/h1&gt;

&lt;h2 id=&quot;beta-complex-numbers&quot;&gt;[Beta] Complex Numbers&lt;/h2&gt;

&lt;p&gt;The PyTorch 1.6 release brings beta level support for complex tensors including torch.complex64 and torch.complex128 dtypes. A complex number is a number that can be expressed in the form a + bj, where a and b are real numbers, and j is a solution of the equation x^2 = −1. Complex numbers frequently occur in mathematics and engineering, especially in signal processing and the area of complex neural networks is an active area of research. The beta release of complex tensors will support common PyTorch and complex tensor functionality, plus functions needed by Torchaudio, ESPnet and others. While this is an early version of this feature, and we expect it to improve over time, the overall goal is provide a NumPy compatible user experience that leverages PyTorch’s ability to run on accelerators and work with autograd to better support the scientific community.&lt;/p&gt;

&lt;h1 id=&quot;updated-domain-libraries&quot;&gt;Updated Domain Libraries&lt;/h1&gt;

&lt;h2 id=&quot;torchvision-07&quot;&gt;torchvision 0.7&lt;/h2&gt;

&lt;p&gt;torchvision 0.7 introduces two new pretrained semantic segmentation models, &lt;a href=&quot;https://arxiv.org/abs/1411.4038&quot;&gt;FCN ResNet50&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1706.05587&quot;&gt;DeepLabV3 ResNet50&lt;/a&gt;, both trained on COCO and using smaller memory footprints than the ResNet101 backbone. We also introduced support for AMP (Automatic Mixed Precision) autocasting for torchvision models and operators, which automatically selects the floating point precision for different GPU operations to improve performance while maintaining accuracy.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Release notes (&lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;torchaudio-06&quot;&gt;torchaudio 0.6&lt;/h2&gt;

&lt;p&gt;torchaudio now officially supports Windows. This release also introduces a new model module (with wav2letter included), new functionals (contrast, cvm, dcshift, overdrive, vad, phaser, flanger, biquad), datasets (GTZAN, CMU), and a new optional sox backend with support for TorchScript.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Release notes (&lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;additional-updates&quot;&gt;Additional updates&lt;/h1&gt;

&lt;h2 id=&quot;hackathon&quot;&gt;HACKATHON&lt;/h2&gt;

&lt;p&gt;The Global PyTorch Summer Hackathon is back! This year, teams can compete in three categories virtually:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;PyTorch Developer Tools:&lt;/strong&gt; Tools or libraries designed to improve productivity and efficiency of PyTorch for researchers and developers&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Web/Mobile Applications powered by PyTorch:&lt;/strong&gt; Applications with web/mobile interfaces and/or embedded devices powered by PyTorch&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PyTorch Responsible AI Development Tools:&lt;/strong&gt; Tools, libraries, or web/mobile apps for responsible AI development&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is a great opportunity to connect with the community and practice your machine learning skills.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch2020.devpost.com/&quot;&gt;Join the hackathon&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/pytorch&quot;&gt;Watch educational videos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lpcv-challenge&quot;&gt;LPCV Challenge&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://lpcv.ai/2020CVPR/video-track&quot;&gt;2020 CVPR Low-Power Vision Challenge (LPCV) - Online Track for UAV video&lt;/a&gt; submission deadline is coming up shortly. You have until July 31, 2020 to build a system that can discover and recognize characters in video captured by an unmanned aerial vehicle (UAV) accurately using PyTorch and Raspberry Pi 3B+.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;Prototype Features&lt;/h2&gt;

&lt;p&gt;To reiterate, Prototype features in PyTorch are early features that we are looking to gather feedback on, gauge the usefulness of and improve ahead of graduating them to Beta or Stable. The following features are not part of the PyTorch 1.6 release and instead are available in nightlies with separate docs/tutorials to help facilitate early usage and feedback.&lt;/p&gt;

&lt;h4 id=&quot;distributed-rpcprofiler&quot;&gt;Distributed RPC/Profiler&lt;/h4&gt;
&lt;p&gt;Allow users to profile training jobs that use &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.distributed.rpc&lt;/code&gt; using the autograd profiler, and remotely invoke the profiler in order to collect profiling information across different nodes. The RFC can be found &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/39675&quot;&gt;here&lt;/a&gt; and a short recipe on how to use this feature can be found &lt;a href=&quot;https://github.com/pytorch/tutorials/tree/master/prototype_source&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;torchscript-module-freezing&quot;&gt;TorchScript Module Freezing&lt;/h4&gt;
&lt;p&gt;Module Freezing is the process of inlining module parameters and attributes values into the TorchScript internal representation. Parameter and attribute values are treated as final value and they cannot be modified in the frozen module. The PR for this feature can be found &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/32178&quot;&gt;here&lt;/a&gt; and a short tutorial on how to use this feature can be found &lt;a href=&quot;https://github.com/pytorch/tutorials/tree/master/prototype_source&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;graph-mode-quantization&quot;&gt;Graph Mode Quantization&lt;/h4&gt;
&lt;p&gt;Eager mode quantization requires users to make changes to their model, including explicitly quantizing activations, module fusion, rewriting use of torch ops with Functional Modules and quantization of functionals are not supported. If we can trace or script the model, then the quantization can be done automatically with graph mode quantization without any of the complexities in eager mode, and it is configurable through a &lt;code class=&quot;highlighter-rouge&quot;&gt;qconfig_dict&lt;/code&gt;. A tutorial on how to use this feature can be found &lt;a href=&quot;https://github.com/pytorch/tutorials/tree/master/prototype_source&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;quantization-numerical-suite&quot;&gt;Quantization Numerical Suite&lt;/h4&gt;
&lt;p&gt;Quantization is good when it works, but it’s difficult to know what’s wrong when it doesn’t satisfy the expected accuracy. A prototype is now available for a Numerical Suite that measures comparison statistics between quantized modules and float modules. This is available to test using eager mode and on CPU only with more support coming. A tutorial on how to use this feature can be found &lt;a href=&quot;https://github.com/pytorch/tutorials/tree/master/prototype_source&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we’re announcing the availability of PyTorch 1.6, along with updated domain libraries. We are also excited to announce the team at Microsoft is now maintaining Windows builds and binaries and will also be supporting the community on GitHub as well as the PyTorch Windows discussion forums.</summary></entry><entry><title type="html">PyTorch feature classification changes</title><link href="https://pytorch.org/blog/pytorch-feature-classification-changes/" rel="alternate" type="text/html" title="PyTorch feature classification changes" /><published>2020-07-28T00:00:00-07:00</published><updated>2020-07-28T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-feature-classification-changes</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-feature-classification-changes/">&lt;p&gt;Traditionally features in PyTorch were classified as either stable or experimental with an implicit third option of testing bleeding edge features by building master or through installing nightly builds (available via prebuilt whls). This has, in a few cases, caused some confusion around the level of readiness, commitment to the feature and backward compatibility that can be expected from a user perspective. Moving forward, we’d like to better classify the 3 types of features as well as define explicitly here what each mean from a user perspective.&lt;/p&gt;

&lt;h1 id=&quot;new-feature-designations&quot;&gt;New Feature Designations&lt;/h1&gt;

&lt;p&gt;We will continue to have three designations for features but, as mentioned, with a few changes: Stable, Beta (previously Experimental) and Prototype (previously Nightlies). Below is a brief description of each and a comment on the backward compatibility expected:&lt;/p&gt;

&lt;h2 id=&quot;stable&quot;&gt;Stable&lt;/h2&gt;
&lt;p&gt;Nothing changes here. A stable feature means that the user value-add is or has been proven, the API isn’t expected to change, the feature is performant and all documentation exists to support end user adoption.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Level of commitment&lt;/em&gt;: We expect to maintain these features long term and generally there should be no major performance limitations, gaps in documentation and we also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).&lt;/p&gt;

&lt;h2 id=&quot;beta&quot;&gt;Beta&lt;/h2&gt;
&lt;p&gt;We previously called these features ‘Experimental’ and we found that this created confusion amongst some of the users. In the case of a Beta level features, the value add, similar to a Stable feature, has been proven (e.g. pruning is a commonly used technique for reducing the number of parameters in NN models, independent of the implementation details of our particular choices) and the feature generally works and is documented. This feature is tagged as Beta because the API may change based on user feedback, because the performance needs to improve or because coverage across operators is not yet complete.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Level of commitment&lt;/em&gt;: We are committing to seeing the feature through to the Stable classification. We are however not committing to Backwards Compatibility. Users can depend on us providing a solution for problems in this area going forward, but the APIs and performance characteristics of this feature may change.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/install-matrix.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;prototype&quot;&gt;Prototype&lt;/h2&gt;
&lt;p&gt;Previously these were features that were known about by developers who paid close attention to RFCs and to features that land in master. In this case the feature is not available as part of binary distributions like PyPI or Conda (except maybe behind run-time flags), but we would like to get high bandwidth partner feedback ahead of a real release in order to gauge utility and any changes we need to make to the UX. To test these kinds of features we would, depending on the feature, recommend building from master or using the nightly whls that are made available on pytorch.org. For each prototype feature, a pointer to draft docs or other instructions will be provided.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Level of commitment&lt;/em&gt;: We are committing to gathering high bandwidth feedback only. Based on this feedback and potential further engagement between community members, we as a community will decide if we want to upgrade the level of commitment or to fail fast. Additionally, while some of these features might be more speculative (e.g. new Frontend APIs), others have obvious utility (e.g. model optimization) but may be in a state where gathering feedback outside of high bandwidth channels is not practical, e.g. the feature may be in an earlier state, may be moving fast (PRs are landing too quickly to catch a major release) and/or generally active development is underway.&lt;/p&gt;

&lt;h1 id=&quot;what-changes-for-current-features&quot;&gt;What changes for current features?&lt;/h1&gt;

&lt;p&gt;First and foremost, you can find these designations on &lt;a href=&quot;http://pytorch.org/docs&quot;&gt;pytorch.org/docs&lt;/a&gt;. We will also be linking any early stage features here for clarity.&lt;/p&gt;

&lt;p&gt;Additionally, the following features will be reclassified under this new rubric:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api&quot;&gt;High Level Autograd APIs&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/quantization.html&quot;&gt;Eager Mode Quantization&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/named_tensor.html&quot;&gt;Named Tensors&lt;/a&gt;: Prototype (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html#rpc&quot;&gt;TorchScript/RPC&lt;/a&gt;: Prototype (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/tensor_attributes.html#torch-memory-format&quot;&gt;Channels Last Memory Layout&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/jit.html?highlight=experimental&quot;&gt;Custom C++ Classes&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/mobile/home/&quot;&gt;PyTorch Mobile&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/packages.html#&quot;&gt;Java Bindings&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html?highlight=experimental#&quot;&gt;Torch.Sparse&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;Joe, Greg, Woo &amp;amp; Jessica&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Traditionally features in PyTorch were classified as either stable or experimental with an implicit third option of testing bleeding edge features by building master or through installing nightly builds (available via prebuilt whls). This has, in a few cases, caused some confusion around the level of readiness, commitment to the feature and backward compatibility that can be expected from a user perspective. Moving forward, we’d like to better classify the 3 types of features as well as define explicitly here what each mean from a user perspective.</summary></entry><entry><title type="html">Microsoft becomes maintainer of the Windows version of PyTorch</title><link href="https://pytorch.org/blog/microsoft-becomes-maintainer-of-the-windows-version-of-pytorch/" rel="alternate" type="text/html" title="Microsoft becomes maintainer of the Windows version of PyTorch" /><published>2020-07-28T00:00:00-07:00</published><updated>2020-07-28T00:00:00-07:00</updated><id>https://pytorch.org/blog/microsoft-becomes-maintainer-of-the-windows-version-of-pytorch</id><content type="html" xml:base="https://pytorch.org/blog/microsoft-becomes-maintainer-of-the-windows-version-of-pytorch/">&lt;p&gt;Along with the PyTorch 1.6 release, we are excited to announce that Microsoft has expanded its participation in the PyTorch community and is taking ownership of the development and maintenance of the PyTorch build for Windows.&lt;/p&gt;

&lt;p&gt;According to the latest &lt;a href=&quot;https://insights.stackoverflow.com/survey/2020#technology-developers-primary-operating-systems&quot;&gt;Stack Overflow developer survey&lt;/a&gt;, Windows remains the primary operating system for the developer community (46% Windows vs 28% MacOS). &lt;a href=&quot;https://github.com/peterjc123&quot;&gt;Jiachen Pu&lt;/a&gt; initially made a heroic effort to add support for PyTorch on Windows, but due to limited resources, Windows support for PyTorch has lagged behind other platforms. Lack of test coverage resulted in unexpected issues popping up every now and then. Some of the core tutorials, meant for new users to learn and adopt PyTorch, would fail to run. The installation experience was also not as smooth, with the lack of official PyPI support for PyTorch on Windows. Lastly, some of the PyTorch functionality was simply not available on the Windows platform, such as the TorchAudio domain library and distributed training support. To help alleviate this pain, Microsoft is happy to bring its Windows expertise to the table and bring PyTorch on Windows to its best possible self.&lt;/p&gt;

&lt;p&gt;In the PyTorch 1.6 release, we have improved the core quality of the Windows build by bringing test coverage up to par with Linux for core PyTorch and its domain libraries and by automating tutorial testing. Thanks to the broader PyTorch community, which contributed TorchAudio support to Windows, we were able to add test coverage to all three domain libraries: TorchVision, TorchText and TorchAudio. In subsequent releases of PyTorch, we will continue improving the Windows experience based on community feedback and requests. So far, the feedback we received from the community points to distributed training support and a better installation experience using pip as the next areas of improvement.&lt;/p&gt;

&lt;p&gt;In addition to the native Windows experience, Microsoft released a preview adding &lt;a href=&quot;https://blogs.windows.com/windowsdeveloper/2020/06/17/gpu-accelerated-ml-training-inside-the-windows-subsystem-for-linux/&quot;&gt;GPU compute support to Windows Subsystem for Linux (WSL) 2&lt;/a&gt; distros, with a focus on enabling AI and ML developer workflows. WSL is designed for developers that want to run any Linux based tools directly on Windows. This preview enables valuable scenarios for a variety of frameworks and Python packages that utilize &lt;a href=&quot;https://developer.nvidia.com/cuda/wsl&quot;&gt;NVIDIA CUDA&lt;/a&gt; for acceleration and only support Linux. This means WSL customers using the preview can run native Linux based PyTorch applications on Windows unmodified without the need for a traditional virtual machine or a dual boot setup.&lt;/p&gt;

&lt;h2 id=&quot;getting-started-with-pytorch-on-windows&quot;&gt;Getting started with PyTorch on Windows&lt;/h2&gt;
&lt;p&gt;It’s easy to get started with PyTorch on Windows. To install PyTorch using Anaconda with the latest GPU support, run the command below. To install different supported configurations of PyTorch, refer to the installation instructions on &lt;a href=&quot;https://pytorch.org&quot;&gt;pytorch.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;conda install pytorch torchvision cudatoolkit=10.2 -c pytorch&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Once you install PyTorch, learn more by visiting the &lt;a href=&quot;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&quot;&gt;PyTorch Tutorials&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/pytorch1.6.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;getting-started-with-pytorch-on-windows-subsystem-for-linux&quot;&gt;Getting started with PyTorch on Windows Subsystem for Linux&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/win32/direct3d12/gpu-cuda-in-wsl&quot;&gt;preview of NVIDIA CUDA support in WSL&lt;/a&gt; is now available to Windows Insiders running Build 20150 or higher. In WSL, the command to install PyTorch using Anaconda is the same as the above command for native Windows. If you prefer pip, use the command below.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pip install torch torchvision&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You can use the same tutorials and documentation inside your WSL environment as on native Windows. This functionality is still in preview so if you run into issues with WSL please share feedback via the &lt;a href=&quot;https://github.com/microsoft/WSL&quot;&gt;WSL GitHub repo&lt;/a&gt; or with NVIDIA CUDA support share via NVIDIA’s &lt;a href=&quot;https://forums.developer.nvidia.com/c/accelerated-computing/cuda/cuda-on-windows-subsystem-for-linux/303&quot;&gt;Community Forum for CUDA on WSL&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;feedback&quot;&gt;Feedback&lt;/h2&gt;
&lt;p&gt;If you find gaps in the PyTorch experience on Windows, please let us know on the &lt;a href=&quot;https://discuss.pytorch.org/c/windows/26&quot;&gt;PyTorch discussion forum&lt;/a&gt; or file an issue on &lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;GitHub&lt;/a&gt; using the #module: windows label.&lt;/p&gt;</content><author><name>Maxim Lukiyanov - Principal PM at Microsoft, Emad Barsoum - Group EM at Microsoft, Guoliang Hua - Principal EM at Microsoft, Nikita Shulga - Tech Lead at Facebook, Geeta Chauhan - PE Lead at Facebook, Chris Gottbrath - Technical PM at Facebook, Jiachen Pu - Engineer at Facebook</name></author><summary type="html">Along with the PyTorch 1.6 release, we are excited to announce that Microsoft has expanded its participation in the PyTorch community and is taking ownership of the development and maintenance of the PyTorch build for Windows.</summary></entry><entry><title type="html">Introducing native PyTorch automatic mixed precision for faster training on NVIDIA GPUs</title><link href="https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/" rel="alternate" type="text/html" title="Introducing native PyTorch automatic mixed precision for faster training on NVIDIA GPUs" /><published>2020-07-28T00:00:00-07:00</published><updated>2020-07-28T00:00:00-07:00</updated><id>https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision</id><content type="html" xml:base="https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/">&lt;p&gt;Most deep learning frameworks, including PyTorch, train with 32-bit floating point (FP32) arithmetic by default. However this is not essential to achieve full accuracy for many deep learning models. In 2017, NVIDIA researchers developed a methodology for &lt;a href=&quot;https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/&quot;&gt;mixed-precision training&lt;/a&gt;, which combined &lt;a href=&quot;https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/&quot;&gt;single-precision&lt;/a&gt; (FP32) with half-precision (e.g. FP16) format when training a network, and achieved the same accuracy as FP32 training using the same hyperparameters, with additional performance benefits on NVIDIA GPUs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shorter training time;&lt;/li&gt;
  &lt;li&gt;Lower memory requirements, enabling larger batch sizes, larger models, or larger inputs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to streamline the user experience of training in mixed precision for researchers and practitioners, NVIDIA developed &lt;a href=&quot;https://developer.nvidia.com/blog/apex-pytorch-easy-mixed-precision-training/&quot;&gt;Apex&lt;/a&gt; in 2018, which is a lightweight PyTorch extension with &lt;a href=&quot;https://developer.nvidia.com/automatic-mixed-precision&quot;&gt;Automatic Mixed Precision&lt;/a&gt; (AMP) feature. This feature enables automatic conversion of certain GPU operations from FP32 precision to mixed precision, thus improving performance while maintaining accuracy.&lt;/p&gt;

&lt;p&gt;For the PyTorch 1.6 release, developers at NVIDIA and Facebook moved mixed precision functionality into PyTorch core as the AMP package, &lt;a href=&quot;https://pytorch.org/docs/stable/amp.html&quot;&gt;torch.cuda.amp&lt;/a&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.amp&lt;/code&gt; is more flexible and intuitive compared to &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp&lt;/code&gt;. Some of &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp&lt;/code&gt;’s known pain points that &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.amp&lt;/code&gt; has been able to fix:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Guaranteed PyTorch version compatibility, because it’s part of PyTorch&lt;/li&gt;
  &lt;li&gt;No need to build extensions&lt;/li&gt;
  &lt;li&gt;Windows support&lt;/li&gt;
  &lt;li&gt;Bitwise accurate &lt;a href=&quot;https://pytorch.org/docs/master/amp.html#torch.cuda.amp.GradScaler.load_state_dict&quot;&gt;saving/restoring&lt;/a&gt; of checkpoints&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/master/notes/amp_examples.html#dataparallel-in-a-single-process&quot;&gt;DataParallel&lt;/a&gt; and intra-process model parallelism (although we still recommend &lt;a href=&quot;https://pytorch.org/docs/master/notes/amp_examples.html#distributeddataparallel-one-gpu-per-process&quot;&gt;torch.nn.DistributedDataParallel&lt;/a&gt; with one GPU per process as the most performant approach)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/master/notes/amp_examples.html#gradient-penalty&quot;&gt;Gradient penalty&lt;/a&gt; (double backward)&lt;/li&gt;
  &lt;li&gt;torch.cuda.amp.autocast() has no effect outside regions where it’s enabled, so it should serve cases that formerly struggled with multiple calls to &lt;a href=&quot;https://github.com/NVIDIA/apex/issues/439&quot;&gt;apex.amp.initialize()&lt;/a&gt; (including &lt;a href=&quot;https://github.com/NVIDIA/apex/issues/392#issuecomment-610038073&quot;&gt;cross-validation)&lt;/a&gt; without difficulty. Multiple convergence runs in the same script should each use a fresh &lt;a href=&quot;https://github.com/NVIDIA/apex/issues/439#issuecomment-610028282&quot;&gt;GradScaler instance&lt;/a&gt;, but GradScalers are lightweight and self-contained so that’s not a problem.&lt;/li&gt;
  &lt;li&gt;Sparse gradient support&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With AMP being added to PyTorch core, we have started the process of deprecating &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp.&lt;/code&gt; We have moved &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp&lt;/code&gt; to maintenance mode and will support customers using &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp.&lt;/code&gt; However, we highly encourage &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp&lt;/code&gt; customers to transition to using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.amp&lt;/code&gt; from PyTorch Core.&lt;/p&gt;

&lt;h1 id=&quot;example-walkthrough&quot;&gt;Example Walkthrough&lt;/h1&gt;
&lt;p&gt;Please see official docs for usage:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/amp.html&quot;&gt;https://pytorch.org/docs/stable/amp.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/amp_examples.html&quot;&gt;https://pytorch.org/docs/stable/notes/amp_examples.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt; 
&lt;span class=&quot;c&quot;&gt;# Creates once at the beginning of training &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
 
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
   &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
   &lt;span class=&quot;c&quot;&gt;# Casts operations to mixed precision &lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autocast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; 
      &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
 
   &lt;span class=&quot;c&quot;&gt;# Scales the loss, and calls backward() &lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# to create scaled gradients &lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
 
   &lt;span class=&quot;c&quot;&gt;# Unscales gradients and calls &lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# or skips optimizer.step() &lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
 
   &lt;span class=&quot;c&quot;&gt;# Updates the scale for next iteration &lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;performance-benchmarks&quot;&gt;Performance Benchmarks&lt;/h1&gt;
&lt;p&gt;In this section, we discuss the accuracy and performance of mixed precision training with AMP on the latest NVIDIA GPU A100 and also previous generation V100 GPU. The mixed precision performance is compared to FP32 performance, when running Deep Learning workloads in the &lt;a href=&quot;https://ngc.nvidia.com/catalog/containers/nvidia:pytorch?ncid=partn-52193#cid=ngc01_partn_en-us&quot;&gt;NVIDIA pytorch:20.06-py3 container&lt;/a&gt; from NGC.&lt;/p&gt;

&lt;h2 id=&quot;accuracy-amp-fp16-fp32&quot;&gt;Accuracy: AMP (FP16), FP32&lt;/h2&gt;
&lt;p&gt;The advantage of using AMP for Deep Learning training is that the models converge to the similar final accuracy while providing improved training performance. To illustrate this point, for &lt;a href=&quot;https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/resnet50v1.5#training-accuracy-nvidia-dgx-a100-8x-a100-40gb&quot;&gt;Resnet 50 v1.5 training&lt;/a&gt;, we see the following accuracy results where higher is better. Please note that the below accuracy numbers are sample numbers that are subject to run to run variance of up to 0.4%. Accuracy numbers for other models including BERT, Transformer, ResNeXt-101, Mask-RCNN, DLRM can be found at  &lt;a href=&quot;https://github.com/NVIDIA/DeepLearningExamples&quot;&gt;NVIDIA Deep Learning Examples Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Training accuracy: NVIDIA DGX A100 (8x A100 40GB)&lt;/p&gt;

&lt;table width=&quot;460&quot; border=&quot;0&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;&amp;nbsp;epochs&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;&amp;nbsp;Mixed Precision Top 1(%)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;strong&gt;TF32 Top1(%)&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&amp;nbsp;90&lt;/td&gt;
      &lt;td&gt;&amp;nbsp;76.93&lt;/td&gt;
      &lt;td&gt;&amp;nbsp;76.85&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Training accuracy: NVIDIA DGX-1 (8x V100 16GB)&lt;/p&gt;

&lt;table width=&quot;460&quot; border=&quot;0&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
     &lt;td&gt;&lt;strong&gt;&amp;nbsp;epochs&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;&amp;nbsp;Mixed Precision Top 1(%)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;strong&gt;FP32 Top1(%)&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;76.25&lt;/td&gt;
      &lt;td&gt;76.26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;77.09&lt;/td&gt;
      &lt;td&gt;77.01&lt;/td&gt;
    &lt;/tr&gt;
	  &lt;tr&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt;78.42&lt;/td&gt;
      &lt;td&gt;78.30&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;speedup-performance&quot;&gt;Speedup Performance:&lt;/h2&gt;

&lt;h3 id=&quot;fp16-on-nvidia-v100-vs-fp32-on-v100&quot;&gt;FP16 on NVIDIA V100 vs. FP32 on V100&lt;/h3&gt;
&lt;p&gt;AMP with FP16 is the most performant option for DL training on the V100. In Table 1, we can observe that for various models, AMP on V100 provides a speedup of 1.5x to 5.5x over FP32 on V100 while converging to the same final accuracy.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/nvidiafp32onv100.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Figure 2. Performance of mixed precision training on NVIDIA 8xV100 vs. FP32 training on 8xV100 GPU. Bars represent the speedup factor of V100 AMP over V100 FP32. The higher the better.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;fp16-on-nvidia-a100-vs-fp16-on-v100&quot;&gt;FP16 on NVIDIA A100 vs. FP16 on V100&lt;/h2&gt;

&lt;p&gt;AMP with FP16 remains the most performant option for DL training on the A100. In Figure 3, we can observe that for various models, AMP on A100 provides a speedup of 1.3x to 2.5x over AMP on V100 while converging to the same final accuracy.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/nvidiafp16onv100.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Figure 3. Performance of mixed precision training on NVIDIA 8xA100 vs. 8xV100 GPU. Bars represent the speedup factor of A100 over V100. The higher the better.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;call-to-action&quot;&gt;Call to action&lt;/h1&gt;
&lt;p&gt;AMP provides a healthy speedup for Deep Learning training workloads on Nvidia Tensor Core GPUs, especially on the latest Ampere generation A100 GPUs.  You can start experimenting with AMP enabled models and model scripts for A100, V100, T4 and other GPUs available at NVIDIA deep learning &lt;a href=&quot;https://github.com/NVIDIA/DeepLearningExamples&quot;&gt;examples&lt;/a&gt;. NVIDIA PyTorch with native AMP support is available from the &lt;a href=&quot;https://ngc.nvidia.com/catalog/containers/nvidia:pytorch?ncid=partn-52193#cid=ngc01_partn_en-us&quot;&gt;PyTorch NGC container&lt;/a&gt; version 20.06. We highly encourage existing &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp&lt;/code&gt; customers to transition to using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.amp&lt;/code&gt; from PyTorch Core available in the latest &lt;a href=&quot;https://pytorch.org/blog/pytorch-1.6-released/&quot;&gt;PyTorch 1.6 release&lt;/a&gt;.&lt;/p&gt;</content><author><name>Mengdi Huang, Chetan Tekur, Michael Carilli</name></author><summary type="html">Most deep learning frameworks, including PyTorch, train with 32-bit floating point (FP32) arithmetic by default. However this is not essential to achieve full accuracy for many deep learning models. In 2017, NVIDIA researchers developed a methodology for mixed-precision training, which combined single-precision (FP32) with half-precision (e.g. FP16) format when training a network, and achieved the same accuracy as FP32 training using the same hyperparameters, with additional performance benefits on NVIDIA GPUs:</summary></entry><entry><title type="html">Updates &amp;amp; Improvements to PyTorch Tutorials</title><link href="https://pytorch.org/blog/updates-improvements-to-pytorch-tutorials/" rel="alternate" type="text/html" title="Updates &amp; Improvements to PyTorch Tutorials" /><published>2020-05-05T00:00:00-07:00</published><updated>2020-05-05T00:00:00-07:00</updated><id>https://pytorch.org/blog/updates-improvements-to-pytorch-tutorials</id><content type="html" xml:base="https://pytorch.org/blog/updates-improvements-to-pytorch-tutorials/">&lt;p&gt;PyTorch.org provides researchers and developers with documentation, installation instructions, latest news, community projects, tutorials, and more. Today, we are introducing usability and content improvements including tutorials in additional categories, a new recipe format for quickly referencing common topics, sorting using tags, and an updated homepage.&lt;/p&gt;

&lt;p&gt;Let’s take a look at them in detail.&lt;/p&gt;

&lt;h2 id=&quot;tutorials-home-page-update&quot;&gt;TUTORIALS HOME PAGE UPDATE&lt;/h2&gt;
&lt;p&gt;The tutorials home page now provides clear actions that developers can take. For new PyTorch users, there is an easy-to-discover button to take them directly to “A 60 Minute Blitz”. Right next to it, there is a button to view all recipes which are designed to teach specific features quickly with examples.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/tutorialhomepage.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;In addition to the existing left navigation bar, tutorials can now be quickly filtered by multi-select tags. Let’s say you want to view all tutorials related to “Production” and “Quantization”. You can select the “Production” and “Quantization” filters as shown in the image shown below:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/blockfiltering.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The following additional resources can also be found at the bottom of the Tutorials homepage:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/ptcheat.html&quot;&gt;PyTorch Cheat Sheet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/examples&quot;&gt;PyTorch Examples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/tutorials&quot;&gt;Tutorial on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pytorch-recipes&quot;&gt;PYTORCH RECIPES&lt;/h2&gt;
&lt;p&gt;Recipes are new bite-sized, actionable examples designed to teach researchers and developers how to use specific PyTorch features. Some notable new recipes include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html&quot;&gt;Loading Data in PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/Captum_Recipe.html&quot;&gt;Model Interpretability Using Captum&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html&quot;&gt;How to Use TensorBoard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;View the full recipes &lt;a href=&quot;http://pytorch.org/tutorials/recipes/recipes_index.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;learning-pytorch&quot;&gt;LEARNING PYTORCH&lt;/h2&gt;
&lt;p&gt;This section includes tutorials designed for users new to PyTorch. Based on community feedback, we have made updates to the current &lt;a href=&quot;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&quot;&gt;Deep Learning with PyTorch: A 60 Minute Blitz&lt;/a&gt; tutorial, one of our most popular tutorials for beginners. Upon completion, one can understand what PyTorch and neural networks are, and be able to build and train a simple image classification network. Updates include adding explanations to clarify output meanings and linking back to where users can read more in the docs, cleaning up confusing syntax errors, and reconstructing and explaining new concepts for easier readability.&lt;/p&gt;

&lt;h2 id=&quot;deploying-models-in-production&quot;&gt;DEPLOYING MODELS IN PRODUCTION&lt;/h2&gt;
&lt;p&gt;This section includes tutorials for developers looking to take their PyTorch models to production. The tutorials include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html&quot;&gt;Deploying PyTorch in Python via a REST API with Flask&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&quot;&gt;Introduction to TorchScript&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;Loading a TorchScript Model in C++&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html&quot;&gt;Exploring a Model from PyTorch to ONNX and Running it using ONNX Runtime&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;frontend-apis&quot;&gt;FRONTEND APIS&lt;/h2&gt;
&lt;p&gt;PyTorch provides a number of frontend API features that can help developers to code, debug, and validate their models more efficiently. This section includes tutorials that teach what these features are and how to use them. Some tutorials to highlight:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html&quot;&gt;Introduction to Named Tensors in PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_frontend.html&quot;&gt;Using the PyTorch C++ Frontend&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html&quot;&gt;Extending TorchScript with Custom C++ Operators&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html&quot;&gt;Extending TorchScript with Custom C++ Classes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_autograd.html&quot;&gt;Autograd in C++ Frontend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-optimization&quot;&gt;MODEL OPTIMIZATION&lt;/h2&gt;
&lt;p&gt;Deep learning models often consume large amounts of memory, power, and compute due to their complexity. This section provides tutorials for model optimization:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/pruning_tutorial.html&quot;&gt;Pruning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;Dynamic Quantization on BERT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;Static Quantization with Eager Mode in PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parallel-and-distributed-training&quot;&gt;PARALLEL AND DISTRIBUTED TRAINING&lt;/h2&gt;
&lt;p&gt;PyTorch provides features that can accelerate performance in research and production such as native support for asynchronous execution of collective operations and peer-to-peer communication that is accessible from Python and C++. This section includes tutorials on parallel and distributed training:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html&quot;&gt;Single-Machine Model Parallel Best Practices&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/ddp_tutorial.html&quot;&gt;Getting started with Distributed Data Parallel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/rpc_tutorial.html&quot;&gt;Getting started with Distributed RPC Framework&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html&quot;&gt;Implementing a Parameter Server Using Distributed RPC Framework&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Making these improvements are just the first step of improving PyTorch.org for the community. Please submit your suggestions &lt;a href=&quot;https://github.com/pytorch/tutorials/pulls&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">PyTorch.org provides researchers and developers with documentation, installation instructions, latest news, community projects, tutorials, and more. Today, we are introducing usability and content improvements including tutorials in additional categories, a new recipe format for quickly referencing common topics, sorting using tags, and an updated homepage.</summary></entry><entry><title type="html">PyTorch library updates including new model serving library</title><link href="https://pytorch.org/blog/pytorch-library-updates-new-model-serving-library/" rel="alternate" type="text/html" title="PyTorch library updates including new model serving library " /><published>2020-04-21T00:00:00-07:00</published><updated>2020-04-21T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-library-updates-new-model-serving-library</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-library-updates-new-model-serving-library/">&lt;p&gt;Along with the PyTorch 1.5 release, we are announcing new libraries for high-performance PyTorch model serving and tight integration with TorchElastic and Kubernetes. Additionally, we are releasing updated packages for torch_xla (Google Cloud TPUs), torchaudio, torchvision, and torchtext. All of these new libraries and enhanced capabilities are available today and accompany all of the core features &lt;a href=&quot;https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis&quot;&gt;released in PyTorch 1.5&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchserve-experimental&quot;&gt;TorchServe (Experimental)&lt;/h2&gt;

&lt;p&gt;TorchServe is a flexible and easy to use library for serving PyTorch models in production performantly at scale. It is cloud and environment agnostic and supports features such as multi-model serving, logging, metrics, and the creation of RESTful endpoints for application integration. TorchServe was jointly developed by engineers from Facebook and AWS with feedback and engagement from the broader PyTorch community. The experimental release of TorchServe is available today. Some of the highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support for both Python-based and TorchScript-based models&lt;/li&gt;
  &lt;li&gt;Default handlers for common use cases (e.g., image segmentation, text classification) as well as the ability to write custom handlers for other use cases&lt;/li&gt;
  &lt;li&gt;Model versioning, the ability to run multiple versions of a model at the same time, and the ability to roll back to an earlier version&lt;/li&gt;
  &lt;li&gt;The ability to package a model, learning weights, and supporting files (e.g., class mappings, vocabularies) into a single, persistent artifact (a.k.a. the “model archive”)&lt;/li&gt;
  &lt;li&gt;Robust management capability, allowing full configuration of models, versions, and individual worker threads via command line, config file, or run-time API&lt;/li&gt;
  &lt;li&gt;Automatic batching of individual inferences across HTTP requests&lt;/li&gt;
  &lt;li&gt;Logging including common metrics, and the ability to incorporate custom metrics&lt;/li&gt;
  &lt;li&gt;Ready-made Dockerfile for easy deployment&lt;/li&gt;
  &lt;li&gt;HTTPS support for secure deployment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To learn more about the APIs and the design of this feature, see the links below:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;See &lt;here&gt; for a full multi-node deployment reference architecture.&lt;/here&gt;&lt;/li&gt;
  &lt;li&gt;The full documentation can be found &lt;a href=&quot;https://pytorch.org/serve&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;torchelastic-integration-with-kubernetes-experimental&quot;&gt;TorchElastic integration with Kubernetes (Experimental)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/elastic&quot;&gt;TorchElastic&lt;/a&gt; is a proven library for training large scale deep neural networks at scale within companies like Facebook, where having the ability to dynamically adapt to server availability and scale as new compute resources come online is critical. Kubernetes enables customers using machine learning frameworks like PyTorch to run training jobs distributed across fleets of powerful GPU instances like the Amazon EC2 P3. Distributed training jobs, however, are not fault-tolerant, and a job cannot continue if a node failure or reclamation interrupts training. Further, jobs cannot start without acquiring all required resources, or scale up and down without being restarted. This lack of resiliency and flexibility results in increased training time and costs from idle resources. TorchElastic addresses these limitations by enabling distributed training jobs to be executed in a fault-tolerant and elastic manner. Until today, Kubernetes users needed to manage Pods and Services required for TorchElastic training jobs manually.&lt;/p&gt;

&lt;p&gt;Through the joint collaboration of engineers at Facebook and AWS, TorchElastic, adding elasticity and fault tolerance, is now supported using vanilla Kubernetes and through the managed EKS service from AWS.&lt;/p&gt;

&lt;p&gt;To learn more see the &lt;a href=&quot;http://pytorch.org/elastic/0.2.0rc0/kubernetes.html&quot;&gt;TorchElastic repo&lt;/a&gt; for the controller implementation and docs on how to use it.&lt;/p&gt;

&lt;h2 id=&quot;torch_xla-15-now-available&quot;&gt;torch_xla 1.5 now available&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://pytorch.org/xla/&quot;&gt;torch_xla&lt;/a&gt; is a Python package that uses the &lt;a href=&quot;https://www.tensorflow.org/xla&quot;&gt;XLA linear algebra compiler&lt;/a&gt; to accelerate the &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch deep learning framework&lt;/a&gt; on &lt;a href=&quot;https://cloud.google.com/tpu/&quot;&gt;Cloud TPUs&lt;/a&gt; and &lt;a href=&quot;https://cloud.google.com/tpu/docs/tutorials/pytorch-pod&quot;&gt;Cloud TPU Pods&lt;/a&gt;. torch_xla aims to give PyTorch users the ability to do everything they can do on GPUs on Cloud TPUs as well while minimizing changes to the user experience. The project began with a conversation at NeurIPS 2017 and gathered momentum in 2018 when teams from Facebook and Google came together to create a proof of concept. We announced this collaboration at PTDC 2018 and made the PyTorch/XLA integration broadly available at PTDC 2019. The project already has 28 contributors, nearly 2k commits, and a repo that has been forked more than 100 times.&lt;/p&gt;

&lt;p&gt;This release of &lt;a href=&quot;http://pytorch.org/xla/&quot;&gt;torch_xla&lt;/a&gt; is aligned and tested with PyTorch 1.5 to reduce friction for developers and to provide a stable and mature PyTorch/XLA stack for training models using Cloud TPU hardware. You can &lt;a href=&quot;https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc&quot;&gt;try it for free&lt;/a&gt; in your browser on an 8-core Cloud TPU device with &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Google Colab&lt;/a&gt;, and you can use it at a much larger scaleon &lt;a href=&quot;https://cloud.google.com/gcp&quot;&gt;Google Cloud&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See the full torch_xla release notes &lt;a href=&quot;https://github.com/pytorch/xla/releases&quot;&gt;here&lt;/a&gt;. Full docs and tutorials can be found &lt;a href=&quot;https://pytorch.org/xla/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://cloud.google.com/tpu/docs/tutorials&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-domain-libraries&quot;&gt;PyTorch Domain Libraries&lt;/h2&gt;

&lt;p&gt;torchaudio, torchvision, and torchtext complement PyTorch with common datasets, models, and transforms in each domain area. We’re excited to share new releases for all three domain libraries alongside PyTorch 1.5 and the rest of the library updates. For this release, all three domain libraries are removing support for Python2 and will support Python3 only.&lt;/p&gt;

&lt;h3 id=&quot;torchaudio-05&quot;&gt;torchaudio 0.5&lt;/h3&gt;
&lt;p&gt;The torchaudio 0.5 release includes new transforms, functionals, and datasets. Highlights for the release include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Added the Griffin-Lim functional and transform, &lt;code class=&quot;highlighter-rouge&quot;&gt;InverseMelScale&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Vol&lt;/code&gt; transforms, and &lt;code class=&quot;highlighter-rouge&quot;&gt;DB_to_amplitude&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Added support for &lt;code class=&quot;highlighter-rouge&quot;&gt;allpass&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fade&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;bandpass&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;bandreject&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;band&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;treble&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;deemph&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;riaa&lt;/code&gt; filters and transformations.&lt;/li&gt;
  &lt;li&gt;New datasets added including &lt;code class=&quot;highlighter-rouge&quot;&gt;LJSpeech&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;SpeechCommands&lt;/code&gt; datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the release full notes &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt; and full docs can be found &lt;a href=&quot;https://pytorch.org/audio/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchvision-06&quot;&gt;torchvision 0.6&lt;/h3&gt;
&lt;p&gt;The torchvision 0.6 release includes updates to datasets, models and a significant number of bug fixes. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Faster R-CNN now supports negative samples which allows the feeding of images without annotations at training time.&lt;/li&gt;
  &lt;li&gt;Added &lt;code class=&quot;highlighter-rouge&quot;&gt;aligned&lt;/code&gt; flag to &lt;code class=&quot;highlighter-rouge&quot;&gt;RoIAlign&lt;/code&gt; to match Detectron2.&lt;/li&gt;
  &lt;li&gt;Refactored abstractions for C++ video decoder&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the release full notes &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt; and full docs can be found &lt;a href=&quot;https://pytorch.org/docs/stable/torchvision/index.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchtext-06&quot;&gt;torchtext 0.6&lt;/h3&gt;
&lt;p&gt;The torchtext 0.6 release includes a number of bug fixes and improvements to documentation. Based on user’s feedback, dataset abstractions are currently being redesigned also. Highlights for the release include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fixed an issue related to the SentencePiece dependency in conda package.&lt;/li&gt;
  &lt;li&gt;Added support for the experimental IMDB dataset to allow a custom vocab.&lt;/li&gt;
  &lt;li&gt;A number of documentation updates including adding a code of conduct and a deduplication of the docs on the torchtext site.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Your feedback and discussions on the experimental datasets API are welcomed. You can send them to &lt;a href=&quot;https://github.com/pytorch/text/issues/664&quot;&gt;issue #664&lt;/a&gt;. We would also like to highlight the pull request &lt;a href=&quot;https://github.com/pytorch/text/pull/701&quot;&gt;here&lt;/a&gt; where the latest dataset abstraction is applied to the text classification datasets. The feedback can be beneficial to finalizing this abstraction.&lt;/p&gt;

&lt;p&gt;See the release full notes &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt; and full docs can be found &lt;a href=&quot;https://pytorch.org/text/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team, the Amazon team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Along with the PyTorch 1.5 release, we are announcing new libraries for high-performance PyTorch model serving and tight integration with TorchElastic and Kubernetes. Additionally, we are releasing updated packages for torch_xla (Google Cloud TPUs), torchaudio, torchvision, and torchtext. All of these new libraries and enhanced capabilities are available today and accompany all of the core features released in PyTorch 1.5.</summary></entry><entry><title type="html">PyTorch 1.5 released, new and updated APIs including C++ frontend API parity with Python</title><link href="https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis/" rel="alternate" type="text/html" title="PyTorch 1.5 released, new and updated APIs including C++ frontend API parity with Python" /><published>2020-04-21T00:00:00-07:00</published><updated>2020-04-21T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis/">&lt;p&gt;Today, we’re announcing the availability of PyTorch 1.5, along with new and updated libraries. This release includes several major new API additions and improvements. PyTorch now includes a significant update to the C++ frontend, ‘channels last’ memory format for computer vision models, and a stable release of the distributed RPC framework used for model-parallel training. The release also has new APIs for autograd for hessians and jacobians, and an API that allows the creation of Custom C++ Classes that was inspired by pybind.&lt;/p&gt;

&lt;p&gt;You can find the detailed release notes &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;c-frontend-api-stable&quot;&gt;C++ Frontend API (Stable)&lt;/h2&gt;

&lt;p&gt;The C++ frontend API is now at parity with Python, and the features overall have been moved to ‘stable’ (previously tagged as experimental). Some of the major highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Now with ~100% coverage and docs for C++ torch::nn module/functional, users can easily translate their model from Python API to C++ API, making the model authoring experience much smoother.&lt;/li&gt;
  &lt;li&gt;Optimizers in C++ had deviated from the Python equivalent: C++ optimizers can’t take parameter groups as input while the Python ones can. Additionally, step function implementations were not exactly the same. With the 1.5 release, C++ optimizers will always behave the same as the Python equivalent.&lt;/li&gt;
  &lt;li&gt;The lack of tensor multi-dim indexing API in C++ is a well-known issue and had resulted in many posts in PyTorch Github issue tracker and forum. The previous workaround was to use a combination of &lt;code class=&quot;highlighter-rouge&quot;&gt;narrow&lt;/code&gt; / &lt;code class=&quot;highlighter-rouge&quot;&gt;select&lt;/code&gt; / &lt;code class=&quot;highlighter-rouge&quot;&gt;index_select&lt;/code&gt; / &lt;code class=&quot;highlighter-rouge&quot;&gt;masked_select&lt;/code&gt;, which was clunky and error-prone compared to the Python API’s elegant &lt;code class=&quot;highlighter-rouge&quot;&gt;tensor[:, 0, ..., mask]&lt;/code&gt; syntax. With the 1.5 release, users can use &lt;code class=&quot;highlighter-rouge&quot;&gt;tensor.index({Slice(), 0, &quot;...&quot;, mask})&lt;/code&gt; to achieve the same purpose.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;channels-last-memory-format-for-computer-vision-models-experimental&quot;&gt;‘Channels last’ memory format for Computer Vision models (Experimental)&lt;/h2&gt;

&lt;p&gt;‘Channels last’ memory layout unlocks ability to use performance efficient convolution algorithms and hardware (NVIDIA’s Tensor Cores, FBGEMM, QNNPACK). Additionally, it is designed to automatically propagate through the operators, which allows easy switching between memory layouts.&lt;/p&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators&quot;&gt;here&lt;/a&gt; on how to write memory format aware operators.&lt;/p&gt;

&lt;h2 id=&quot;custom-c-classes-experimental&quot;&gt;Custom C++ Classes (Experimental)&lt;/h2&gt;

&lt;p&gt;This release adds a new API, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch::class_&lt;/code&gt;, for binding custom C++ classes into TorchScript and Python simultaneously. This API is almost identical in syntax to &lt;a href=&quot;https://pybind11.readthedocs.io/en/stable/&quot;&gt;pybind11&lt;/a&gt;. It allows users to expose their C++ class and its methods to the TorchScript type system and runtime system such that they can instantiate and manipulate arbitrary C++ objects from TorchScript and Python. An example C++ binding:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CustomClassHolder&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testStack&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;myclasses&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;MyStackClass&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;push&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pop&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intrusive_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which exposes a class you can use in Python and TorchScript like so:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nd&quot;&gt;@torch.jit.script&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;do_stacks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myclasses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myclasses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hi&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;mom&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# &quot;mom&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;foobar&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [&quot;hi&quot;, &quot;foobar&quot;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can try it out in the tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;distributed-rpc-framework-apis-now-stable&quot;&gt;Distributed RPC framework APIs (Now Stable)&lt;/h2&gt;

&lt;p&gt;The Distributed &lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html&quot;&gt;RPC framework&lt;/a&gt; was launched as experimental in the 1.4 release and the proposal is to mark Distributed RPC framework as stable and no longer experimental. This work involves a lot of enhancements and bug fixes to make the distributed RPC framework more reliable and robust overall, as well as adding a couple of new features, including profiling support, using TorchScript functions in RPC, and several enhancements for ease of use. Below is an overview of the various APIs within the framework:&lt;/p&gt;

&lt;h3 id=&quot;rpc-api&quot;&gt;RPC API&lt;/h3&gt;
&lt;p&gt;The RPC API allows users to specify functions to run and objects to be instantiated on remote nodes. These functions are transparently recorded so that gradients can backpropagate through remote nodes using Distributed Autograd.&lt;/p&gt;

&lt;h3 id=&quot;distributed-autograd&quot;&gt;Distributed Autograd&lt;/h3&gt;
&lt;p&gt;Distributed Autograd connects the autograd graph across several nodes and allows gradients to flow through during the backwards pass. Gradients are accumulated into a context (as opposed to the .grad field as with Autograd) and users must specify their model’s forward pass under a with &lt;code class=&quot;highlighter-rouge&quot;&gt;dist_autograd.context()&lt;/code&gt; manager in order to ensure that all RPC communication is recorded properly. Currently, only FAST mode is implemented (see &lt;a href=&quot;https://pytorch.org/docs/stable/rpc/distributed_autograd.html#distributed-autograd-design&quot;&gt;here&lt;/a&gt; for the difference between FAST and SMART modes).&lt;/p&gt;

&lt;h3 id=&quot;distributed-optimizer&quot;&gt;Distributed Optimizer&lt;/h3&gt;
&lt;p&gt;The distributed optimizer creates RRefs to optimizers on each worker with parameters that require gradients, and then uses the RPC API to run the optimizer remotely. The user must collect all remote parameters and wrap them in an &lt;code class=&quot;highlighter-rouge&quot;&gt;RRef&lt;/code&gt;, as this is required input to the distributed optimizer. The user must also specify the distributed autograd &lt;code class=&quot;highlighter-rouge&quot;&gt;context_id&lt;/code&gt; so that the optimizer knows in which context to look for gradients.&lt;/p&gt;

&lt;p&gt;Learn more about distributed RPC framework APIs &lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;new-high-level-autograd-api-experimental&quot;&gt;New High level autograd API (Experimental)&lt;/h2&gt;

&lt;p&gt;PyTorch 1.5 brings new functions including jacobian, hessian, jvp, vjp, hvp and vhp to the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.autograd.functional&lt;/code&gt; submodule. This feature builds on the current API and allows the user to easily perform these functions.&lt;/p&gt;

&lt;p&gt;Detailed design discussion on GitHub can be found &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/30632&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;python-2-no-longer-supported&quot;&gt;Python 2 no longer supported&lt;/h2&gt;

&lt;p&gt;Starting PyTorch 1.5.0, we will no longer support Python 2, specifically version 2.7. Going forward support for Python will be limited to Python 3, specifically Python 3.5, 3.6, 3.7 and 3.8 (first enabled in PyTorch 1.4.0).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we’re announcing the availability of PyTorch 1.5, along with new and updated libraries. This release includes several major new API additions and improvements. PyTorch now includes a significant update to the C++ frontend, ‘channels last’ memory format for computer vision models, and a stable release of the distributed RPC framework used for model-parallel training. The release also has new APIs for autograd for hessians and jacobians, and an API that allows the creation of Custom C++ Classes that was inspired by pybind.</summary></entry><entry><title type="html">Introduction to Quantization on PyTorch</title><link href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/" rel="alternate" type="text/html" title="Introduction to Quantization on PyTorch" /><published>2020-03-26T00:00:00-07:00</published><updated>2020-03-26T00:00:00-07:00</updated><id>https://pytorch.org/blog/introduction-to-quantization-on-pytorch</id><content type="html" xml:base="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/">&lt;p&gt;It’s important to make efficient use of both server-side and on-device compute resources when developing machine learning applications. To support more efficient deployment on servers and edge devices, PyTorch added a support for model quantization using the familiar eager mode Python API.&lt;/p&gt;

&lt;p&gt;Quantization leverages 8bit integer (int8) instructions to reduce the model size and run the inference faster (reduced latency) and can be the difference between a model achieving quality of service goals or even fitting into the resources available on a mobile device. Even when resources aren’t quite so constrained it may enable you to deploy a larger and more accurate model. Quantization is available in PyTorch starting in version 1.3 and with the release of PyTorch 1.4 we published quantized models for ResNet, ResNext, MobileNetV2, GoogleNet, InceptionV3 and ShuffleNetV2 in the PyTorch torchvision 0.5 library.&lt;/p&gt;

&lt;p&gt;This blog post provides an overview of the quantization support on PyTorch and its incorporation with the TorchVision domain library.&lt;/p&gt;

&lt;h2 id=&quot;what-is-quantization&quot;&gt;&lt;strong&gt;What is Quantization?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Quantization refers to techniques for doing both computations and memory accesses with lower precision data, usually int8 compared to floating point implementations. This enables performance gains in several important areas:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;4x reduction in model size;&lt;/li&gt;
  &lt;li&gt;2-4x reduction in memory bandwidth;&lt;/li&gt;
  &lt;li&gt;2-4x faster inference due to savings in memory bandwidth and faster compute with int8 arithmetic (the exact speed up varies depending on the hardware, the runtime, and the model).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quantization does not however come without additional cost. Fundamentally quantization means introducing approximations and the resulting networks have slightly less accuracy. These techniques attempt to minimize the gap between the full floating point accuracy and the quantized accuracy.&lt;/p&gt;

&lt;p&gt;We designed quantization to fit into the PyTorch framework. The means that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;PyTorch has data types corresponding to &lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor&quot;&gt;quantized tensors&lt;/a&gt;, which share many of the features of tensors.&lt;/li&gt;
  &lt;li&gt;One can write kernels with quantized tensors, much like kernels for floating point tensors to customize their implementation. PyTorch supports quantized modules for common operations as part of the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.quantized&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.quantized.dynamic&lt;/code&gt; name-space.&lt;/li&gt;
  &lt;li&gt;Quantization is compatible with the rest of PyTorch: quantized models are traceable and scriptable. The quantization method is virtually identical for both server and mobile backends. One can easily mix quantized and floating point operations in a model.&lt;/li&gt;
  &lt;li&gt;Mapping of floating point tensors to quantized tensors is customizable with user defined observer/fake-quantization blocks. PyTorch provides default implementations that should work for most use cases.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/torch_stack1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We developed three techniques for quantizing neural networks in PyTorch as part of quantization tooling in the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization&lt;/code&gt; name-space.&lt;/p&gt;

&lt;h2 id=&quot;the-three-modes-of-quantization-supported-in-pytorch-starting-version-13&quot;&gt;&lt;strong&gt;The Three Modes of Quantization Supported in PyTorch starting version 1.3&lt;/strong&gt;&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;dynamic-quantization&quot;&gt;&lt;strong&gt;Dynamic Quantization&lt;/strong&gt;&lt;/h3&gt;
    &lt;p&gt;The easiest method of quantization PyTorch supports is called &lt;strong&gt;dynamic quantization&lt;/strong&gt;. This involves not just converting the weights to int8 - as happens in all quantization variants - but also converting the activations to int8 on the fly, just before doing the computation (hence “dynamic”). The computations will thus be performed using efficient int8 matrix multiplication and convolution implementations, resulting in faster compute. However, the activations are read and written to memory in floating point format.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;PyTorch API&lt;/strong&gt;: we have a simple API for dynamic quantization in PyTorch. &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.quantize_dynamic&lt;/code&gt; takes in a model, as well as a couple other arguments, and produces a quantized model! Our &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;end-to-end tutorial&lt;/a&gt; illustrates this for a BERT model; while the tutorial is long and contains sections on loading pre-trained models and other concepts unrelated to quantization, the part the quantizes the BERT model is simply:&lt;/li&gt;
    &lt;/ul&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.quantization&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;quantized_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantize_dynamic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qint8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;See the documentation for the function &lt;a href=&quot;https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic&quot;&gt;here&lt;/a&gt; an end-to-end example in our tutorials &lt;a href=&quot;https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;post-training-static-quantization&quot;&gt;&lt;strong&gt;Post-Training Static Quantization&lt;/strong&gt;&lt;/h3&gt;

    &lt;p&gt;One can further improve the performance (latency) by converting networks to use both integer arithmetic and int8 memory accesses. Static quantization performs the additional step of first feeding batches of data through the network and computing the resulting distributions of the different activations (specifically, this is done by inserting “observer” modules at different points that record these distributions). This information is used to determine how specifically the different activations should be quantized at inference time (a simple technique would be to simply divide the entire range of activations into 256 levels, but we support more sophisticated methods as well). Importantly, this additional step allows us to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.&lt;/p&gt;

    &lt;p&gt;With this release, we’re supporting several features that allow users to optimize their static quantization:&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Observers: you can customize observer modules which specify how statistics are collected prior to quantization to try out more advanced methods to quantize your data.&lt;/li&gt;
      &lt;li&gt;Operator fusion: you can fuse multiple operations into a single operation, saving on memory access while also improving the operation’s numerical accuracy.&lt;/li&gt;
      &lt;li&gt;Per-channel quantization: we can independently quantize weights for each output channel in a convolution/linear layer, which can lead to higher accuracy with almost the same speed.&lt;/li&gt;
    &lt;/ol&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;h3 id=&quot;pytorch-api&quot;&gt;&lt;strong&gt;PyTorch API&lt;/strong&gt;:&lt;/h3&gt;
        &lt;ul&gt;
          &lt;li&gt;To fuse modules, we have &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.fuse_modules&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;Observers are inserted using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.prepare&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;Finally, quantization itself is done using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.convert&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;We have a tutorial with an end-to-end example of quantization (this same tutorial also covers our third quantization method, quantization-aware training), but because of our simple API, the three lines that perform post-training static quantization on the pre-trained model &lt;code class=&quot;highlighter-rouge&quot;&gt;myModel&lt;/code&gt; are:&lt;/p&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# set quantization config for server (x86)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;deploymentmyModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fbgemm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# insert observers&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Calibrate the model and collect statistics&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# convert to quantized version&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;quantization-aware-training&quot;&gt;&lt;strong&gt;Quantization Aware Training&lt;/strong&gt;&lt;/h3&gt;
    &lt;p&gt;&lt;strong&gt;Quantization-aware training(QAT)&lt;/strong&gt; is the third method, and the one that typically results in highest accuracy of these three. With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method usually yields higher accuracy than the other two methods.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;h3 id=&quot;pytorch-api-1&quot;&gt;&lt;strong&gt;PyTorch API&lt;/strong&gt;:&lt;/h3&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.prepare_qat&lt;/code&gt; inserts fake quantization modules to model quantization.&lt;/li&gt;
          &lt;li&gt;Mimicking the static quantization API, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.convert&lt;/code&gt; actually quantizes the model once training is complete.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;For example, in &lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;the end-to-end example&lt;/a&gt;, we load in a pre-trained model as &lt;code class=&quot;highlighter-rouge&quot;&gt;qat_model&lt;/code&gt;, then we simply perform quantization-aware training using:&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# specify quantization config for QAT&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;qat_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_qat_qconfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fbgemm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# prepare QAT&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare_qat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qat_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# convert to quantized version, removing dropout, to check for accuracy on each&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epochquantized_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qat_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;device-and-operator-support&quot;&gt;&lt;strong&gt;Device and Operator Support&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Quantization support is restricted to a subset of available operators, depending on the method being used, for a list of supported operators, please see the documentation at &lt;a href=&quot;https://pytorch.org/docs/stable/quantization.html&quot;&gt;https://pytorch.org/docs/stable/quantization.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The set of available operators and the quantization numerics also depend on the backend being used to run quantized models. Currently quantized operators are supported only for CPU inference in the following backends: x86 and ARM. Both the quantization configuration (how tensors should be quantized and the quantized kernels (arithmetic with quantized tensors) are backend dependent. One can specify the backend by doing:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchbackend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fbgemm'&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 'fbgemm' for server, 'qnnpack' for mobile&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_qconfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# prepare and convert model&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Set the backend on which the quantized kernels need to be run&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backends&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, quantization aware training occurs in full floating point and can run on either GPU or CPU. Quantization aware training is typically only used in CNN models when post training static or dynamic quantization doesn’t yield sufficient accuracy. This can occur with models that are highly optimized to achieve small size (such as Mobilenet).&lt;/p&gt;

&lt;h4 id=&quot;integration-in-torchvision&quot;&gt;&lt;strong&gt;Integration in torchvision&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We’ve also enabled quantization for some of the most popular models in &lt;a href=&quot;https://github.com/pytorch/vision/tree/master/torchvision/models/quantization&quot;&gt;torchvision&lt;/a&gt;: Googlenet, Inception, Resnet, ResNeXt, Mobilenet and Shufflenet. We have upstreamed these changes to torchvision in three forms:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Pre-trained quantized weights so that you can use them right away.&lt;/li&gt;
  &lt;li&gt;Quantization ready model definitions so that you can do post-training quantization or quantization aware training.&lt;/li&gt;
  &lt;li&gt;A script for doing quantization aware training — which is available for any of these model though, as you will learn below, we only found it necessary for achieving accuracy with Mobilenet.&lt;/li&gt;
  &lt;li&gt;We also have a &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html&quot;&gt;tutorial&lt;/a&gt; showing how you can do transfer learning with quantization using one of the torchvision models.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;choosing-an-approach&quot;&gt;&lt;strong&gt;Choosing an approach&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The choice of which scheme to use depends on multiple factors:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Model/Target requirements: Some models might be sensitive to quantization, requiring quantization aware training.&lt;/li&gt;
  &lt;li&gt;Operator/Backend support: Some backends require fully quantized operators.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Currently, operator coverage is limited and may restrict the choices listed in the table below:
The table below provides a guideline.&lt;/p&gt;

&lt;style type=&quot;text/css&quot;&gt;
.tg  {border-collapse:collapse;border-spacing:0;border-color:black;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top;font-weight:bold;color:black;}
article.pytorch-article table tr th:first-of-type, article.pytorch-article table tr td:first-of-type{padding-left:5px}
&lt;/style&gt;

&lt;table class=&quot;tg&quot;&gt;
  &lt;tr&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;Model Type&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;Preferred scheme&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;Why&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;LSTM/RNN&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Dynamic Quantization&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Throughput dominated by compute/memory bandwidth for weights&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;BERT/Transformer&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Dynamic Quantization&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Throughput dominated by compute/memory bandwidth for weights&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;CNN&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Static Quantization&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Throughput limited by memory bandwidth for activations&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;CNN&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Quantization Aware Training&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;In the case where accuracy can't be achieved with static quantization&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;performance-results&quot;&gt;&lt;strong&gt;Performance Results&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Quantization provides a 4x reduction in the model size and a speedup of 2x to 3x compared to floating point implementations depending on the hardware platform and the model being benchmarked. Some sample results are:&lt;/p&gt;

&lt;div class=&quot;table-responsive&quot;&gt;
  &lt;table class=&quot;tg&quot;&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Model&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Float Latency (ms)&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Quantized Latency (ms)&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Inference Performance Gain&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Device&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Notes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;BERT&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;581&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;313&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;1.8x&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Xeon-D2191 (1.6GHz)&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Batch size = 1, Maximum sequence length= 128, Single thread, x86-64, Dynamic quantization&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Resnet-50&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;214&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;103&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;2x&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Xeon-D2191 (1.6GHz)&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Single thread, x86-64, Static quantization&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Mobilenet-v2&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;97&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;17&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;5.7x&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Samsung S9&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Static quantization, Floating point numbers are based on Caffe2 run-time and are not optimized&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;accuracy-results&quot;&gt;&lt;strong&gt;Accuracy results&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;We also compared the accuracy of static quantized models with the floating point models on Imagenet. For dynamic quantization, we &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/run_glue.py&quot;&gt;compared&lt;/a&gt; the F1 score of BERT on the GLUE benchmark for MRPC.&lt;/p&gt;

&lt;h4 id=&quot;computer-vision-model-accuracy&quot;&gt;&lt;strong&gt;Computer Vision Model accuracy&lt;/strong&gt;&lt;/h4&gt;

&lt;table class=&quot;tg&quot;&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Model&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Top-1 Accuracy (Float)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Top-1 Accuracy (Quantized)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Quantization scheme&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Googlenet&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;69.8&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;69.7&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Inception-v3&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;77.5&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;77.1&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;ResNet-18&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;69.8&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;69.4&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Resnet-50&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;76.1&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;75.9&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;ResNext-101 32x8d&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;79.3&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;79&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Mobilenet-v2&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;71.9&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;71.6&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Quantization Aware Training&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Shufflenet-v2&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;69.4&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;68.4&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;speech-and-nlp-model-accuracy&quot;&gt;&lt;strong&gt;Speech and NLP Model accuracy&lt;/strong&gt;&lt;/h4&gt;

&lt;div class=&quot;table-responsive&quot;&gt;
  &lt;table class=&quot;tg&quot;&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Model&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;F1 (GLUEMRPC) Float&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;F1 (GLUEMRPC) Quantized&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Quantization scheme&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;BERT&lt;/td&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;0.902&lt;/td&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;0.895&lt;/td&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;Dynamic quantization&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;To get started on quantizing your models in PyTorch, start with &lt;a href=&quot;https://pytorch.org/tutorials/#model-optimization&quot;&gt;the tutorials on the PyTorch website&lt;/a&gt;. If you are working with sequence data start with &lt;a href=&quot;https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html&quot;&gt;dynamic quantization for LSTM&lt;/a&gt;, or &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;BERT&lt;/a&gt;. If you are working with image data then we recommend starting with the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html&quot;&gt;transfer learning with quantization&lt;/a&gt; tutorial. Then you can explore &lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;static post training quantization&lt;/a&gt;. If you find that the accuracy drop with post training quantization is too high, then try &lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;quantization aware training&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you run into issues you can get community help by posting in at &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;discuss.pytorch.org&lt;/a&gt;, use the quantization category for quantization related issues.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is authored by Raghuraman Krishnamoorthi, James Reed, Min Ni, Chris Gottbrath and Seth Weidman. Special thanks to Jianyu Huang, Lingyi Liu and Haixin Liu for producing quantization metrics included in this post.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;PyTorch quantization presentation at Neurips: &lt;a href=&quot;https://research.fb.com/wp-content/uploads/2019/12/2.-Quantization.pptx&quot;&gt;(https://research.fb.com/wp-content/uploads/2019/12/2.-Quantization.pptx)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Quantized Tensors &lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor&quot;&gt;(https://github.com/pytorch/pytorch/wiki/
Introducing-Quantized-Tensor)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Quantization RFC on Github &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/18318&quot;&gt;(https://github.com/pytorch/pytorch/
issues/18318)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Raghuraman Krishnamoorthi, James Reed, Min Ni, Chris Gottbrath, and Seth Weidman</name></author><summary type="html">It’s important to make efficient use of both server-side and on-device compute resources when developing machine learning applications. To support more efficient deployment on servers and edge devices, PyTorch added a support for model quantization using the familiar eager mode Python API.</summary></entry><entry><title type="html">PyTorch 1.4 released, domain libraries updated</title><link href="https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated/" rel="alternate" type="text/html" title="PyTorch 1.4 released, domain libraries updated" /><published>2020-01-15T00:00:00-08:00</published><updated>2020-01-15T00:00:00-08:00</updated><id>https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated/">&lt;p&gt;Today, we’re announcing the availability of PyTorch 1.4, along with updates to the PyTorch domain libraries. These releases build on top of the announcements from &lt;a href=&quot;https://pytorch.org/blog/pytorch-adds-new-tools-and-libraries-welcomes-preferred-networks-to-its-community/&quot;&gt;NeurIPS 2019&lt;/a&gt;, where we shared the availability of PyTorch Elastic, a new classification framework for image and video, and the addition of Preferred Networks to the PyTorch community. For those that attended the workshops at NeurIPS, the content can be found &lt;a href=&quot;https://research.fb.com/neurips-2019-expo-workshops/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-14&quot;&gt;PyTorch 1.4&lt;/h2&gt;

&lt;p&gt;The 1.4 release of PyTorch adds new capabilities, including the ability to do fine grain build level customization for PyTorch Mobile, and new experimental features including support for model parallel training and Java language bindings.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-mobile---build-level-customization&quot;&gt;PyTorch Mobile - Build level customization&lt;/h3&gt;

&lt;p&gt;Following the open sourcing of &lt;a href=&quot;https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/&quot;&gt;PyTorch Mobile in the 1.3 release&lt;/a&gt;, PyTorch 1.4 adds additional mobile support including the ability to customize build scripts at a fine-grain level. This allows mobile developers to optimize library size by only including the operators used by their models and, in the process, reduce their on device footprint significantly. Initial results show that, for example, a customized MobileNetV2 is 40% to 50% smaller than the prebuilt PyTorch mobile library. You can learn more &lt;a href=&quot;https://pytorch.org/mobile/home/&quot;&gt;here&lt;/a&gt; about how to create your own custom builds and, as always, please engage with the community on the &lt;a href=&quot;https://discuss.pytorch.org/c/mobile&quot;&gt;PyTorch forums&lt;/a&gt; to provide any feedback you have.&lt;/p&gt;

&lt;p&gt;Example code snippet for selectively compiling only the operators needed for MobileNetV2:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Dump list of operators used by MobileNetV2:&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yaml&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'MobileNetV2.pt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;export_opnames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'MobileNetV2.yaml'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;yaml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;#&lt;/span&gt; Build PyTorch Android library customized &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;MobileNetV2:
&lt;span class=&quot;go&quot;&gt;SELECTED_OP_LIST=MobileNetV2.yaml scripts/build_pytorch_android.sh arm64-v8a

&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;#&lt;/span&gt; Build PyTorch iOS library customized &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;MobileNetV2:
&lt;span class=&quot;go&quot;&gt;SELECTED_OP_LIST=MobileNetV2.yaml BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 scripts/build_ios.sh
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;distributed-model-parallel-training-experimental&quot;&gt;Distributed model parallel training (Experimental)&lt;/h3&gt;

&lt;p&gt;With the scale of models, such as RoBERTa, continuing to increase into the billions of parameters, model parallel training has become ever more important to help researchers push the limits. This release provides a distributed RPC framework to support distributed model parallel training. It allows for running functions remotely and referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backwards and update parameters across RPC boundaries.&lt;/p&gt;

&lt;p&gt;To learn more about the APIs and the design of this feature, see the links below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html&quot;&gt;API documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/distributed_autograd.html&quot;&gt;Distributed Autograd design doc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/rref.html&quot;&gt;Remote Reference design doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the full tutorials, see the links below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/rpc_tutorial.html&quot;&gt;A full RPC tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/examples/tree/master/distributed/rpc&quot;&gt;Examples using model parallel training for reinforcement learning and with an LSTM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As always, you can connect with community members and discuss more on the &lt;a href=&quot;https://discuss.pytorch.org/c/distributed/distributed-rpc&quot;&gt;forums&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;java-bindings-experimental&quot;&gt;Java bindings (Experimental)&lt;/h3&gt;

&lt;p&gt;In addition to supporting Python and C++, this release adds experimental support for Java bindings. Based on the interface developed for Android in PyTorch Mobile, the new bindings allow you to invoke TorchScript models from any Java program. Note that the Java bindings are only available for Linux for this release, and for inference only. We expect support to expand in subsequent releases. See the code snippet below for how to use PyTorch within Java:&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;demo-model.pt1&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;fromBlob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// data&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// shape&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IValue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toTensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;shape: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Arrays&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Arrays&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getDataAsFloatArray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Learn more about how to use PyTorch from Java &lt;a href=&quot;https://github.com/pytorch/java-demo&quot;&gt;here&lt;/a&gt;, and see the full Javadocs API documentation &lt;a href=&quot;https://pytorch.org/javadoc/1.4.0/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For the full 1.4 release notes, see &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;domain-libraries&quot;&gt;Domain Libraries&lt;/h2&gt;

&lt;p&gt;PyTorch domain libraries like torchvision, torchtext, and torchaudio complement PyTorch with common datasets, models, and transforms. We’re excited to share new releases for all three domain libraries alongside the PyTorch 1.4 core release.&lt;/p&gt;

&lt;h3 id=&quot;torchvision-05&quot;&gt;torchvision 0.5&lt;/h3&gt;

&lt;p&gt;The improvements to torchvision 0.5 mainly focus on adding support for production deployment including quantization, TorchScript, and ONNX. Some of the highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All models in torchvision are now torchscriptable making them easier to ship into non-Python production environments&lt;/li&gt;
  &lt;li&gt;ResNets, MobileNet, ShuffleNet, GoogleNet and InceptionV3 now have quantized counterparts with pre-trained models, and also include scripts for quantization-aware training.&lt;/li&gt;
  &lt;li&gt;In partnership with the Microsoft team, we’ve added ONNX support for all models including Mask R-CNN.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about torchvision 0.5 &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchaudio-04&quot;&gt;torchaudio 0.4&lt;/h3&gt;

&lt;p&gt;Improvements in torchaudio 0.4 focus on enhancing the currently available transformations, datasets, and backend support. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SoX is now optional, and a new extensible backend dispatch mechanism exposes SoundFile as an alternative to SoX.&lt;/li&gt;
  &lt;li&gt;The interface for datasets has been unified. This enables the addition of two large datasets: LibriSpeech and Common Voice.&lt;/li&gt;
  &lt;li&gt;New filters such as biquad, data augmentation such as time and frequency masking, transforms such as MFCC, gain and dither, and new feature computation such as deltas, are now available.&lt;/li&gt;
  &lt;li&gt;Transformations now support batches and are jitable.&lt;/li&gt;
  &lt;li&gt;An interactive speech recognition demo with voice activity detection is available for experimentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about torchaudio 0.4 &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchtext-05&quot;&gt;torchtext 0.5&lt;/h3&gt;

&lt;p&gt;torchtext 0.5 focuses mainly on improvements to the dataset loader APIs, including compatibility with core PyTorch APIs, but also adds support for unsupervised text tokenization. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Added bindings for SentencePiece for unsupervised text tokenization .&lt;/li&gt;
  &lt;li&gt;Added a new unsupervised learning dataset - enwik9.&lt;/li&gt;
  &lt;li&gt;Made revisions to PennTreebank, WikiText103, WikiText2, IMDb to make them compatible with torch.utils.data. Those datasets are in an experimental folder and we welcome your feedback.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about torchtext 0.5 &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we’re announcing the availability of PyTorch 1.4, along with updates to the PyTorch domain libraries. These releases build on top of the announcements from NeurIPS 2019, where we shared the availability of PyTorch Elastic, a new classification framework for image and video, and the addition of Preferred Networks to the PyTorch community. For those that attended the workshops at NeurIPS, the content can be found here.</summary></entry></feed>