<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pytorch.org/" rel="alternate" type="text/html" /><updated>2021-03-08T07:02:56-08:00</updated><id>https://pytorch.org/</id><title type="html">PyTorch Website</title><subtitle>Scientific Computing...</subtitle><author><name>Facebook</name></author><entry><title type="html">PyTorch 1.8 Release, including Compiler and Distributed Training updates, and New Mobile Tutorials</title><link href="https://pytorch.org/blog/pytorch-1.8-released/" rel="alternate" type="text/html" title="PyTorch 1.8 Release, including Compiler and Distributed Training updates, and New Mobile Tutorials" /><published>2021-03-04T00:00:00-08:00</published><updated>2021-03-04T00:00:00-08:00</updated><id>https://pytorch.org/blog/pytorch-1.8-released</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.8-released/">&lt;p&gt;We are excited to announce the availability of PyTorch 1.8. This release is composed of more than 3,000 commits since 1.7. It includes major updates and new features for compilation, code optimization, frontend APIs for scientific computing, and AMD ROCm support through binaries that are available via pytorch.org. It also provides improved features for large-scale training for pipeline and model parallelism, and gradient compression. A few of the highlights include:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Support for doing python to python functional transformations via &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fx&lt;/code&gt;;&lt;/li&gt;
  &lt;li&gt;Added or stabilized APIs to support FFTs (&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt;), Linear Algebra functions (&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt;), added support for autograd for complex tensors and updates to improve performance for calculating hessians and jacobians; and&lt;/li&gt;
  &lt;li&gt;Significant updates and improvements to distributed training including: Improved NCCL reliability; Pipeline parallelism support; RPC profiling; and support for communication hooks adding gradient compression.
See the full release notes &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Along with 1.8, we are also releasing major updates to PyTorch libraries including &lt;a href=&quot;https://github.com/pytorch/csprng&quot;&gt;TorchCSPRNG&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/vision&quot;&gt;TorchVision&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/text&quot;&gt;TorchText&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/audio&quot;&gt;TorchAudio&lt;/a&gt;. For more on the library releases, see the post &lt;a href=&quot;http://pytorch.org/blog/pytorch-1.8-new-library-releases&quot;&gt;here&lt;/a&gt;. As previously noted, features in PyTorch releases are classified as Stable, Beta and Prototype. You can learn more about the definitions in the post &lt;a href=&quot;https://pytorch.org/blog/pytorch-feature-classification-changes/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;new-and-updated-apis&quot;&gt;New and Updated APIs&lt;/h1&gt;
&lt;p&gt;The PyTorch 1.8 release brings a host of new and updated API surfaces ranging from additional APIs for NumPy compatibility, also support for ways to improve and scale your code for performance at both inference and training time. Here is a brief summary of the major features coming in this release:&lt;/p&gt;

&lt;h3 id=&quot;stable-torchfft-support-for-high-performance-numpy-style-ffts&quot;&gt;[Stable] &lt;code class=&quot;highlighter-rouge&quot;&gt;Torch.fft&lt;/code&gt; support for high performance NumPy style FFTs&lt;/h3&gt;
&lt;p&gt;As part of PyTorch’s goal to support scientific computing, we have invested in improving our FFT support and with PyTorch 1.8, we are releasing the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module. This module implements the same functions as NumPy’s &lt;code class=&quot;highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module, but with support for hardware acceleration and autograd.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;See this &lt;a href=&quot;https://pytorch.org/blog/the-torch.fft-module-accelerated-fast-fourier-transforms-with-autograd-in-pytorch&quot;&gt;blog post&lt;/a&gt; for more details&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/1.8.0/fft.html&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;beta-support-for-numpy-style-linear-algebra-functions-via-torchlinalg&quot;&gt;[Beta] Support for NumPy style linear algebra functions via &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; module, modeled after NumPy’s &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.linalg.html?highlight=linalg#module-numpy.linalg&quot;&gt;np.linalg&lt;/a&gt; module, brings NumPy-style support for common linear algebra operations including Cholesky decompositions, determinants, eigenvalues and many others.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/1.8.0/linalg.html&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-python-code-transformations-with-fx&quot;&gt;[Beta] Python code Transformations with FX&lt;/h2&gt;
&lt;p&gt;FX allows you to write transformations of the form &lt;code class=&quot;highlighter-rouge&quot;&gt;transform(input_module : nn.Module)&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt;, where you can feed in a &lt;code class=&quot;highlighter-rouge&quot;&gt;Module&lt;/code&gt; instance and get a transformed &lt;code class=&quot;highlighter-rouge&quot;&gt;Module&lt;/code&gt; instance out of it.&lt;/p&gt;

&lt;p&gt;This kind of functionality is applicable in many scenarios. For example, the FX-based Graph Mode Quantization product is releasing as a prototype contemporaneously with FX. Graph Mode Quantization automates the process of quantizing a neural net and does so by leveraging FX’s program capture, analysis and transformation facilities. We are also developing many other transformation products with FX and we are excited to share this powerful toolkit with the community.&lt;/p&gt;

&lt;p&gt;Because FX transforms consume and produce nn.Module instances, they can be used within many existing PyTorch workflows. This includes workflows that, for example, train in Python then deploy via TorchScript.&lt;/p&gt;

&lt;p&gt;You can read more about FX in the official &lt;a href=&quot;https://pytorch.org/docs/master/fx.html&quot;&gt;documentation&lt;/a&gt;. You can also find several examples of program transformations implemented using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fx&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/fx&quot;&gt;here&lt;/a&gt;. We are constantly improving FX and invite you to share any feedback you have about the toolkit on the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;forums&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/pytorch/issues&quot;&gt;issue tracker&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;distributed-training&quot;&gt;Distributed Training&lt;/h1&gt;
&lt;p&gt;The PyTorch 1.8 release added a number of new features as well as improvements to reliability and usability. Concretely, support for: &lt;a href=&quot;https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group&quot;&gt;Stable level async error/timeout handling&lt;/a&gt; was added to improve NCCL reliability; and stable support for &lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html&quot;&gt;RPC based profiling&lt;/a&gt;. Additionally, we have added support for pipeline parallelism as well as gradient compression through the use of communication hooks in DDP. Details are below:&lt;/p&gt;

&lt;h3 id=&quot;beta-pipeline-parallelism&quot;&gt;[Beta] Pipeline Parallelism&lt;/h3&gt;
&lt;p&gt;As machine learning models continue to grow in size, traditional Distributed DataParallel (DDP) training no longer scales as these models don’t fit on a single GPU device. The new pipeline parallelism feature provides an easy to use PyTorch API to leverage pipeline parallelism as part of your training loop.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/44827&quot;&gt;RFC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/1.8.0/pipeline.html?highlight=pipeline#&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;beta-ddp-communication-hook&quot;&gt;[Beta] DDP Communication Hook&lt;/h3&gt;
&lt;p&gt;The DDP communication hook is a generic interface to control how to communicate gradients across workers by overriding the vanilla allreduce in DistributedDataParallel. A few built-in communication hooks are provided including PowerSGD, and users can easily apply any of these hooks to optimize communication. Additionally, the communication hook interface can also support user-defined communication strategies for more advanced use cases.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/39272&quot;&gt;RFC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/1.8.0/ddp_comm_hooks.html?highlight=powersgd&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;additional-prototype-features-for-distributed-training&quot;&gt;Additional Prototype Features for Distributed Training&lt;/h3&gt;
&lt;p&gt;In addition to the major stable and beta distributed training features in this release, we also have a number of prototype features available in our nightlies to try out and provide feedback. We have linked in the draft docs below for reference:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;(Prototype) ZeroRedundancyOptimizer&lt;/strong&gt; - Based on and in partnership with the Microsoft DeepSpeed team, this feature helps reduce per-process memory footprint by sharding optimizer states across all participating processes in the &lt;code class=&quot;highlighter-rouge&quot;&gt;ProcessGroup&lt;/code&gt; gang. Refer to this &lt;a href=&quot;https://pytorch.org/docs/master/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer&quot;&gt;documentation&lt;/a&gt; for more details.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(Prototype) Process Group NCCL Send/Recv&lt;/strong&gt; - The NCCL send/recv API was introduced in v2.7 and this feature adds support for it in NCCL process groups. This feature will provide an option for users to implement collective operations at Python layer instead of C++ layer. Refer to this &lt;a href=&quot;https://pytorch.org/docs/master/distributed.html#distributed-communication-package-torch-distributed&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/distributed/distributed_c10d.py#L899&quot;&gt;code examples&lt;/a&gt; to learn more.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(Prototype) CUDA-support in RPC using TensorPipe&lt;/strong&gt; - This feature should bring consequent speed improvements for users of PyTorch RPC with multiple-GPU machines, as TensorPipe will automatically leverage NVLink when available, and avoid costly copies to and from host memory when exchanging GPU tensors between processes. When not on the same machine, TensorPipe will fall back to copying the tensor to host memory and sending it as a regular CPU tensor. This will also improve the user experience as users will be able to treat GPU tensors like regular CPU tensors in their code. Refer to this &lt;a href=&quot;https://pytorch.org/docs/1.8.0/rpc.html&quot;&gt;documentation&lt;/a&gt; for more details.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(Prototype) Remote Module&lt;/strong&gt; - This feature allows users to operate a module on a remote worker like using a local module, where the RPCs are transparent to the user. In the past, this functionality was implemented in an ad-hoc way and overall this feature will improve the usability of model parallelism on PyTorch. Refer to this &lt;a href=&quot;https://pytorch.org/docs/master/rpc.html#remotemodule&quot;&gt;documentation&lt;/a&gt; for more details.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;pytorch-mobile&quot;&gt;PyTorch Mobile&lt;/h1&gt;
&lt;p&gt;Support for PyTorch Mobile is expanding with a new set of tutorials to help new users launch models on-device quicker and give existing users a tool to get more out of our framework. These include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/deeplabv3_on_ios.html&quot;&gt;Image segmentation DeepLabV3 on iOS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/deeplabv3_on_android.html&quot;&gt;Image segmentation DeepLabV3 on Android&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our new demo apps also include examples of image segmentation, object detection, neural machine translation, question answering, and vision transformers. They are available on both iOS and Android:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/ios-demo-app&quot;&gt;iOS demo app&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/android-demo-app&quot;&gt;Android demo app&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to performance improvements on CPU for MobileNetV3 and other models, we also revamped our Android GPU backend prototype for broader models coverage and faster inferencing:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/prototype/vulkan_workflow.html&quot;&gt;Android tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lastly, we are launching the PyTorch Mobile Lite Interpreter as a prototype feature in this release. The Lite Interpreter allows users to reduce the runtime binary size. Please try these out and send us your feedback on the &lt;a href=&quot;https://discuss.pytorch.org/c/mobile/&quot;&gt;PyTorch Forums&lt;/a&gt;. All our latest updates can be found on the &lt;a href=&quot;https://pytorch.org/mobile/home/&quot;&gt;PyTorch Mobile page&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;prototype-pytorch-mobile-lite-interpreter&quot;&gt;[Prototype] PyTorch Mobile Lite Interpreter&lt;/h3&gt;
&lt;p&gt;PyTorch Lite Interpreter is a streamlined version of the PyTorch runtime that can execute PyTorch programs in resource constrained devices, with reduced binary size footprint. This prototype feature reduces binary sizes by up to 70% compared to the current on-device runtime in the current release.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/prototype/lite_interpreter.html&quot;&gt;iOS/Android Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;performance-optimization&quot;&gt;Performance Optimization&lt;/h1&gt;
&lt;p&gt;In 1.8, we are releasing the support for benchmark utils to enable users to better monitor performance. We are also opening up a new automated quantization API. See the details below:&lt;/p&gt;

&lt;h3 id=&quot;beta-benchmark-utils&quot;&gt;(Beta) Benchmark utils&lt;/h3&gt;
&lt;p&gt;Benchmark utils allows users to take accurate performance measurements, and provides composable tools to help with both benchmark formulation and post processing. This expected to be helpful for contributors to PyTorch to quickly understand how their contributions are impacting PyTorch performance.&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.benchmark&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;timer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;stmt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;torch.add(x, y, out=out)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;setup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
            n = 1024
            x = torch.ones((n, n))
            y = torch.ones((n, 1))
            out = torch.empty((n, n))
        &quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blocked_autorange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_run_time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{num_threads} thread{'s' if num_threads &amp;gt; 1 else ' ':&amp;lt;4}&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{results[-1].median * 1e6:&amp;gt;4.0f} us   &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;({results[0].median / results[-1].median:.1f}x)&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;376&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;   
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threads&lt;/span&gt;    &lt;span class=&quot;mi&quot;&gt;189&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threads&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.8&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/1.8.0/benchmark_utils.html?highlight=benchmark#&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/benchmark.html&quot;&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prototype-fx-graph-mode-quantization&quot;&gt;(Prototype) FX Graph Mode Quantization&lt;/h3&gt;
&lt;p&gt;FX Graph Mode Quantization is the new automated quantization API in PyTorch. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fx&lt;/code&gt;).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tutorials:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html&quot;&gt;(Prototype) FX Graph Mode Post Training Dynamic Quantization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html&quot;&gt;(Prototype) FX Graph Mode Quantization User Guide&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hardware-support&quot;&gt;Hardware Support&lt;/h1&gt;

&lt;h3 id=&quot;beta-ability-to-extend-the-pytorch-dispatcher-for-a-new-backend-in-c&quot;&gt;[Beta] Ability to Extend the PyTorch Dispatcher for a new backend in C++&lt;/h3&gt;
&lt;p&gt;In PyTorch 1.8, you can now create new out-of-tree devices that live outside the &lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch/pytorch&lt;/code&gt; repo. The tutorial linked below shows how to register your device and keep it in sync with native PyTorch devices.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/extend_dispatcher.html&quot;&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;beta-amd-gpu-binaries-now-available&quot;&gt;[Beta] AMD GPU Binaries Now Available&lt;/h3&gt;
&lt;p&gt;Starting in PyTorch 1.8, we have added support for ROCm wheels providing an easy onboarding to using AMD GPUs. You can simply go to the standard &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;PyTorch installation selector&lt;/a&gt; and choose ROCm as an installation option and execute the provided command.&lt;/p&gt;

&lt;p&gt;Thanks for reading, and if you are excited about these updates and want to participate in the future of PyTorch, we encourage you to join the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;discussion forums&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/issues&quot;&gt;open GitHub issues&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Team PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">We are excited to announce the availability of PyTorch 1.8. This release is composed of more than 3,000 commits since 1.7. It includes major updates and new features for compilation, code optimization, frontend APIs for scientific computing, and AMD ROCm support through binaries that are available via pytorch.org. It also provides improved features for large-scale training for pipeline and model parallelism, and gradient compression. A few of the highlights include: Support for doing python to python functional transformations via torch.fx; Added or stabilized APIs to support FFTs (torch.fft), Linear Algebra functions (torch.linalg), added support for autograd for complex tensors and updates to improve performance for calculating hessians and jacobians; and Significant updates and improvements to distributed training including: Improved NCCL reliability; Pipeline parallelism support; RPC profiling; and support for communication hooks adding gradient compression. See the full release notes here.</summary></entry><entry><title type="html">New PyTorch library releases including TorchVision Mobile, TorchAudio I/O, and more</title><link href="https://pytorch.org/blog/pytorch-1.8-new-library-releases/" rel="alternate" type="text/html" title="New PyTorch library releases including TorchVision Mobile, TorchAudio I/O, and more" /><published>2021-03-04T00:00:00-08:00</published><updated>2021-03-04T00:00:00-08:00</updated><id>https://pytorch.org/blog/pytorch-1.8-new-library-releases</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.8-new-library-releases/">&lt;p&gt;Today, we are announcing updates to a number of PyTorch libraries, alongside the &lt;a href=&quot;https://pytorch.org/blog/pytorch-1.8-released&quot;&gt;PyTorch 1.8 release&lt;/a&gt;. The updates include new releases for the domain libraries including TorchVision, TorchText and TorchAudio as well as new version of TorchCSPRNG. These releases include a number of new features and improvements and, along with the PyTorch 1.8 release, provide a broad set of updates for the PyTorch community to build on and leverage.&lt;/p&gt;

&lt;p&gt;Some highlights include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;TorchVision&lt;/strong&gt; - Added support for PyTorch Mobile including &lt;a href=&quot;https://ai.facebook.com/blog/d2go-brings-detectron2-to-mobile&quot;&gt;Detectron2Go&lt;/a&gt; (D2Go), auto-augmentation of data during training, on the fly type conversion, and &lt;a href=&quot;https://pytorch.org/docs/stable/amp.html&quot;&gt;AMP autocasting&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchAudio&lt;/strong&gt; - Major improvements to I/O, including defaulting to sox_io backend and file-like object support. Added Kaldi Pitch feature and support for CMake based build allowing TorchAudio to better support no-Python environments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchText&lt;/strong&gt; - Updated the dataset loading API to be compatible with standard PyTorch data loading utilities.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchCSPRNG&lt;/strong&gt; - Support for cryptographically secure pseudorandom number generators for PyTorch is now stable with new APIs for AES128 ECB/CTR and CUDA support on Windows.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please note that, starting in PyTorch 1.6, features are classified as Stable, Beta, and Prototype. Prototype features are not included as part of the binary distribution and are instead available through either building from source, using nightlies or via compiler flag. You can see the detailed announcement &lt;a href=&quot;https://pytorch.org/blog/pytorch-feature-classification-changes/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;torchvision-090&quot;&gt;TorchVision 0.9.0&lt;/h1&gt;
&lt;h3 id=&quot;stable-torchvision-mobile-operators-android-binaries-and-tutorial&quot;&gt;[Stable] TorchVision Mobile: Operators, Android Binaries, and Tutorial&lt;/h3&gt;
&lt;p&gt;We are excited to announce the first on-device support and binaries for a PyTorch domain library. We have seen significant appetite in both research and industry for on-device vision support to allow low latency, privacy friendly, and resource efficient mobile vision experiences. You can follow this &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/d2go/D2Go&quot;&gt;new tutorial&lt;/a&gt; to build your own Android object detection app using TorchVision operators, D2Go, or your own customer operators and model.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/tochvisionmobile.png&quot; width=&quot;30%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;stable-new-mobile-models-for-classification-object-detection-and-semantic-segmentation&quot;&gt;[Stable] New Mobile models for Classification, Object Detection and Semantic Segmentation&lt;/h3&gt;
&lt;p&gt;We have added support for the MobileNetV3 architecture and provided pre-trained weights for Classification, Object Detection and Segmentation. It is easy to get up and running with these models, just import and load them as you would any &lt;code class=&quot;highlighter-rouge&quot;&gt;torchvision&lt;/code&gt; model:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Classification&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mobilenet_v3_large&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Quantized Classification&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mobilenet_v3_large&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_classifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Object Detection: Highly Accurate High Resolution Mobile Model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fasterrcnn_mobilenet_v3_large_fpn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Semantic Segmentation: Highly Accurate Mobile Model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;520&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;520&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_segmenter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;segmentation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deeplabv3_mobilenet_v3_large&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_segmenter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_segmenter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;These models are highly competitive with TorchVision’s existing models on resource efficiency, speed, and accuracy. See our &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;release notes&lt;/a&gt; for detailed performance metrics.&lt;/p&gt;

&lt;h3 id=&quot;stable-autoaugment&quot;&gt;[Stable] AutoAugment&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1805.09501.pdf&quot;&gt;AutoAugment&lt;/a&gt; is a common Data Augmentation technique that can increase the accuracy of Scene Classification models. Though the data augmentation policies are directly linked to their trained dataset, empirical studies show that ImageNet policies provide significant improvements when applied to other datasets. We’ve implemented 3 policies learned on the following datasets: ImageNet, CIFA10 and SVHN. These can be used standalone or mixed-and-matched with existing transforms:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AutoAugment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Compose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AutoAugment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;other-new-features-for-torchvision&quot;&gt;Other New Features for TorchVision&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;[Stable] All read and decode methods in the io.image package now support:
    &lt;ul&gt;
      &lt;li&gt;Palette, Grayscale Alpha and RBG Alpha image types during PNG decoding&lt;/li&gt;
      &lt;li&gt;On-the-fly conversion of image from one type to the other during read&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;[Stable] WiderFace dataset&lt;/li&gt;
  &lt;li&gt;[Stable] Improved FasterRCNN speed and accuracy by introducing a score threshold on RPN&lt;/li&gt;
  &lt;li&gt;[Stable] Modulation input for DeformConv2D&lt;/li&gt;
  &lt;li&gt;[Stable] Option to write audio to a video file&lt;/li&gt;
  &lt;li&gt;[Stable] Utility to draw bounding boxes&lt;/li&gt;
  &lt;li&gt;[Beta] Autocast support in all Operators
Find the full TorchVision release notes &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;torchaudio-080&quot;&gt;TorchAudio 0.8.0&lt;/h1&gt;
&lt;h3 id=&quot;io-improvements&quot;&gt;I/O Improvements&lt;/h3&gt;
&lt;p&gt;We have continued our work from the &lt;a href=&quot;https://github.com/pytorch/audio/releases/tag/v0.7.0&quot;&gt;previous release&lt;/a&gt; to improve TorchAudio’s I/O support, including:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;[Stable] Changing the default backend to “sox_io” (for Linux/macOS), and updating the “soundfile” backend’s interface to align with that of “sox_io”. The legacy backend and interface are still accessible, though it is strongly discouraged to use them.&lt;/li&gt;
  &lt;li&gt;[Stable] File-like object support in both “sox_io” backend, “soundfile” backend and sox_effects.&lt;/li&gt;
  &lt;li&gt;[Stable] New options to change the format, encoding, and bits_per_sample when saving.&lt;/li&gt;
  &lt;li&gt;[Stable] Added GSM, HTK, AMB, AMR-NB and AMR-WB format support to the “sox_io” backend.&lt;/li&gt;
  &lt;li&gt;[Beta] A new &lt;code class=&quot;highlighter-rouge&quot;&gt;functional.apply_codec&lt;/code&gt; function which can degrade audio data by applying audio codecs supported by “sox_io” backend in an in-memory fashion.
Here are some examples of features landed in this release:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Load audio over HTTP&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;waveform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;c&quot;&gt;# Saving to Bytes buffer as 32-bit floating-point PCM&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;buffer_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BytesIO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;buffer_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;waveform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;wav&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;PCM_S&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits_per_sample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;c&quot;&gt;# Apply effects while loading audio from S3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boto3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Bucket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S3_BUCKET&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S3_KEY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;waveform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sox_effects&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply_effect_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Body'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lowpass&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;-1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;300&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;8000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
 
&lt;span class=&quot;c&quot;&gt;# Apply GSM codec to Tensor&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply_codec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;waveform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gsm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check out the revamped audio preprocessing tutorial, &lt;a href=&quot;https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html&quot;&gt;Audio Manipulation with TorchAudio&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stable-switch-to-cmake-based-build&quot;&gt;[Stable] Switch to CMake-based build&lt;/h3&gt;
&lt;p&gt;In the previous version of TorchAudio, it was utilizing CMake to build third party dependencies. Starting in 0.8.0, TorchaAudio uses CMake to build its C++ extension. This will open the door to integrate TorchAudio in non-Python environments (such as C++ applications and mobile). We will continue working on adding example applications and mobile integrations.&lt;/p&gt;

&lt;h3 id=&quot;beta-improved-and-new-audio-transforms&quot;&gt;[Beta] Improved and New Audio Transforms&lt;/h3&gt;
&lt;p&gt;We have added two widely requested operators in this release: the SpectralCentroid transform and the Kaldi Pitch feature extraction (detailed in &lt;a href=&quot;https://ieeexplore.ieee.org/document/6854049&quot;&gt;“A pitch extraction algorithm tuned for automatic speech recognition”&lt;/a&gt;). We’ve also exposed a normalization method to Mel transforms, and additional STFT arguments to Spectrogram. We would like to ask  our community to continue to &lt;a href=&quot;https://github.com/pytorch/audio/issues/new?assignees=&amp;amp;labels=&amp;amp;template=feature-request.md&quot;&gt;raise feature requests&lt;/a&gt; for core audio processing features like these!&lt;/p&gt;

&lt;h3 id=&quot;community-contributions&quot;&gt;Community Contributions&lt;/h3&gt;
&lt;p&gt;We had more contributions from the open source community in this release than ever before, including several completely new features. We would like to extend our sincere thanks to the community. Please check out the newly added &lt;a href=&quot;https://github.com/pytorch/audio/blob/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for ways to contribute code, and remember that reporting bugs and requesting features are just as valuable. We will continue posting well-scoped work items as issues labeled “help-wanted” and “contributions-welcome” for anyone who would like to contribute code, and are happy to coach new contributors through the contribution process.&lt;/p&gt;

&lt;p&gt;Find the full TorchAudio release notes &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;torchtext-090&quot;&gt;TorchText 0.9.0&lt;/h1&gt;
&lt;h3 id=&quot;beta-dataset-api-updates&quot;&gt;[Beta] Dataset API Updates&lt;/h3&gt;
&lt;p&gt;In this release, we are updating TorchText’s dataset API to be compatible with PyTorch data utilities, such as DataLoader, and are deprecating TorchText’s custom data abstractions such as &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;. The updated datasets are simple string-by-string iterators over the data. For guidance about migrating from the legacy abstractions to use modern PyTorch data utilities, please refer to our &lt;a href=&quot;https://github.com/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb&quot;&gt;migration guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The text datasets listed below have been updated as part of this work. For examples of how to use these datasets, please refer to our &lt;a href=&quot;https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;end-to-end text classification tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Language modeling:&lt;/strong&gt; WikiText2, WikiText103, PennTreebank, EnWik9&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Text classification:&lt;/strong&gt; AG_NEWS, SogouNews, DBpedia, YelpReviewPolarity, YelpReviewFull, YahooAnswers, AmazonReviewPolarity, AmazonReviewFull, IMDB&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sequence tagging:&lt;/strong&gt; UDPOS, CoNLL2000Chunking&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Translation:&lt;/strong&gt; IWSLT2016, IWSLT2017&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Question answer:&lt;/strong&gt; SQuAD1, SQuAD2&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Find the full TorchText release notes &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;stable-torchcsprng-020&quot;&gt;[Stable] TorchCSPRNG 0.2.0&lt;/h1&gt;
&lt;p&gt;We &lt;a href=&quot;https://pytorch.org/blog/torchcsprng-release-blog/&quot;&gt;released TorchCSPRNG in August 2020&lt;/a&gt;, a PyTorch C++/CUDA extension that provides cryptographically secure pseudorandom number generators for PyTorch. Today, we are releasing the 0.2.0 version and designating the library as stable. This release includes a new API for encrypt/decrypt with AES128 ECB/CTR as well as CUDA 11 and Windows CUDA support.&lt;/p&gt;

&lt;p&gt;Find the full TorchCSPRNG release notes &lt;a href=&quot;https://github.com/pytorch/csprng/releases/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks for reading, and if you are excited about these updates and want to participate in the future of PyTorch, we encourage you to join the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;discussion forums&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch&quot;&gt;open GitHub issues&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Team PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we are announcing updates to a number of PyTorch libraries, alongside the PyTorch 1.8 release. The updates include new releases for the domain libraries including TorchVision, TorchText and TorchAudio as well as new version of TorchCSPRNG. These releases include a number of new features and improvements and, along with the PyTorch 1.8 release, provide a broad set of updates for the PyTorch community to build on and leverage.</summary></entry><entry><title type="html">The torch.fft module: Accelerated Fast Fourier Transforms with Autograd in PyTorch</title><link href="https://pytorch.org/blog/the-torch.fft-module-accelerated-fast-fourier-transforms-with-autograd-in-pyTorch/" rel="alternate" type="text/html" title="The torch.fft module: Accelerated Fast Fourier Transforms with Autograd in PyTorch" /><published>2021-03-03T00:00:00-08:00</published><updated>2021-03-03T00:00:00-08:00</updated><id>https://pytorch.org/blog/the-torch.fft-module-accelerated-fast-fourier-transforms-with-autograd-in-pyTorch</id><content type="html" xml:base="https://pytorch.org/blog/the-torch.fft-module-accelerated-fast-fourier-transforms-with-autograd-in-pyTorch/">&lt;p&gt;The Fast Fourier Transform (FFT) calculates the Discrete Fourier Transform in O(n log n) time. It is foundational to a wide variety of numerical algorithms and signal processing techniques since it makes working in signals’ “frequency domains” as tractable as working in their spatial or temporal domains.&lt;/p&gt;

&lt;p&gt;As part of PyTorch’s goal to support hardware-accelerated deep learning and scientific computing, we have invested in improving our FFT support, and with PyTorch 1.8, we are releasing the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module. This module implements the same functions as NumPy’s &lt;code class=&quot;highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module, but with support for accelerators, like GPUs, and autograd.&lt;/p&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting started&lt;/h2&gt;

&lt;p&gt;Getting started with the new &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module is easy whether you are familiar with NumPy’s &lt;code class=&quot;highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module or not. While complete documentation for each function in the module can be found &lt;a href=&quot;https://pytorch.org/docs/1.8.0/fft.html&quot;&gt;here&lt;/a&gt;, a breakdown of what it offers is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fft&lt;/code&gt;, which computes a complex FFT over a single dimension, and &lt;code class=&quot;highlighter-rouge&quot;&gt;ifft&lt;/code&gt;, its inverse&lt;/li&gt;
  &lt;li&gt;the more general &lt;code class=&quot;highlighter-rouge&quot;&gt;fftn&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ifftn&lt;/code&gt;, which support multiple dimensions&lt;/li&gt;
  &lt;li&gt;The “real” FFT functions, &lt;code class=&quot;highlighter-rouge&quot;&gt;rfft&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;irfft&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;rfftn&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;irfftn&lt;/code&gt;,  designed to work with signals that are real-valued in their time domains&lt;/li&gt;
  &lt;li&gt;The “Hermitian” FFT functions, &lt;code class=&quot;highlighter-rouge&quot;&gt;hfft&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ihfft&lt;/code&gt;, designed to work with signals that are real-valued in their frequency domains&lt;/li&gt;
  &lt;li&gt;Helper functions, like &lt;code class=&quot;highlighter-rouge&quot;&gt;fftfreq&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;rfftfreq&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fftshift&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;ifftshift&lt;/code&gt;, that make it easier to manipulate signals&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We think these functions provide a straightforward interface for FFT functionality, as vetted by the NumPy community, although we are always interested in feedback and suggestions!&lt;/p&gt;

&lt;p&gt;To better illustrate how easy it is to move from NumPy’s &lt;code class=&quot;highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module to PyTorch’s &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module, let’s look at a NumPy implementation of a simple low-pass filter that removes high-frequency variance from a 2-dimensional image, a form of noise reduction or blurring:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.fft&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lowpass_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pass1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rfftfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pass2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fftfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pass2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pass1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;fft_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rfft2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;irfft2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now let’s see the same filter implemented in PyTorch:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.fft&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lowpass_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pass1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rfftfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pass2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fftfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pass2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pass1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;fft_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rfft2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;irfft2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Not only do current uses of NumPy’s &lt;code class=&quot;highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module translate directly to &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt;, the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; operations also support tensors on accelerators, like GPUs and autograd. This makes it possible to (among other things) develop new neural network modules using the FFT.&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module is not only easy to use — it is also fast! PyTorch natively supports Intel’s MKL-FFT library on Intel CPUs, and NVIDIA’s cuFFT library on CUDA devices, and we have carefully optimized how we use those libraries to maximize performance. While your own results will depend on your CPU and CUDA hardware, computing Fast Fourier Transforms on CUDA devices can be many times faster than computing it on the CPU, especially for larger signals.&lt;/p&gt;

&lt;p&gt;In the future, we may add support for additional math libraries to support more hardware. See below for where you can request additional hardware support.&lt;/p&gt;

&lt;h2 id=&quot;updating-from-older-pytorch-versions&quot;&gt;Updating from older PyTorch versions&lt;/h2&gt;

&lt;p&gt;Some PyTorch users might know that older versions of PyTorch also offered FFT functionality with the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fft()&lt;/code&gt; function. Unfortunately, this function had to be removed because its name conflicted with the new module’s name, and we think the new functionality is the best way to use the Fast Fourier Transform in PyTorch. In particular, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fft()&lt;/code&gt; was developed before PyTorch supported complex tensors, while the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.fft&lt;/code&gt; module was designed to work with them.&lt;/p&gt;

&lt;p&gt;PyTorch also has a “Short Time Fourier Transform”, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.stft&lt;/code&gt;, and its inverse &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.istft&lt;/code&gt;. These functions are being kept but updated to support complex tensors.&lt;/p&gt;

&lt;h2 id=&quot;future&quot;&gt;Future&lt;/h2&gt;

&lt;p&gt;As mentioned, PyTorch 1.8 offers the torch.fft module, which makes it easy to use the Fast Fourier Transform (FFT) on accelerators and with support for autograd. We encourage you to try it out!&lt;/p&gt;

&lt;p&gt;While this module has been modeled after NumPy’s &lt;code class=&quot;highlighter-rouge&quot;&gt;np.fft&lt;/code&gt; module so far, we are not stopping there. We are eager to hear from you, our community, on what FFT-related functionality you need, and we encourage you to create posts on our forums at &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;https://discuss.pytorch.org/&lt;/a&gt;, or &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/new?assignees=&amp;amp;labels=&amp;amp;template=feature-request.md&quot;&gt;file issues on our Github&lt;/a&gt; with your feedback and requests. Early adopters have already started asking about Discrete Cosine Transforms and support for more hardware platforms, for example, and we are investigating those features now.&lt;/p&gt;

&lt;p&gt;We look forward to hearing from you and seeing what the community does with PyTorch’s new FFT functionality!&lt;/p&gt;</content><author><name>Mike Ruberry, Peter Bell, and Joe Spisak</name></author><summary type="html">The Fast Fourier Transform (FFT) calculates the Discrete Fourier Transform in O(n log n) time. It is foundational to a wide variety of numerical algorithms and signal processing techniques since it makes working in signals’ “frequency domains” as tractable as working in their spatial or temporal domains.</summary></entry><entry><title type="html">Prototype Features Now Available - APIs for Hardware Accelerated Mobile and ARM64 Builds</title><link href="https://pytorch.org/blog/prototype-features-now-available-apis-for-hardware-accelerated-mobile-and-arm64-builds/" rel="alternate" type="text/html" title="Prototype Features Now Available - APIs for Hardware Accelerated Mobile and ARM64 Builds" /><published>2020-11-12T00:00:00-08:00</published><updated>2020-11-12T00:00:00-08:00</updated><id>https://pytorch.org/blog/prototype-features-now-available-apis-for-hardware-accelerated-mobile-and-arm64-builds</id><content type="html" xml:base="https://pytorch.org/blog/prototype-features-now-available-apis-for-hardware-accelerated-mobile-and-arm64-builds/">&lt;p&gt;Today, we are announcing four PyTorch prototype features. The first three of these will enable Mobile machine-learning developers to execute models on the full set of hardware (HW) engines making up a system-on-chip (SOC). This gives developers options to optimize their model execution for unique performance, power, and system-level concurrency.&lt;/p&gt;

&lt;p&gt;These features include enabling execution on the following on-device HW engines:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;DSP and NPUs using the Android Neural Networks API (NNAPI), developed in collaboration with Google&lt;/li&gt;
  &lt;li&gt;GPU execution on Android via Vulkan&lt;/li&gt;
  &lt;li&gt;GPU execution on iOS via Metal&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This release also includes developer efficiency benefits with newly introduced support for ARM64 builds for Linux.&lt;/p&gt;

&lt;p&gt;Below, you’ll find brief descriptions of each feature with the links to get you started. These features are available through our &lt;a href=&quot;https://pytorch.org/&quot;&gt;nightly builds&lt;/a&gt;. Reach out to us on the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;PyTorch Forums&lt;/a&gt; for any comment or feedback. We would love to get your feedback on those and hear how you are using them!&lt;/p&gt;

&lt;h2 id=&quot;nnapi-support-with-google-android&quot;&gt;NNAPI Support with Google Android&lt;/h2&gt;

&lt;p&gt;The Google Android and PyTorch teams collaborated to enable support for Android’s Neural Networks API (NNAPI) via PyTorch Mobile. Developers can now unlock high-performance execution on Android phones as their machine-learning models will be able to access additional hardware blocks on the phone’s system-on-chip. NNAPI allows Android apps to run computationally intensive neural networks on the most powerful and efficient parts of the chips that power mobile phones, including DSPs (Digital Signal Processors) and NPUs (specialized Neural Processing Units). The API was introduced in Android 8 (Oreo) and significantly expanded in Android 10 and 11 to support a richer set of AI models. With this integration, developers can now seamlessly access NNAPI directly from PyTorch Mobile. This initial release includes fully-functional support for a core set of features and operators, and Google and Facebook will be working to expand capabilities in the coming months.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://android-developers.googleblog.com/2020/11/android-neural-networks-api-13.html&quot;&gt;Android Blog: Android Neural Networks API 1.3 and PyTorch Mobile support&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://bit.ly/android-nnapi-pytorch-mobile-announcement&quot;&gt;PyTorch Medium Blog: Support for Android NNAPI with PyTorch Mobile&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pytorch-mobile-gpu-support&quot;&gt;PyTorch Mobile GPU support&lt;/h2&gt;

&lt;p&gt;Inferencing on GPU can provide great performance on many models types, especially those utilizing high-precision floating-point math. Leveraging the GPU for ML model execution as those found in SOCs from Qualcomm, Mediatek, and Apple allows for CPU-offload, freeing up the Mobile CPU for non-ML use cases. This initial prototype level support provided for on device GPUs is via the Metal API specification for iOS, and the Vulkan API specification for Android. As this feature is in an early stage: performance is not optimized and model coverage is limited. We expect this to improve significantly over the course of 2021 and would like to hear from you which models and devices you would like to see performance improvements on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/tutorials/tree/master/prototype_source&quot;&gt;Prototype source workflows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arm64-builds-for-linux&quot;&gt;ARM64 Builds for Linux&lt;/h2&gt;

&lt;p&gt;We will now provide prototype level PyTorch builds for ARM64 devices on Linux. As we see more ARM usage in our community with platforms such as Raspberry Pis and Graviton(2) instances spanning both at the edge and on servers respectively. This feature is available through our &lt;a href=&quot;https://pytorch.org/&quot;&gt;nightly builds&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We value your feedback on these features and look forward to collaborating with you to continuously improve them further!&lt;/p&gt;

&lt;p&gt;Thank you,&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we are announcing four PyTorch prototype features. The first three of these will enable Mobile machine-learning developers to execute models on the full set of hardware (HW) engines making up a system-on-chip (SOC). This gives developers options to optimize their model execution for unique performance, power, and system-level concurrency.</summary></entry><entry><title type="html">Announcing PyTorch Developer Day 2020</title><link href="https://pytorch.org/blog/pytorch-developer-day-2020/" rel="alternate" type="text/html" title="Announcing PyTorch Developer Day 2020" /><published>2020-11-01T00:00:00-07:00</published><updated>2020-11-01T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-developer-day-2020</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-developer-day-2020/">&lt;p&gt;Starting this year, we plan to host two separate events for PyTorch: one for developers and users to discuss core technical development, ideas and roadmaps called &lt;strong&gt;“Developer Day”&lt;/strong&gt;, and another for the PyTorch ecosystem and industry communities to showcase their work and discover opportunities to collaborate called &lt;strong&gt;“Ecosystem Day”&lt;/strong&gt; (scheduled for early 2021).&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/PTD2-social-asset.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;PyTorch Developer Day&lt;/strong&gt; (#PTD2) is kicking off on November 12, 2020, 8AM PST with a full day of technical talks on a variety of topics, including updates to the core framework, new tools and libraries to support development across a variety of domains. You’ll also see talks covering the latest research around systems and tooling in ML.&lt;/p&gt;

&lt;p&gt;For Developer Day, we have an online networking event limited to people composed of PyTorch maintainers and contributors, long-time stakeholders and experts in areas relevant to PyTorch’s future. Conversations from the networking event will strongly shape the future of PyTorch. Hence, invitations are required to attend the networking event.&lt;/p&gt;

&lt;p&gt;All talks will be livestreamed and available to the public.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.facebook.com/events/802177440559164/&quot;&gt;Livestream event page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorchdeveloperday.fbreg.com/apply&quot;&gt;Apply for an invitation to the networking event&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Visit the &lt;a href=&quot;https://pytorchdeveloperday.fbreg.com/&quot;&gt;event website&lt;/a&gt; to learn more. We look forward to welcoming you to PyTorch Developer Day on November 12th!&lt;/p&gt;

&lt;p&gt;Thank you,&lt;/p&gt;

&lt;p&gt;The PyTorch team&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Starting this year, we plan to host two separate events for PyTorch: one for developers and users to discuss core technical development, ideas and roadmaps called “Developer Day”, and another for the PyTorch ecosystem and industry communities to showcase their work and discover opportunities to collaborate called “Ecosystem Day” (scheduled for early 2021).</summary></entry><entry><title type="html">Adding a Contributor License Agreement for PyTorch</title><link href="https://pytorch.org/blog/a-contributor-license-agreement-for-pytorch/" rel="alternate" type="text/html" title="Adding a Contributor License Agreement for PyTorch" /><published>2020-10-28T00:00:00-07:00</published><updated>2020-10-28T00:00:00-07:00</updated><id>https://pytorch.org/blog/a-contributor-license-agreement-for-pytorch</id><content type="html" xml:base="https://pytorch.org/blog/a-contributor-license-agreement-for-pytorch/">&lt;p&gt;To ensure the ongoing growth and success of the framework, we’re introducing the use of the Apache Contributor License Agreement (CLA) for PyTorch. We care deeply about the broad community of contributors who make PyTorch such a great framework, so we want to take a moment to explain why we are adding a CLA.&lt;/p&gt;

&lt;h4 id=&quot;why-does-pytorch-need-a-cla&quot;&gt;Why Does PyTorch Need a CLA?&lt;/h4&gt;

&lt;p&gt;CLAs help clarify that users and maintainers have the relevant rights to use and maintain code contributed to an open source project, while allowing contributors to retain ownership rights to their code.&lt;/p&gt;

&lt;p&gt;PyTorch has grown from a small group of enthusiasts to a now global community with over 1,600 contributors from dozens of countries, each bringing their own diverse perspectives, values and approaches to collaboration. Looking forward, clarity about how this collaboration is happening is an important milestone for the framework as we continue to build a stronger, safer and more scalable community around PyTorch.&lt;/p&gt;

&lt;p&gt;The text of the Apache CLA can be found &lt;a href=&quot;https://www.apache.org/licenses/contributor-agreements.html&quot;&gt;here&lt;/a&gt;, together with an accompanying &lt;a href=&quot;https://www.apache.org/licenses/cla-faq.html&quot;&gt;FAQ&lt;/a&gt;. The language in the PyTorch CLA is identical to the Apache template. Although CLAs have been the subject of significant discussion in the open source community, we are seeing that using a CLA, and particularly the Apache CLA, is now standard practice when projects and communities reach a certain scale. Popular projects that have adopted some type of CLA include: Visual Studio Code, Flutter, TensorFlow, kubernetes, Ubuntu, Django, Python, Go, Android and many others.&lt;/p&gt;

&lt;h4 id=&quot;what-is-not-changing&quot;&gt;What is Not Changing&lt;/h4&gt;

&lt;p&gt;PyTorch’s BSD license is &lt;strong&gt;not&lt;/strong&gt; changing. There is no impact to PyTorch users. CLAs will only be required for new contributions to the project. For past contributions, no action is necessary. Everything else stays the same, whether it’s IP ownership, workflows, contributor roles or anything else that you’ve come to expect from PyTorch.&lt;/p&gt;

&lt;h4 id=&quot;how-the-new-cla-will-work&quot;&gt;How the New CLA will Work&lt;/h4&gt;

&lt;p&gt;Moving forward, all contributors to projects under the PyTorch GitHub organization will need to sign a CLA to merge their contributions.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/clacheck.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If you’ve contributed to other Facebook Open Source projects, you may have already signed the CLA, and no action is required. If you have not signed the CLA, a GitHub check will prompt you to sign it before your pull requests can be merged. You can reach the CLA from this &lt;a href=&quot;https://code.facebook.com/cla&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/clafb.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If you’re contributing as an individual, meaning the code is not something you worked on as part of your job, you should sign the individual contributor agreement. This agreement associates your GitHub username with future contributions and only needs to be signed once.&lt;/p&gt;

&lt;p&gt;If you’re contributing as part of your employment, you may need to sign the &lt;a href=&quot;https://code.facebook.com/cla/corporate&quot;&gt;corporate contributor agreement&lt;/a&gt;. Check with your legal team on filling this out. Also you will include a list of github ids from your company.&lt;/p&gt;

&lt;p&gt;As always, we continue to be humbled and grateful for all your support, and we look forward to scaling PyTorch together to even greater heights in the years to come.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">To ensure the ongoing growth and success of the framework, we’re introducing the use of the Apache Contributor License Agreement (CLA) for PyTorch. We care deeply about the broad community of contributors who make PyTorch such a great framework, so we want to take a moment to explain why we are adding a CLA.</summary></entry><entry><title type="html">PyTorch 1.7 released w/ CUDA 11, New APIs for FFTs, Windows support for Distributed training and more</title><link href="https://pytorch.org/blog/pytorch-1.7-released/" rel="alternate" type="text/html" title="PyTorch 1.7 released w/ CUDA 11, New APIs for FFTs, Windows support for Distributed training and more" /><published>2020-10-27T00:00:00-07:00</published><updated>2020-10-27T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.7-released</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.7-released/">&lt;p&gt;Today, we’re announcing the availability of PyTorch 1.7, along with updated domain libraries. The PyTorch 1.7 release includes a number of new APIs including support for NumPy-Compatible FFT operations, profiling tools and major updates to both distributed data parallel (DDP) and remote procedure call (RPC) based distributed training. In addition, several features moved to &lt;a href=&quot;https://pytorch.org/docs/stable/index.html#pytorch-documentation&quot;&gt;stable&lt;/a&gt; including custom C++ Classes, the memory profiler, extensions via custom tensor-like objects, user async functions in RPC and a number of other features in torch.distributed such as Per-RPC timeout, DDP dynamic bucketing and RRef helper.&lt;/p&gt;

&lt;p&gt;A few of the highlights include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CUDA 11 is now officially supported with binaries available at &lt;a href=&quot;http://pytorch.org/&quot;&gt;PyTorch.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Updates and additions to profiling and performance for RPC, TorchScript and Stack traces in the autograd profiler&lt;/li&gt;
  &lt;li&gt;(Beta) Support for NumPy compatible Fast Fourier transforms (FFT) via torch.fft&lt;/li&gt;
  &lt;li&gt;(Prototype) Support for Nvidia A100 generation GPUs and native TF32 format&lt;/li&gt;
  &lt;li&gt;(Prototype) Distributed training on Windows now supported&lt;/li&gt;
  &lt;li&gt;torchvision
    &lt;ul&gt;
      &lt;li&gt;(Stable) Transforms now support Tensor inputs, batch computation, GPU, and TorchScript&lt;/li&gt;
      &lt;li&gt;(Stable) Native image I/O for JPEG and PNG formats&lt;/li&gt;
      &lt;li&gt;(Beta) New Video Reader API&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;torchaudio
    &lt;ul&gt;
      &lt;li&gt;(Stable) Added support for speech rec (wav2letter), text to speech (WaveRNN) and source separation (ConvTasNet)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To reiterate, starting PyTorch 1.6, features are now classified as stable, beta and prototype. You can see the detailed announcement &lt;a href=&quot;https://pytorch.org/blog/pytorch-feature-classification-changes/&quot;&gt;here&lt;/a&gt;. Note that the prototype features listed in this blog are available as part of this release.&lt;/p&gt;

&lt;p&gt;Find the full release notes &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;front-end-apis&quot;&gt;Front End APIs&lt;/h1&gt;
&lt;h2 id=&quot;beta-numpy-compatible-torchfft-module&quot;&gt;[Beta] NumPy Compatible torch.fft module&lt;/h2&gt;
&lt;p&gt;FFT-related functionality is commonly used in a variety of scientific fields like signal processing. While PyTorch has historically supported a few FFT-related functions, the 1.7 release adds a new torch.fft module that implements FFT-related functions with the same API as NumPy.&lt;/p&gt;

&lt;p&gt;This new module must be imported to be used in the 1.7 release, since its name conflicts with the historic (and now deprecated) torch.fft function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example usage:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.fft&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;7.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;12.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;16.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;8.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;8.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/fft.html#torch-fft&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-c-support-for-transformer-nn-modules&quot;&gt;[Beta] C++ Support for Transformer NN Modules&lt;/h2&gt;
&lt;p&gt;Since &lt;a href=&quot;https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis/&quot;&gt;PyTorch 1.5&lt;/a&gt;, we’ve continued to maintain parity between the python and C++ frontend APIs. This update allows developers to use the nn.transformer module abstraction from the C++ Frontend. And moreover, developers no longer need to save a module from python/JIT and load into C++ as it can now be used it in C++ directly.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/cppdocs/api/classtorch_1_1nn_1_1_transformer_impl.html#_CPPv4N5torch2nn15TransformerImplE&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-torchset_deterministic&quot;&gt;[Beta] torch.set_deterministic&lt;/h2&gt;
&lt;p&gt;Reproducibility (bit-for-bit determinism) may help identify errors when debugging or testing a program. To facilitate reproducibility, PyTorch 1.7 adds the  &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.set_deterministic(bool)&lt;/code&gt; function that can direct PyTorch operators to select deterministic algorithms when available, and to throw a runtime error if an operation may result in nondeterministic behavior. By default, the flag this function controls is false and there is no change in behavior, meaning PyTorch may implement its operations nondeterministically by default.&lt;/p&gt;

&lt;p&gt;More precisely, when this flag is true:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Operations known to not have a deterministic implementation throw a runtime error;&lt;/li&gt;
  &lt;li&gt;Operations with deterministic variants use those variants (usually with a performance penalty versus the non-deterministic version); and&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.backends.cudnn.deterministic = True&lt;/code&gt; is set.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that this is necessary, &lt;strong&gt;but not sufficient&lt;/strong&gt;, for determinism &lt;strong&gt;within a single run of a PyTorch program&lt;/strong&gt;. Other sources of randomness like random number generators, unknown operations, or asynchronous or distributed computation may still cause nondeterministic behavior.&lt;/p&gt;

&lt;p&gt;See the documentation for &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.set_deterministic(bool)&lt;/code&gt; for the list of affected operations.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/15359&quot;&gt;RFC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.set_deterministic.html&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;performance--profiling&quot;&gt;Performance &amp;amp; Profiling&lt;/h1&gt;
&lt;h2 id=&quot;beta-stack-traces-added-to-profiler&quot;&gt;[Beta] Stack traces added to profiler&lt;/h2&gt;
&lt;p&gt;Users can now see not only operator name/inputs in the profiler output table but also where the operator is in the code. The workflow requires very little change to take advantage of this capability. The user uses the &lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#profiler&quot;&gt;autograd profiler&lt;/a&gt; as before but with optional new parameters: &lt;code class=&quot;highlighter-rouge&quot;&gt;with_stack&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;group_by_stack_n&lt;/code&gt;. Caution: regular profiling runs should not use this feature as it adds significant overhead.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/pull/43898/&quot;&gt;Detail&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;distributed-training--rpc&quot;&gt;Distributed Training &amp;amp; RPC&lt;/h1&gt;
&lt;h2 id=&quot;stable-torchelastic-now-bundled-into-pytorch-docker-image&quot;&gt;[Stable] TorchElastic now bundled into PyTorch docker image&lt;/h2&gt;
&lt;p&gt;Torchelastic offers a strict superset of the current &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.distributed.launch&lt;/code&gt; CLI with the added features for fault-tolerance and elasticity. If the user is not be interested in fault-tolerance, they can get the exact functionality/behavior parity by setting &lt;code class=&quot;highlighter-rouge&quot;&gt;max_restarts=0&lt;/code&gt; with the added convenience of auto-assigned &lt;code class=&quot;highlighter-rouge&quot;&gt;RANK&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;MASTER_ADDR|PORT&lt;/code&gt; (versus manually specified in &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.distributed.launch)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;By bundling &lt;code class=&quot;highlighter-rouge&quot;&gt;torchelastic&lt;/code&gt; in the same docker image as PyTorch, users can start experimenting with TorchElastic right-away without having to separately install &lt;code class=&quot;highlighter-rouge&quot;&gt;torchelastic&lt;/code&gt;. In addition to convenience, this work is a nice-to-have when adding support for elastic parameters in the existing Kubeflow’s distributed PyTorch operators.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/elastic/0.2.0/examples.html&quot;&gt;Usage examples and how to get started&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-support-for-uneven-dataset-inputs-in-ddp&quot;&gt;[Beta] Support for uneven dataset inputs in DDP&lt;/h2&gt;
&lt;p&gt;PyTorch 1.7 introduces a new context manager to be used in conjunction with models trained using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt; to enable training with uneven dataset size across different processes. This feature enables greater flexibility when using DDP and prevents the user from having to manually ensure dataset sizes are the same across different process. With this context manager, DDP will handle uneven dataset sizes automatically, which can prevent errors or hangs at the end of training.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/38174&quot;&gt;RFC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-nccl-reliability---async-errortimeout-handling&quot;&gt;[Beta] NCCL Reliability - Async Error/Timeout Handling&lt;/h2&gt;
&lt;p&gt;In the past, NCCL training runs would hang indefinitely due to stuck collectives, leading to a very unpleasant experience for users. This feature will abort stuck collectives and throw an exception/crash the process if a potential hang is detected. When used with something like torchelastic (which can recover the training process from the last checkpoint), users can have much greater reliability for distributed training. This feature is completely opt-in and sits behind an environment variable that needs to be explicitly set in order to enable this functionality (otherwise users will see the same behavior as before).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/46874&quot;&gt;RFC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-torchscript-rpc_remote-and-rpc_sync&quot;&gt;[Beta] TorchScript &lt;code class=&quot;highlighter-rouge&quot;&gt;rpc_remote&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;rpc_sync&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.distributed.rpc.rpc_async&lt;/code&gt; has been available in TorchScript in prior releases. For PyTorch 1.7, this functionality will be extended the remaining two core RPC APIs, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.distributed.rpc.rpc_sync&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.distributed.rpc.remote&lt;/code&gt;. This will complete the major RPC APIs targeted for support in TorchScript, it allows users to use the existing python RPC APIs within TorchScript (in a script function or script method, which releases the python Global Interpreter Lock) and could possibly improve application performance in multithreaded environment.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html#rpc&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/58ed60c259834e324e86f3e3118e4fcbbfea8dd1/torch/testing/_internal/distributed/rpc/jit/rpc_test.py#L505-L525&quot;&gt;Usage examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-distributed-optimizer-with-torchscript-support&quot;&gt;[Beta] Distributed optimizer with TorchScript support&lt;/h2&gt;
&lt;p&gt;PyTorch provides a broad set of optimizers for training algorithms, and these have been used repeatedly as part of the python API. However, users often want to use multithreaded training instead of multiprocess training as it provides better resource utilization and efficiency in the context of large scale distributed training (e.g. Distributed Model Parallel) or any RPC-based training application). Users couldn’t do this with with distributed optimizer before because we need to get rid of the python Global Interpreter Lock (GIL) limitation to achieve this.&lt;/p&gt;

&lt;p&gt;In PyTorch 1.7, we are enabling the TorchScript support in distributed optimizer to remove the GIL, and make it possible to run optimizer in multithreaded applications. The new distributed optimizer has the exact same interface as before but it automatically converts optimizers within each worker into TorchScript to make each GIL free. This is done by leveraging a functional optimizer concept and allowing the distributed optimizer to convert the computational portion of the optimizer into TorchScript. This will help use cases like distributed model parallel training and improve performance using multithreading.&lt;/p&gt;

&lt;p&gt;Currently, the only optimizer that supports automatic conversion with TorchScript is &lt;code class=&quot;highlighter-rouge&quot;&gt;Adagrad&lt;/code&gt; and all other optimizers will still work as before without TorchScript support. We are working on expanding the coverage to all PyTorch optimizers and expect more to come in future releases. The usage to enable TorchScript support is automatic and exactly the same with existing python APIs, here is an example of how to use this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributed.autograd&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist_autograd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributed.rpc&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributed.optim&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DistributedOptimizer&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist_autograd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Forward pass.&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rref1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;worker1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rref2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;worker1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rref1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_here&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rref2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_here&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

  &lt;span class=&quot;c&quot;&gt;# Backward pass.&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dist_autograd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;

  &lt;span class=&quot;c&quot;&gt;# Optimizer, pass in optim.Adagrad, DistributedOptimizer will&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# automatically convert/compile it to TorchScript (GIL-free)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dist_optim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DistributedOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adagrad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rref1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rref2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dist_optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/46883&quot;&gt;RFC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.optim&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-enhancements-to-rpc-based-profiling&quot;&gt;[Beta] Enhancements to RPC-based Profiling&lt;/h2&gt;
&lt;p&gt;Support for using the PyTorch profiler in conjunction with the RPC framework was first introduced in PyTorch 1.6. In PyTorch 1.7, the following enhancements have been made:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Implemented better support for profiling TorchScript functions over RPC&lt;/li&gt;
  &lt;li&gt;Achieved parity in terms of profiler features that work with RPC&lt;/li&gt;
  &lt;li&gt;Added support for asynchronous RPC functions on the server-side (functions decorated with &lt;code class=&quot;highlighter-rouge&quot;&gt;rpc.functions.async_execution)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Users are now able to use familiar profiling tools such as with &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.autograd.profiler.profile()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;with torch.autograd.profiler.record_function&lt;/code&gt;, and this works transparently with the RPC framework with full feature support, profiles asynchronous functions, and TorchScript functions.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/39675&quot;&gt;Design doc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/distributed_rpc_profiling.html&quot;&gt;Usage examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prototype-windows-support-for-distributed-training&quot;&gt;[Prototype] Windows support for Distributed Training&lt;/h2&gt;
&lt;p&gt;PyTorch 1.7 brings prototype support for &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt; and collective communications on the Windows platform. In this release, the support only covers Gloo-based &lt;code class=&quot;highlighter-rouge&quot;&gt;ProcessGroup&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;FileStore&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To use this feature across multiple machines, please provide a file from a shared file system in &lt;code class=&quot;highlighter-rouge&quot;&gt;init_process_group&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# initialize the process group&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_process_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;gloo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# multi-machine example:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# init_method = &quot;file://////{machine}/{share_folder}/file&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;init_method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;file:///{your local file path}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DistributedDataParallel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/42095&quot;&gt;Design doc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/master/distributed.html#backends-that-come-with-pytorch&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Acknowledgement (&lt;a href=&quot;https://github.com/gunandrose4u&quot;&gt;gunandrose4u&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;mobile&quot;&gt;Mobile&lt;/h1&gt;
&lt;p&gt;PyTorch Mobile supports both &lt;a href=&quot;https://pytorch.org/mobile/ios&quot;&gt;iOS&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/mobile/android/&quot;&gt;Android&lt;/a&gt; with binary packages available in &lt;a href=&quot;https://cocoapods.org/&quot;&gt;Cocoapods&lt;/a&gt; and &lt;a href=&quot;https://mvnrepository.com/repos/jcenter&quot;&gt;JCenter&lt;/a&gt; respectively. You can learn more about PyTorch Mobile &lt;a href=&quot;https://pytorch.org/mobile/home/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-pytorch-mobile-caching-allocator-for-performance-improvements&quot;&gt;[Beta] PyTorch Mobile Caching allocator for performance improvements&lt;/h2&gt;
&lt;p&gt;On some mobile platforms, such as Pixel, we observed that memory is returned to the system more aggressively. This results in frequent page faults as PyTorch being a functional framework does not maintain state for the operators. Thus outputs are allocated dynamically on each execution of the op, for the most ops. To ameliorate performance penalties due to this, PyTorch 1.7 provides a simple caching allocator for CPU. The allocator caches allocations by tensor sizes and, is currently, available only via the PyTorch C++ API. The caching allocator itself is owned by client and thus the lifetime of the allocator is also maintained by client code. Such a client owned caching allocator can then be used with scoped guard, &lt;code class=&quot;highlighter-rouge&quot;&gt;c10::WithCPUCachingAllocatorGuard&lt;/code&gt;, to enable the use of cached allocation within that scope.
&lt;strong&gt;Example usage:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#include &amp;lt;c10/mobile/CPUCachingAllocator.h&amp;gt;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;.....&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CPUCachingAllocator&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caching_allocator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Owned&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Can&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;be&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;member&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;some&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;so&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tie&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lifetime&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caching&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;allocator&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;.....&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;c10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optional&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithCPUCachingAllocatorGuard&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caching_allocator_guard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FLAGS_use_caching_allocator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;caching_allocator_guard&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emplace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;caching_allocator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;....&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Caching allocator is only available on mobile builds, thus the use of caching allocator outside of mobile builds won’t be effective.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/c10/mobile/CPUCachingAllocator.h#L13-L43&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/binaries/speed_benchmark_torch.cc#L207&quot;&gt;Usage examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;torchvision&quot;&gt;torchvision&lt;/h1&gt;
&lt;h2 id=&quot;stable-transforms-now-support-tensor-inputs-batch-computation-gpu-and-torchscript&quot;&gt;[Stable] Transforms now support Tensor inputs, batch computation, GPU, and TorchScript&lt;/h2&gt;
&lt;p&gt;torchvision transforms are now inherited from &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt; and can be torchscripted and applied on torch Tensor inputs as well as on PIL images. They also support Tensors with batch dimensions and work seamlessly on CPU/GPU devices:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.transforms&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# to fix random seed, use torch.manual_seed&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# instead of random.seed&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;manual_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomCrop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomHorizontalFlip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ConvertImageDtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.485&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.456&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.406&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.229&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.225&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scripted_transforms&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Note: we can similarly use T.Compose to define transforms&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# transforms = T.Compose([...]) and &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# scripted_transforms = torch.jit.script(torch.nn.Sequential(*transforms.transforms))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tensor_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uint8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# works directly on Tensors&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out_image1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# on the GPU&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out_image1_cuda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# with batches&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batched_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uint8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out_image_batched&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batched_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# and has torchscript support&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out_image2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scripted_transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;These improvements enable the following new features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;support for GPU acceleration&lt;/li&gt;
  &lt;li&gt;batched transformations e.g. as needed for videos&lt;/li&gt;
  &lt;li&gt;transform multi-band torch tensor images (with more than 3-4 channels)&lt;/li&gt;
  &lt;li&gt;torchscript transforms together with your model for deployment
&lt;strong&gt;Note:&lt;/strong&gt; Exceptions for TorchScript support includes &lt;code class=&quot;highlighter-rouge&quot;&gt;Compose&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;RandomChoice&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;RandomOrder&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Lambda&lt;/code&gt; and those applied on PIL images, such as &lt;code class=&quot;highlighter-rouge&quot;&gt;ToPILImage&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;stable-native-image-io-for-jpeg-and-png-formats&quot;&gt;[Stable] Native image IO for JPEG and PNG formats&lt;/h2&gt;
&lt;p&gt;torchvision 0.8.0 introduces native image reading and writing operations for JPEG and PNG formats. Those operators support TorchScript and return &lt;code class=&quot;highlighter-rouge&quot;&gt;CxHxW&lt;/code&gt; tensors in &lt;code class=&quot;highlighter-rouge&quot;&gt;uint8&lt;/code&gt; format, and can thus be now part of your model for deployment in C++ environments.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.io&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_image&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# tensor_image is a CxHxW uint8 Tensor&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'path_to_image.jpeg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# or equivalently&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.io&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decode_image&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# raw_data is a 1d uint8 Tensor with the raw bytes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;raw_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'path_to_image.jpeg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tensor_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decode_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# all operators are torchscriptable and can be&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# serialized together with your model torchscript code&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scripted_read_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;stable-retinanet-detection-model&quot;&gt;[Stable] RetinaNet detection model&lt;/h2&gt;
&lt;p&gt;This release adds pretrained models for RetinaNet with a ResNet50 backbone from &lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;&gt;Focal Loss for Dense Object Detection&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-new-video-reader-api&quot;&gt;[Beta] New Video Reader API&lt;/h2&gt;
&lt;p&gt;This release introduces a new video reading abstraction, which gives more fine-grained control of iteration over videos. It supports image and audio, and implements an iterator interface so that it is interoperable with other the python libraries such as itertools.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.io&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VideoReader&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# stream indicates if reading from audio or video&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VideoReader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'path_to_video.mp4'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'video'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# can change the stream after construction&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# via reader.set_current_stream&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# to read all frames in a video starting at 2 seconds&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seek&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# frame is a dict with &quot;data&quot; and &quot;pts&quot; metadata&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# because reader is an iterator you can combine it with&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# itertools&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;itertools&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;takewhile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;islice&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# read 10 frames starting from 2 seconds&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;islice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seek&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
    
&lt;span class=&quot;c&quot;&gt;# or to return all frames between 2 and 5 seconds&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;takewhile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In order to use the Video Reader API beta, you must compile torchvision from source and have ffmpeg installed in your system.&lt;/li&gt;
  &lt;li&gt;The VideoReader API is currently released as beta and its API may change following user feedback.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;torchaudio&quot;&gt;torchaudio&lt;/h1&gt;
&lt;p&gt;With this release, torchaudio is expanding its support for models and &lt;a href=&quot;https://github.com/pytorch/audio/tree/master/examples&quot;&gt;end-to-end applications&lt;/a&gt;, adding a wav2letter training pipeline and end-to-end text-to-speech and source separation pipelines. Please file an issue on &lt;a href=&quot;https://github.com/pytorch/audio/issues/new?template=questions-help-support.md&quot;&gt;github&lt;/a&gt; to provide feedback on them.&lt;/p&gt;

&lt;h2 id=&quot;stable-speech-recognition&quot;&gt;[Stable] Speech Recognition&lt;/h2&gt;
&lt;p&gt;Building on the addition of the wav2letter model for speech recognition in the last release, we’ve now added an &lt;a href=&quot;https://github.com/pytorch/audio/tree/master/examples/pipeline_wav2letter&quot;&gt;example wav2letter training pipeline&lt;/a&gt; with the LibriSpeech dataset.&lt;/p&gt;

&lt;h2 id=&quot;stable-text-to-speech&quot;&gt;[Stable] Text-to-speech&lt;/h2&gt;
&lt;p&gt;With the goal of supporting text-to-speech applications, we added a vocoder based on the WaveRNN model, based on the implementation from &lt;a href=&quot;https://github.com/fatchord/WaveRNN&quot;&gt;this repository&lt;/a&gt;. The original implementation was introduced in “Efficient Neural Audio Synthesis”. We also provide an &lt;a href=&quot;https://github.com/pytorch/audio/tree/master/examples/pipeline_wavernn&quot;&gt;example WaveRNN training pipeline&lt;/a&gt; that uses the LibriTTS dataset added to torchaudio in this release.&lt;/p&gt;

&lt;h2 id=&quot;stable-source-separation&quot;&gt;[Stable] Source Separation&lt;/h2&gt;
&lt;p&gt;With the addition of the ConvTasNet model, based on the paper “Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation,” torchaudio now also supports source separation. An &lt;a href=&quot;https://github.com/pytorch/audio/tree/master/examples/source_separation&quot;&gt;example ConvTasNet training pipeline&lt;/a&gt; is provided with the wsj-mix dataset.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we’re announcing the availability of PyTorch 1.7, along with updated domain libraries. The PyTorch 1.7 release includes a number of new APIs including support for NumPy-Compatible FFT operations, profiling tools and major updates to both distributed data parallel (DDP) and remote procedure call (RPC) based distributed training. In addition, several features moved to stable including custom C++ Classes, the memory profiler, extensions via custom tensor-like objects, user async functions in RPC and a number of other features in torch.distributed such as Per-RPC timeout, DDP dynamic bucketing and RRef helper.</summary></entry><entry><title type="html">Announcing the Winners of the 2020 Global PyTorch Summer Hackathon</title><link href="https://pytorch.org/blog/announcing-the-winners-of-the-2020-global-pytorch-summer-hackathon/" rel="alternate" type="text/html" title="Announcing the Winners of the 2020 Global PyTorch Summer Hackathon" /><published>2020-10-01T00:00:00-07:00</published><updated>2020-10-01T00:00:00-07:00</updated><id>https://pytorch.org/blog/announcing-the-winners-of-the-2020-global-pytorch-summer-hackathon</id><content type="html" xml:base="https://pytorch.org/blog/announcing-the-winners-of-the-2020-global-pytorch-summer-hackathon/">&lt;p&gt;More than 2,500 participants in this year’s Global PyTorch Summer Hackathon pushed the envelope to create unique new tools and applications for PyTorch developers and researchers.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/Summer_hackathon.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notice&lt;/strong&gt;: None of the projects submitted to the hackathon are associated with or offered by Facebook, Inc.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This year’s projects fell into three categories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PyTorch Developer Tools:&lt;/strong&gt; a tool or library for improving productivity and efficiency for PyTorch researchers and developers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Web/Mobile Applications Powered by PyTorch:&lt;/strong&gt; a web or mobile interface and/or an embedded device built using PyTorch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PyTorch Responsible AI Development Tools:&lt;/strong&gt; a tool, library, or web/mobile app to support researchers and developers in creating responsible AI that factors in fairness, security, privacy, and more throughout its entire development process.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The virtual hackathon ran from June 22 to August 25, with more than 2,500 registered participants, representing 114 countries from Republic of Azerbaijan, to Zimbabwe, to Japan, submitting a total of 106 projects. Entrants were judged on their idea’s quality, originality, potential impact, and how well they implemented it.&lt;/p&gt;

&lt;p&gt;Meet the winners of each category below.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-developer-tools&quot;&gt;PyTorch Developer Tools&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1st place&lt;/strong&gt; - &lt;a href=&quot;https://pytorch2020.devpost.com/review/submissions/MlVLNmlyc2JDREtGMmFua3FIZGNMUTZIelR3SHJOL2cwUi9vTXNnQ3F5ak5vZDhFZk1NU0hRQVFDSE9hTDA2V1BPb2VLRDFLN0lIY3Bva2RvK1hnOEltMnQ2aW9jVnpCemZ0c0o3bkNzeUU9LS1yWTNMdkR6blBJVlBJV2lnUkxHdStnPT0=--81e61a941d3b90d97a725a81a491458c838f25dd&quot;&gt;DeMask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;DeMask is an end-to-end model for enhancing speech while wearing face masks — offering a clear benefit during times when face masks are mandatory in many spaces and for workers who wear face masks on the job. Built with &lt;a href=&quot;https://github.com/mpariente/asteroid&quot;&gt;Asteroid&lt;/a&gt;, a PyTorch-based audio source separation toolkit, DeMask is trained to recognize distortions in speech created by the muffling from face masks and to adjust the speech to make it sound clearer.&lt;/p&gt;

&lt;p&gt;This submission stood out in particular because it represents both a high-quality idea and an implementation that can be reproduced by other researchers.&lt;/p&gt;

&lt;p&gt;Here is an example on how to train a speech separation model in less than 20 lines:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pytorch_lightning&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Trainer&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;asteroid&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ConvTasNet&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;asteroid.losses&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PITLossWrapper&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;asteroid.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LibriMix&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;asteroid.engine&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LibriMix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loaders_from_mini&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sep_clean'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ConvTasNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PITLossWrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# MSE&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pit_from&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pw_pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# Point in the pairwise matrix.&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;system&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fast_dev_run&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;2nd place&lt;/strong&gt; - &lt;a href=&quot;https://pytorch2020.devpost.com/review/submissions/bW12TG9QN0R4NjJ5SmJRMStXcGNzTjMxLytHYnV3b3QzUWkvZXpVRlZaOXhFY2dycGVFdU8ydUQxdW5GcTdoQ2lmS1I2Mko1UmpwblJhaGw5a2t6NGR4SG5DMkNkektOblB5ZFR5RG1lNmM9LS15dkJZellPcWlxNllNajlKdXU0b2xRPT0=--a3242fbd80399f90f56559bf4c7efff73e4bc50b&quot;&gt;carefree-learn&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A PyTorch-based automated machine learning (AutoML) solution, carefree-learn provides high-level APIs to make training models using tabular data sets simpler. It features an interface similar to &lt;a href=&quot;https://scikit-learn.org/stable/&quot;&gt;scikit-learn&lt;/a&gt; and functions as an end-to-end end pipeline for tabular data sets. It automatically detects feature column types and redundant feature columns, imputes missing values, encodes string columns and categorical columns, and preprocesses numerical columns, among other features.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3rd Place&lt;/strong&gt; - &lt;a href=&quot;https://pytorch2020.devpost.com/review/submissions/MVE2OXN4anlnUVZ2Tkphd2NSZXpuZkY1MjliUU8vNWFmNVdLNXBCWmczY2ZPZXVlWGlxOVBkTlNIYkFaMWhsWDUyendBUmE5bEF6emQ5bjZhTkJkMU5HV0R4em1sU29KdFpycDFVcjZYcXc9LS0vMFZ2MTNtV29lSVFJeE1hcUswRGhnPT0=--c5e184a5faa38ea5a57884765c4956dd5c9370a7&quot;&gt;TorchExpo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;TorchExpo is a collection of models and extensions that simplifies taking PyTorch from research to production in mobile devices. This library is more than a web and mobile application, and also comes with a Python library. The Python library is available via pip install and it helps researchers convert a state-of-the-art model in TorchScript and ONNX format in just one line. Detailed docs are available &lt;a href=&quot;https://torchexpo.readthedocs.io/en/latest/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;webmobile-applications-powered-by-pytorch&quot;&gt;Web/Mobile Applications Powered by PyTorch&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1st place&lt;/strong&gt; - &lt;a href=&quot;https://pytorch2020.devpost.com/review/submissions/a0Z3NHRLU1k1OHpkWnRSb0Z3TlhHQTFjQzJicUI5UTBlZDJlQW5DdTBMQjlVVjk0KytPelVVUzVsaDFjTXgvQjQyd2crdDRYNEU4Rm9oZE5Uam45TWI2UmQ2S051OWFkNGRqY0prNkVRMlE9LS1RbEFUWUlSS0hXaU1ZK0dNTXViYUlRPT0=--1a7fb49eaf16ee0fa1aa7dbffb7f2b0a6a465aca&quot;&gt;Q&amp;amp;Aid&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Q&amp;amp;Aid is a conceptual health-care chatbot aimed at making health-care diagnoses and facilitating communication between patients and doctors. It relies on a series of machine learning models to filter, label, and answer medical questions, based on a medical image and/or questions in text provided by a patient. The transcripts from the chat app then can be forwarded to the local hospitals and the patient will be contacted by one of them to make an appointment to determine proper diagnosis and care. The team hopes that this concept application helps hospitals to work with patients more efficiently and provide proper care.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/Q_AID_architecture.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;2nd place&lt;/strong&gt; - &lt;a href=&quot;https://pytorch2020.devpost.com/review/submissions/cjBvdzBPdVpPQUJZYWdVNXRDL2Y5RVc0YVJzemlqRUY1L0ppT0NCMERMNm9NSmtjbjNnSW1qZHEyMjJHSDNuc0c2bSt2R0NldVkrTk5MaXFwWW11OXdSSUhSUlYvS0Uwc25TWjJVNm5vTjQ9LS1XQ25BK2YvemR5bGcydXdvMEMxMFFnPT0=--9f1410c15a8b6a2a2518b7ccf55369af9bc89de1&quot;&gt;Rasoee&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Rasoee is an application that can take images as input and output the name of the dish. It also lists the ingredients and recipe, along with the link to the original recipe online. Additionally, users can choose a cuisine from the list of cuisines in the drop menu, and describe the taste and/or method of preparation in text. Then the application will return matching dishes from the &lt;a href=&quot;https://github.com/arijitgupta42/Rasoee/blob/master/Dishes.txt&quot;&gt;list of 308 identifiable dishes&lt;/a&gt;. The team has put a significant amount of effort gathering and cleaning various datasets to build more accurate and comprehensive models. You can check out the application &lt;a href=&quot;https://rasoee.herokuapp.com&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3rd place&lt;/strong&gt; - &lt;a href=&quot;https://pytorch2020.devpost.com/review/submissions/TG01NDFmRGFmRzBWdmVMQVB6cjhXSzA3NUpsb0taN3VmVVQ1TU9aUVpYWXF2ZnRoeU9jRFAvcnp2cG4ybmRSdUtHb083cUtOYkYySFVaeVVSV2pIamErSmUzYzRMV0tVS3g1OWdhZlREMms9LS1vK0pGaUdadmxkWGRSRUNBR1RXUTJ3PT0=--a52143d8b10a7c33a2016333c5ff9fe52bd7287a&quot;&gt;Rexana the Robot — PyTorch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Rexana is an AI voice assistant meant to lay the foundation for a physical robot that can complete basic tasks around the house. The system is capable of autonomous navigation (knowing its position around the house relative to landmarks), recognizing voice commands, and object detection and recognition — meaning it can be commanded to perform various household tasks (e.g., “Rexana, water the potted plant in the lounge room.”). Rexana can be controlled remotely via a mobile device, and the robot itself features customizable hands (magnets, grippers, etc.) for taking on different jobs.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-responsible-ai-development-tools&quot;&gt;PyTorch Responsible AI Development Tools&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1st place&lt;/strong&gt;: &lt;a href=&quot;https://pytorch2020.devpost.com/review/submissions/WERhTlpRZTg2ZTVFbG9nckxhWFAvc2FHRXBRalAwS3ZOeDVrRzBaeU9YdVhqYzVITFpad1ZpeFo2RE02a0tZNDY5Q0l5eGlEeFZid25Da1lLRUM5eHF2THdqUk5OdzlWeUdmYkhPU3dIOWs9LS1CWFpQajA2Y0pQZ3AxYXdzN2pwZ1RnPT0=--23f21d95c74ab20daf9a05b1e339095bf87d0bca&quot;&gt;FairTorch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;FairTorch is a fairness library for PyTorch. It lets developers add constraints to their models to equalize metrics across subgroups by simply adding a few lines of code. Model builders can choose a metric definition of fairness for their context, and enforce it at time of training. The library offers a suite of metrics that measure an AI system’s performance among subgroups, and can apply to high-stakes examples where decision-making algorithms are deployed, such as hiring, school admissions, and banking.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://www.youtube.com/watch?v=b2Cj4VflFKQ&quot;&gt;
     &lt;img src=&quot;https://yt-embed.herokuapp.com/embed?v=b2Cj4VflFKQ&quot; alt=&quot;FairTorch&quot; style=&quot;width:50%;&quot; /&gt;
      &lt;/a&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;strong&gt;2nd place&lt;/strong&gt;: &lt;a href=&quot;https://pytorch2020.devpost.com/review/submissions/SnJDYmw0cGt3RmN6UWVHUmhYSW5LK3RhdU40WGhhVGNrb3pBUExudTRQRndzWTh3bVRWVDF0c0FYaFdObkJqRnhnTjFUZUw5ekFJck5DZ0ozeUU4a0Z5dURhcVZqcGlLZVYyOXBhblNxQWc9LS02UmQ5T1Bxbk1JSjRod2E0ZzkwajN3PT0=--d87fbb0e849dbced6e42f03937def2e99ab23587&quot;&gt;Fluence&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Fluence is a PyTorch-based deep learning library for language research. It specifically addresses the large compute demands of natural language processing (NLP) research. Fluence aims to provide low-resource and computationally efficient algorithms for NLP, giving researchers algorithms that can enhance current NLP methods or help discover where current methods fall short.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3rd place&lt;/strong&gt;: &lt;a href=&quot;https://pytorch2020.devpost.com/review/submissions/N2F2eDFSMkt2b3Nzd0FvYU9HQmlQQStVWDA3RjhCeXVRL25ub2E3UXFFWHZpVTcwZ3BaWEIwY2pid2J6YmVabk1oYlhtbFNUdTNoZkVKT01QRWtnOTkxZmNYTkpMV1dndzZvZnpmVnlFTzg9LS1oSVVvUHlSTFA3dTJrUDJuYTQweVFnPT0=--c6a4027e353de3c122ccd68333e9c84a289e44e8&quot;&gt;Causing: CAUSal INterpretation using Graphs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Causing (CAUSal INterpretation using Graphs) is a multivariate graphic analysis tool for bringing transparency to neural networks. It explains causality and helps researchers and developers interpret the causal effects of a given equation system to ensure fairness. Developers can input data and a model describing the dependencies between the variables within the data set into Causing, and Causing will output a colored graph of quantified effects acting between the model’s variables. In addition, it also allows developers to estimate these effects to validate whether data fits a model.&lt;/p&gt;

&lt;p&gt;Thank you,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The PyTorch team&lt;/strong&gt;&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">More than 2,500 participants in this year’s Global PyTorch Summer Hackathon pushed the envelope to create unique new tools and applications for PyTorch developers and researchers.</summary></entry><entry><title type="html">PyTorch framework for cryptographically secure random number generation, torchcsprng, now available</title><link href="https://pytorch.org/blog/torchcsprng-release-blog/" rel="alternate" type="text/html" title="PyTorch framework for cryptographically secure random number generation, torchcsprng, now available" /><published>2020-08-24T00:00:00-07:00</published><updated>2020-08-24T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchcsprng-release-blog</id><content type="html" xml:base="https://pytorch.org/blog/torchcsprng-release-blog/">&lt;p&gt;One of the key components of modern cryptography is the pseudorandom number generator. Katz and Lindell stated, “The use of badly designed or inappropriate random number generators can often leave a good cryptosystem vulnerable to attack. Particular care must be taken to use a random number generator that is designed for cryptographic use, rather than a ‘general-purpose’ random number generator which may be fine for some applications but not ones that are required to be cryptographically secure.”[1] Additionally, most pseudorandom number generators scale poorly to massively parallel high-performance computation because of their sequential nature. Others don’t satisfy cryptographically secure properties.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/csprng&quot;&gt;torchcsprng&lt;/a&gt; is a PyTorch &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_extension.html&quot;&gt;C++/CUDA extension&lt;/a&gt; that provides &lt;a href=&quot;https://en.wikipedia.org/wiki/Cryptographically_secure_pseudorandom_number_generator&quot;&gt;cryptographically secure pseudorandom number generators&lt;/a&gt; for PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;torchcsprng-overview&quot;&gt;torchcsprng overview&lt;/h2&gt;

&lt;p&gt;Historically, PyTorch had only two pseudorandom number generator implementations: Mersenne Twister for CPU and Nvidia’s cuRAND Philox for CUDA. Despite good performance properties, neither of them are suitable for cryptographic applications. Over the course of the past several months, the PyTorch team developed the torchcsprng extension API. Based on PyTorch dispatch mechanism and operator registration, it allows the users to extend c10::GeneratorImpl and implement their own custom pseudorandom number generator.&lt;/p&gt;

&lt;p&gt;torchcsprng generates a random 128-bit key on the CPU using one of its generators and then runs AES128 in &lt;a href=&quot;https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#Counter_(CTR)&quot;&gt;CTR mode&lt;/a&gt; either on CPU or GPU using CUDA. This then generates a random 128-bit state and applies a transformation function to map it to target tensor values. This approach is based on &lt;a href=&quot;http://www.thesalmons.org/john/random123/papers/random123sc11.pdf&quot;&gt;Parallel Random Numbers: As Easy as 1, 2, 3 (John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, D. E. Shaw Research)&lt;/a&gt;. It makes torchcsprng both crypto-secure and parallel on both CPU and CUDA.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/torchcsprng.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Since torchcsprng is a PyTorch extension, it is available on the platforms where PyTorch is available (support for Windows-CUDA will be available in the coming months).&lt;/p&gt;

&lt;h2 id=&quot;using-torchcsprng&quot;&gt;Using torchcsprng&lt;/h2&gt;

&lt;p&gt;The torchcsprng API is very simple to use and is fully compatible with the PyTorch random infrastructure:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1: Install via binary distribution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Anaconda:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchcsprng&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;pip:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchcsprng&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 2: import packages as usual but add csprng&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchcsprng&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csprng&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 3: Create a cryptographically secure pseudorandom number generator from /dev/urandom:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;urandom_gen&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csprng&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_random_device_generator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/dev/urandom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and simply use it with the existing PyTorch methods:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cpu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urandom_gen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 4: Test with Cuda&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One of the advantages of torchcsprng generators is that they can be used with both CPU and CUDA tensors:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urandom_gen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Another advantage of torchcsprng generators is that they are parallel on CPU unlike the default PyTorch CPU generator.&lt;/p&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;The easiest way to get started with torchcsprng is by visiting the &lt;a href=&quot;https://github.com/pytorch/csprng&quot;&gt;GitHub page&lt;/a&gt; where you can find installation and build instructions, and more how-to examples.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;The PyTorch Team&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://www.amazon.com/Introduction-Modern-Cryptography-Principles-Protocols/dp/1584885513&quot;&gt;Introduction to Modern Cryptography: Principles and Protocols (Chapman &amp;amp; Hall/CRC Cryptography and Network Security Series)&lt;/a&gt; by Jonathan Katz and Yehuda Lindell&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">One of the key components of modern cryptography is the pseudorandom number generator. Katz and Lindell stated, “The use of badly designed or inappropriate random number generators can often leave a good cryptosystem vulnerable to attack. Particular care must be taken to use a random number generator that is designed for cryptographic use, rather than a ‘general-purpose’ random number generator which may be fine for some applications but not ones that are required to be cryptographically secure.”[1] Additionally, most pseudorandom number generators scale poorly to massively parallel high-performance computation because of their sequential nature. Others don’t satisfy cryptographically secure properties.</summary></entry><entry><title type="html">PyTorch 1.6 now includes Stochastic Weight Averaging</title><link href="https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/" rel="alternate" type="text/html" title="PyTorch 1.6 now includes Stochastic Weight Averaging" /><published>2020-08-18T00:00:00-07:00</published><updated>2020-08-18T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/">&lt;p&gt;Do you use stochastic gradient descent (SGD) or Adam? Regardless of the procedure you use to train your neural network, you can likely achieve significantly better generalization at virtually no additional cost with a simple new technique now natively supported in PyTorch 1.6, Stochastic Weight Averaging (SWA) [1]. Even if you have already trained your model, it’s easy to realize the benefits of SWA by running SWA for a small number of epochs starting with a pre-trained model. &lt;a href=&quot;https://twitter.com/MilesCranmer/status/1282140440892932096&quot;&gt;Again&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/leopd/status/1285969855062192129&quot;&gt;again&lt;/a&gt;, researchers are  discovering that SWA improves the performance of well-tuned models in a wide array of practical applications with little cost or effort!&lt;/p&gt;

&lt;p&gt;SWA has a wide range of applications and features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SWA significantly improves performance compared to standard training techniques in computer vision (e.g., VGG, ResNets, Wide ResNets and DenseNets on ImageNet and CIFAR benchmarks [1, 2]).&lt;/li&gt;
  &lt;li&gt;SWA provides state-of-the-art performance on key benchmarks in semi-supervised learning and domain adaptation [2].&lt;/li&gt;
  &lt;li&gt;SWA was shown to improve performance in language modeling (e.g., AWD-LSTM on WikiText-2 [4]) and policy-gradient methods in deep reinforcement learning [3].&lt;/li&gt;
  &lt;li&gt;SWAG, an extension of SWA, can approximate Bayesian model averaging in Bayesian deep learning and achieves state-of-the-art uncertainty calibration results in various settings. Moreover, its recent generalization MultiSWAG provides significant additional performance gains and mitigates double-descent [4, 10]. Another approach, Subspace Inference, approximates the Bayesian posterior in a small subspace of the parameter space around the SWA solution [5].&lt;/li&gt;
  &lt;li&gt;SWA for low precision training, SWALP, can match the performance of full-precision SGD training, even with all numbers quantized down to 8 bits, including gradient accumulators [6].&lt;/li&gt;
  &lt;li&gt;SWA in parallel, SWAP, was shown to greatly speed up the training of neural networks by using large batch sizes and, in particular, set a record by training a neural network to 94% accuracy on CIFAR-10 in 27 seconds [11].&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;. &lt;em&gt;Illustrations of SWA and SGD with a Preactivation ResNet-164 on CIFAR-100 [1]. &lt;strong&gt;Left&lt;/strong&gt;: test error surface for three FGE samples and the corresponding SWA solution (averaging in weight space). &lt;strong&gt;Middle&lt;/strong&gt; and &lt;strong&gt;Right&lt;/strong&gt;: test error and train loss surfaces showing the weights proposed by SGD (at convergence) and SWA, starting from the same initialization of SGD after 125 training epochs. Please see [1] for details on how these figures were constructed&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In short, SWA performs an equal average of the weights traversed by SGD (or any stochastic optimizer) with a modified learning rate schedule (see the left panel of Figure 1.). SWA solutions end up in the center of a wide flat region of loss, while SGD tends to converge to the boundary of the low-loss region, making it susceptible to the shift between train and test error surfaces (see the middle and right panels of Figure 1). We emphasize that SWA &lt;strong&gt;can be used with any optimizer, such as Adam, and is not specific to SGD&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Previously, SWA was in PyTorch contrib. In PyTorch 1.6, we provide a new convenient implementation of SWA in &lt;a href=&quot;https://pytorch.org/docs/stable/optim.html#stochastic-weight-averaging&quot;&gt;torch.optim.swa_utils&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;is-this-just-averaged-sgd&quot;&gt;Is this just Averaged SGD?&lt;/h2&gt;

&lt;p&gt;At a high level, averaging SGD iterates dates back several decades in convex optimization [7, 8], where it is sometimes referred to as Polyak-Ruppert averaging, or averaged SGD. &lt;strong&gt;But the details matter&lt;/strong&gt;. Averaged SGD is often used in conjunction with a decaying learning rate, and an exponential moving average (EMA), typically for convex optimization. In convex optimization, the focus has been on improved rates of convergence. In deep learning, this form of averaged SGD smooths the trajectory of SGD iterates but does not perform very differently.&lt;/p&gt;

&lt;p&gt;By contrast, SWA uses an &lt;strong&gt;equal average&lt;/strong&gt; of SGD iterates with a modified &lt;strong&gt;cyclical or high constant learning rate&lt;/strong&gt; and exploits the flatness of training objectives [8] specific to &lt;strong&gt;deep learning&lt;/strong&gt; for &lt;strong&gt;improved generalization&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;how-does-stochastic-weight-averaging-work&quot;&gt;How does Stochastic Weight Averaging Work?&lt;/h2&gt;

&lt;p&gt;There are two important ingredients that make SWA work. First, SWA uses a &lt;strong&gt;modified learning rate&lt;/strong&gt; schedule so that SGD (or other optimizers such as Adam) continues to bounce around the optimum and explore diverse models instead of simply converging to a single solution. For example, we can use the standard decaying learning rate strategy for the first 75% of training time and then set the learning rate to a reasonably high constant value for the remaining 25% of the time (see Figure 2 below). The second ingredient is to take an average of the weights &lt;strong&gt;(typically an equal average)&lt;/strong&gt; of the networks traversed by SGD. For example, we can maintain a running average of the weights obtained at the end of every epoch within the last 25% of training time (see Figure 2). After training is complete, we then set the weights of the network to the computed SWA averages.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/nswapytorch2.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;. &lt;em&gt;Illustration of the learning rate schedule adopted by SWA. Standard decaying schedule is used for the first 75% of the training and then a high constant value is used for the remaining 25%. The SWA averages are formed during the last 25% of training&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;One important detail is the batch normalization. Batch normalization layers compute running statistics of activations during training. Note that the SWA averages of the weights are never used to make predictions during training. So the batch normalization layers do not have the activation statistics computed at the end of training. We can compute these statistics by doing a single forward pass on the train data with the SWA model.&lt;/p&gt;

&lt;p&gt;While we focus on SGD for simplicity in the description above, SWA can be combined with any optimizer. You can also use cyclical learning rates instead of a high constant value (see e.g., [2]).&lt;/p&gt;

&lt;h2 id=&quot;how-to-use-swa-in-pytorch&quot;&gt;How to use SWA in PyTorch?&lt;/h2&gt;

&lt;p&gt;In &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.optim.swa_utils&lt;/code&gt; we implement all the SWA ingredients to make it convenient to use SWA with any model. In particular, we implement &lt;code class=&quot;highlighter-rouge&quot;&gt;AveragedModel&lt;/code&gt; class for SWA models, &lt;code class=&quot;highlighter-rouge&quot;&gt;SWALR&lt;/code&gt; learning rate scheduler, and &lt;code class=&quot;highlighter-rouge&quot;&gt;update_bn&lt;/code&gt; utility function to update SWA batch normalization statistics at the end of training.&lt;/p&gt;

&lt;p&gt;In the example below, &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_model&lt;/code&gt; is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs, and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.optim.swa_utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AveragedModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SWALR&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.optim.lr_scheduler&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CosineAnnealingLR&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;swa_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AveragedModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CosineAnnealingLR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T_max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;swa_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;swa_scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SWALR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;swa_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;swa_scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Update bn statistics for the swa_model at the end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swa_utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_bn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Use swa_model to make predictions on test data &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we explain each component of &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.optim.swa_utils&lt;/code&gt; in detail.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AveragedModel&lt;/code&gt; class serves to compute the weights of the SWA model. You can create an averaged model by running  &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_model = AveragedModel(model)&lt;/code&gt;. You can then update the parameters of the averaged model by &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_model.update_parameters(model)&lt;/code&gt;. By default, &lt;code class=&quot;highlighter-rouge&quot;&gt;AveragedModel&lt;/code&gt; computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the &lt;code class=&quot;highlighter-rouge&quot;&gt;avg_fn&lt;/code&gt; parameter. In the following example, &lt;code class=&quot;highlighter-rouge&quot;&gt;ema_model&lt;/code&gt; computes an exponential moving average.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ema_avg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averaged_model_parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_averaged&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\
&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averaged_model_parameter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_parameter&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ema_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swa_utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AveragedModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ema_avg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In practice, we find an equal average with the modified learning rate schedule in Figure 2 provides the best performance.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SWALR&lt;/code&gt; is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to &lt;code class=&quot;highlighter-rouge&quot;&gt;0.05&lt;/code&gt; in &lt;code class=&quot;highlighter-rouge&quot;&gt;5&lt;/code&gt; epochs within each parameter group.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;swa_scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swa_utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SWALR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;anneal_strategy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;linear&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;anneal_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We also implement cosine annealing to a fixed value (&lt;code class=&quot;highlighter-rouge&quot;&gt;anneal_strategy=&quot;cos&quot;&lt;/code&gt;). In practice, we typically switch to &lt;code class=&quot;highlighter-rouge&quot;&gt;SWALR&lt;/code&gt; at epoch &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_start&lt;/code&gt;  (e.g. after 75% of the training epochs), and simultaneously start to compute the running averages of the weights:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr_scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CosineAnnealingLR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T_max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;swa_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;75&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;# &amp;lt;train epoch&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;swa_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;swa_scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, &lt;code class=&quot;highlighter-rouge&quot;&gt;update_bn&lt;/code&gt; is a utility function that computes the batchnorm statistics for the SWA model on a given dataloader &lt;code class=&quot;highlighter-rouge&quot;&gt;loader&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.optim.swa_utils.update_bn(loader, swa_model) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;update_bn&lt;/code&gt; applies the &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_model&lt;/code&gt; to every element in the dataloader and computes the activation statistics for each batch normalization layer in the model.&lt;/p&gt;

&lt;p&gt;Once you computed the SWA averages and updated the batch normalization layers, you can apply &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_model&lt;/code&gt; to make predictions on test data.&lt;/p&gt;

&lt;h2 id=&quot;why-does-it-work&quot;&gt;Why does it work?&lt;/h2&gt;

&lt;p&gt;There are large flat regions of the loss surface [9]. In Figure 3 below, we show a visualization of the loss surface in a subspace of the parameter space containing a path connecting two independently trained SGD solutions, such that the loss is similarly low at every point along the path. SGD converges near the boundary of these regions because there isn’t much gradient signal to move inside, as the points in the region all have similarly low values of loss. By increasing the learning rate, SWA spins around this flat region, and then by averaging the iterates, moves towards the center of the flat region.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch3.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: &lt;em&gt;visualization of mode connectivity for ResNet-20 with no skip connections on CIFAR-10 dataset. The visualization is created in collaboration with Javier Ideami &lt;a href=&quot;https://losslandscape.com/&quot;&gt;(https://losslandscape.com/)&lt;/a&gt;. For more details, see this &lt;a href=&quot;https://izmailovpavel.github.io/curves_blogpost/&quot;&gt;blogpost&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We expect solutions that are centered in the flat region of the loss to generalize better than those near the boundary. Indeed, train and test error surfaces are not perfectly aligned in the weight space. Solutions that are centered in the flat region are not as susceptible to the shifts between train and test error surfaces as those near the boundary. In Figure 4 below, we show the train loss and test error surfaces along the direction connecting the SWA and SGD solutions. As you can see, while the SWA solution has a higher train loss compared to the SGD solution, it is centered in a region of low loss and has a substantially better test error.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch4.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;. &lt;em&gt;Train loss and test error along the line connecting the SWA solution (circle) and SGD solution (square). The SWA solution is centered in a wide region of low train loss, while the SGD solution lies near the boundary. Because of the shift between train loss and test error surfaces, the SWA solution leads to much better generalization&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-results-achieved-with-swa&quot;&gt;What are the results achieved with SWA?&lt;/h2&gt;

&lt;p&gt;We release a GitHub &lt;a href=&quot;https://github.com/izmailovpavel/torch_swa_examples&quot;&gt;repo&lt;/a&gt; with examples using the PyTorch implementation of SWA for training DNNs. For example, these examples can be used to achieve the following results on CIFAR-100:&lt;/p&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;VGG-16&lt;/th&gt;
      &lt;th&gt;ResNet-164&lt;/th&gt;
      &lt;th&gt;WideResNet-28x10&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;SGD&lt;/td&gt;
      &lt;td&gt;72.8 ± 0.3&lt;/td&gt;
      &lt;td&gt;78.4 ± 0.3&lt;/td&gt;
      &lt;td&gt;81.0 ± 0.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SWA&lt;/td&gt;
      &lt;td&gt;74.4 ± 0.3&lt;/td&gt;
      &lt;td&gt;79.8 ± 0.4&lt;/td&gt;
      &lt;td&gt;82.5 ± 0.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;semi-supervised-learning&quot;&gt;Semi-Supervised Learning&lt;/h2&gt;

&lt;p&gt;In a follow-up &lt;a href=&quot;https://arxiv.org/abs/1806.05594&quot;&gt;paper&lt;/a&gt; SWA was applied to semi-supervised learning, where it improved the best reported results in multiple settings [2]. For example, with SWA you can get  95% accuracy on CIFAR-10 if you only have the training labels for 4k training data points (the previous best reported result on this problem was 93.7%). This paper also explores averaging multiple times within epochs, which can accelerate convergence and find still flatter solutions in a given time.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch5.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;. Performance of fast-SWA on semi-supervised learning with CIFAR-10. fast-SWA achieves record results in every setting considered.&lt;/p&gt;

&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;In another follow-up &lt;a href=&quot;http://www.gatsby.ucl.ac.uk/~balaji/udl-camera-ready/UDL-24.pdf&quot;&gt;paper&lt;/a&gt; SWA was shown to improve the performance of policy gradient methods A2C and DDPG on several Atari games and MuJoCo environments [3]. This application is also an instance of where SWA is used with Adam. Recall that SWA is not specific to SGD and can benefit essentially any optimizer.&lt;/p&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Environment Name&lt;/th&gt;
      &lt;th&gt;A2C&lt;/th&gt;
      &lt;th&gt;A2C + SWA&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Breakout&lt;/td&gt;
      &lt;td&gt;522 ± 34&lt;/td&gt;
      &lt;td&gt;703 ± 60&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Qbert&lt;/td&gt;
      &lt;td&gt;18777 ± 778&lt;/td&gt;
      &lt;td&gt;21272 ± 655&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SpaceInvaders&lt;/td&gt;
      &lt;td&gt;7727 ± 1121&lt;/td&gt;
      &lt;td&gt;21676 ± 8897&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Seaquest&lt;/td&gt;
      &lt;td&gt;1779 ± 4&lt;/td&gt;
      &lt;td&gt;1795 ± 4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BeamRider&lt;/td&gt;
      &lt;td&gt;9999 ± 402&lt;/td&gt;
      &lt;td&gt;11321 ± 1065&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CrazyClimber&lt;/td&gt;
      &lt;td&gt;147030 ± 10239&lt;/td&gt;
      &lt;td&gt;139752 ± 11618&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;low-precision-training&quot;&gt;Low Precision Training&lt;/h2&gt;

&lt;p&gt;We can filter through quantization noise by combining weights that have been rounded down with weights that have been rounded up. Moreover, by averaging weights to find a flat region of the loss surface, large perturbations of the weights will not affect the quality of the solution (Figures 9 and 10). Recent &lt;a href=&quot;https://arxiv.org/abs/1904.11943&quot;&gt;work&lt;/a&gt; shows that by adapting SWA to the low precision setting, in a method called SWALP, one can match the performance of full-precision SGD even with all training in 8 bits [5]. This is quite a practically important result, given that (1) SGD training in 8 bits performs notably worse than full precision SGD, and (2) low precision training is significantly harder than predictions in low precision after training (the usual setting). For example, a ResNet-164 trained on CIFAR-100 with float (16-bit) SGD achieves 22.2% error, while 8-bit SGD achieves 24.0% error. By contrast, SWALP with 8 bit training achieves 21.8% error.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/nswapytorch6.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 9&lt;/strong&gt;. &lt;em&gt;Quantizing a solution leads to a perturbation of the weights which has a greater effect on the quality of the sharp solution (left) compared to wide solution (right)&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch7.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 10&lt;/strong&gt;. &lt;em&gt;The difference between standard low precision training and SWALP&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Another &lt;a href=&quot;https://arxiv.org/abs/2002.00343&quot;&gt;work&lt;/a&gt;, SQWA, presents an approach for quantization and fine-tuning of neural networks in low precision [12]. In particular, SQWA achieved state-of-the-art results for DNNs quantized to 2 bits on CIFAR-100 and ImageNet.&lt;/p&gt;

&lt;h2 id=&quot;calibration-and-uncertainty-estimates&quot;&gt;Calibration and Uncertainty Estimates&lt;/h2&gt;

&lt;p&gt;By finding a centred solution in the loss, SWA can also improve calibration and uncertainty representation. Indeed, SWA can be viewed as an approximation to an ensemble, resembling a Bayesian model average, but with a single model [1].&lt;/p&gt;

&lt;p&gt;SWA can be viewed as taking the first moment of SGD iterates with a modified learning rate schedule. We can directly generalize SWA by also taking the second moment of iterates to form a Gaussian approximate posterior over the weights, further characterizing the loss geometry with SGD iterates.  This approach,&lt;a href=&quot;https://arxiv.org/abs/1902.02476&quot;&gt;SWA-Gaussian (SWAG)&lt;/a&gt; is a simple, scalable and convenient approach to uncertainty estimation and calibration in Bayesian deep learning [4]. The SWAG distribution approximates the shape of the true posterior: Figure 6 below shows the SWAG distribution and the posterior log-density for ResNet-20 on CIFAR-10.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/nswapytorch8.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;. &lt;em&gt;SWAG posterior approximation and the loss surface for a ResNet-20 without skip-connections trained on CIFAR-10 in the subspace formed by the two largest eigenvalues of the SWAG covariance matrix. The shape of SWAG distribution is aligned with the posterior: the peaks of the two distributions coincide, and both distributions are wider in one direction than in the orthogonal direction. Visualization created in collaboration with&lt;/em&gt; &lt;a href=&quot;https://losslandscape.com/&quot;&gt;Javier Ideami&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Empirically, SWAG performs on par or better than popular alternatives including MC dropout, KFAC Laplace, and temperature scaling on uncertainty quantification, out-of-distribution detection, calibration and transfer learning in computer vision tasks. Code for SWAG is available &lt;a href=&quot;https://github.com/wjmaddox/swa_gaussian&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch9.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 7&lt;/strong&gt;. &lt;em&gt;MultiSWAG generalizes SWAG and deep ensembles, to perform Bayesian model averaging over multiple basins of attraction, leading to significantly improved performance. By contrast, as shown here, deep ensembles select different modes, while standard variational inference (VI) marginalizes (model averages) within a single basin&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;MultiSWAG [9] uses multiple independent SWAG models to form a mixture of Gaussians as an approximate posterior distribution. Different basins of attraction contain highly complementary explanations of the data. Accordingly, marginalizing over these multiple basins provides a significant boost in accuracy and uncertainty representation. MultiSWAG can be viewed as a generalization of deep ensembles, but with performance improvements.&lt;/p&gt;

&lt;p&gt;Indeed, we see in Figure 8 that MultiSWAG entirely mitigates double descent – more flexible models have monotonically improving performance – and provides significantly improved generalization over SGD. For example, when the ResNet-18 has layers of width 20, Multi-SWAG achieves under 30% error whereas SGD achieves over 45%, more than a 15% gap!&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch10.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 8&lt;/strong&gt;. &lt;em&gt;SGD, SWAG, and Multi-SWAG on CIFAR-100 for a ResNet-18 with varying widths. We see Multi-SWAG in particular mitigates double descent and provides significant accuracy improvements over SGD&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Reference [10] also considers Multi-SWA, which uses multiple independently trained SWA solutions in an ensemble, providing performance improvements over deep ensembles without any additional computational cost. Code for MultiSWA and MultiSWAG is available &lt;a href=&quot;https://github.com/izmailovpavel/understandingbdl&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another &lt;a href=&quot;https://arxiv.org/abs/1907.07504&quot;&gt;method&lt;/a&gt;, Subspace Inference, constructs a low-dimensional subspace around the SWA solution and marginalizes the weights in this subspace to approximate the Bayesian model average [5]. Subspace Inference uses the statistics from the SGD iterates to construct both the SWA solution and the subspace. The method achieves strong performance in terms of prediction accuracy and uncertainty calibration both in classification and regression problems. Code is available &lt;a href=&quot;https://github.com/wjmaddox/drbayes&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;try-it-out&quot;&gt;Try it Out!&lt;/h2&gt;

&lt;p&gt;One of the greatest open questions in deep learning is why SGD manages to find good solutions, given that the training objectives are highly multimodal, and there are many settings of parameters that achieve no training loss but poor generalization. By understanding geometric features such as flatness, which relate to generalization, we can begin to resolve these questions and build optimizers that provide even better generalization, and many other useful features, such as uncertainty representation. We have presented SWA, a simple drop-in replacement for standard optimizers such as SGD and Adam, which can in principle, benefit anyone training a deep neural network. SWA has been demonstrated to have a strong performance in several areas, including computer vision, semi-supervised learning, reinforcement learning, uncertainty representation, calibration, Bayesian model averaging, and low precision training.&lt;/p&gt;

&lt;p&gt;We encourage you to try out SWA! SWA is now as easy as any standard training in PyTorch. And even if you have already trained your model, you can use SWA to significantly improve performance by running it for a small number of epochs from a pre-trained model.&lt;/p&gt;

&lt;p&gt;[1] Averaging Weights Leads to Wider Optima and Better Generalization; Pavel Izmailov, Dmitry Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson; Uncertainty in Artificial Intelligence (UAI), 2018.&lt;/p&gt;

&lt;p&gt;[2] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average; Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson; 
International Conference on Learning Representations (ICLR), 2019.&lt;/p&gt;

&lt;p&gt;[3] Improving Stability in Deep Reinforcement Learning with Weight Averaging; Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, 
Timur Garipov, Pavel Shvechikov, Dmitry Vetrov, Andrew Gordon Wilson; UAI 2018 Workshop: Uncertainty in Deep Learning, 2018.&lt;/p&gt;

&lt;p&gt;[4]  A Simple Baseline for Bayesian Uncertainty in Deep Learning
Wesley Maddox, Timur Garipov, Pavel Izmailov, Andrew Gordon Wilson; Neural Information Processing Systems (NeurIPS), 2019.&lt;/p&gt;

&lt;p&gt;[5] Subspace Inference for Bayesian Deep Learning
Pavel Izmailov, Wesley Maddox, Polina Kirichenko, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson
Uncertainty in Artificial Intelligence (UAI), 2019.&lt;/p&gt;

&lt;p&gt;[6] SWALP : Stochastic Weight Averaging in Low Precision Training
Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, 
Andrew Gordon Wilson, Christopher De Sa; International Conference on Machine Learning  (ICML), 2019.&lt;/p&gt;

&lt;p&gt;[7] David Ruppert. Efficient estimations from a slowly convergent Robbins-Monro process; Technical report, Cornell University Operations Research and Industrial Engineering, 1988.&lt;/p&gt;

&lt;p&gt;[8] Acceleration of stochastic approximation by averaging. Boris T Polyak and Anatoli B Juditsky; SIAM Journal on Control and Optimization, 30(4):838–855, 1992.&lt;/p&gt;

&lt;p&gt;[9] Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, 
Andrew Gordon Wilson. Neural Information Processing Systems (NeurIPS), 2018.&lt;/p&gt;

&lt;p&gt;[10] Bayesian Deep Learning and a Probabilistic Perspective of Generalization
Andrew Gordon Wilson, Pavel Izmailov. ArXiv preprint, 2020.&lt;/p&gt;

&lt;p&gt;[11] Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well
Gupta, Vipul, Santiago Akle Serrano, and Dennis DeCoste; International Conference on Learning Representations (ICLR). 2019.&lt;/p&gt;

&lt;p&gt;[12] SQWA: Stochastic Quantized Weight Averaging for Improving the Generalization Capability of Low-Precision Deep Neural Networks
Shin, Sungho, Yoonho Boo, and Wonyong Sung; arXiv preprint 2020.&lt;/p&gt;</content><author><name>Pavel Izmailov, Andrew Gordon Wilson and Vincent Queneneville-Belair</name></author><summary type="html">Do you use stochastic gradient descent (SGD) or Adam? Regardless of the procedure you use to train your neural network, you can likely achieve significantly better generalization at virtually no additional cost with a simple new technique now natively supported in PyTorch 1.6, Stochastic Weight Averaging (SWA) [1]. Even if you have already trained your model, it’s easy to realize the benefits of SWA by running SWA for a small number of epochs starting with a pre-trained model. Again and again, researchers are discovering that SWA improves the performance of well-tuned models in a wide array of practical applications with little cost or effort!</summary></entry></feed>