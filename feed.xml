<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pytorch.org/" rel="alternate" type="text/html" /><updated>2021-07-29T01:20:42-07:00</updated><id>https://pytorch.org/feed.xml</id><title type="html">PyTorch Website</title><subtitle>Scientific Computing...</subtitle><author><name>Facebook</name></author><entry><title type="html">Everything You Need To Know About Torchvision’s SSDlite Implementation</title><link href="https://pytorch.org/blog/torchvision-ssdlite-implementation/" rel="alternate" type="text/html" title="Everything You Need To Know About Torchvision’s SSDlite Implementation" /><published>2021-06-27T00:00:00-07:00</published><updated>2021-06-27T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchvision-ssdlite-implementation</id><content type="html" xml:base="https://pytorch.org/blog/torchvision-ssdlite-implementation/">&lt;p&gt;In the &lt;a href=&quot;https://pytorch.org/blog/torchvision-ssd-implementation/&quot;&gt;previous article&lt;/a&gt;, we’ve discussed how the SSD algorithm works, covered its implementation details and presented its training process. If you have not read the previous blog post, I encourage you to check it out before continuing.&lt;/p&gt;

&lt;p&gt;In this part 2 of the series, we will focus on the mobile-friendly variant of SSD called SSDlite. Our plan is to first go through the main components of the algorithm highlighting the parts that differ from the original SSD, then discuss how the released model was trained and finally provide detailed benchmarks for all the new Object Detection models that we explored.&lt;/p&gt;

&lt;h1 id=&quot;the-ssdlite-network-architecture&quot;&gt;The SSDlite Network Architecture&lt;/h1&gt;

&lt;p&gt;The SSDlite is an adaptation of SSD which was first briefly introduced on the &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;MobileNetV2 paper&lt;/a&gt; and later reused on the &lt;a href=&quot;https://arxiv.org/abs/1905.02244&quot;&gt;MobileNetV3 paper&lt;/a&gt;. Because the main focus of the two papers was to introduce novel CNN architectures, most of the implementation details of SSDlite were not clarified. Our code follows all the details presented on the two papers and where necessary fills the gaps from the &lt;a href=&quot;https://github.com/tensorflow/models/tree/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection&quot;&gt;official implementation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As noted before, the SSD is a family of models because one can configure it with different backbones (such as VGG, MobileNetV3 etc) and different Heads (such as using regular convolutions, separable convolutions etc). Thus many of the SSD components remain the same in SSDlite. Below we discuss only those that are different&lt;/p&gt;

&lt;h2 id=&quot;classification-and-regression-heads&quot;&gt;Classification and Regression Heads&lt;/h2&gt;

&lt;p&gt;Following the Section 6.2 of the MobileNetV2 paper, SSDlite replaces the regular convolutions used on the original Heads with separable convolutions. Consequently, our implementation introduces &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L65-L95&quot;&gt;new heads&lt;/a&gt; that use &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L26-L36&quot;&gt;3x3 Depthwise convolutions and 1x1 projections&lt;/a&gt;. Since all other components of the SSD method remain the same, to create an SSDlite model our implementation &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L222-L223&quot;&gt;initializes the SSDlite head&lt;/a&gt; and passes it directly to the SSD constructor.&lt;/p&gt;

&lt;h2 id=&quot;backbone-feature-extractor&quot;&gt;Backbone Feature Extractor&lt;/h2&gt;

&lt;p&gt;Our implementation introduces a new class for building MobileNet &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L98&quot;&gt;feature extractors&lt;/a&gt;. Following the Section 6.3 of the MobileNetV3 paper, the backbone returns the &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L106&quot;&gt;output of the expansion layer&lt;/a&gt; of the Inverted Bottleneck block which has an output stride of 16 and the &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L107&quot;&gt;output of the layer just before the pooling&lt;/a&gt; which has an output stride of 32. Moreover, all &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L111-L116&quot;&gt;extra blocks&lt;/a&gt; of the backbone are replaced with &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L39-L54&quot;&gt;lightweight equivalents&lt;/a&gt; which use a 1x1 compression, a separable 3x3 convolution with stride 2 and a 1x1 expansion. Finally to ensure that the heads have enough prediction power even when small &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L99&quot;&gt;width multipliers&lt;/a&gt; are used, the &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L110&quot;&gt;minimum depth&lt;/a&gt; size of all convolutions is controlled by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min_depth&lt;/code&gt; hyperparameter.&lt;/p&gt;

&lt;h1 id=&quot;the-ssdlite320-mobilenetv3-large-model&quot;&gt;The SSDlite320 MobileNetV3-Large model&lt;/h1&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/ssdlite-pre-trained.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;This section discusses the configuration of the provided &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L159-L162&quot;&gt;SSDlite pre-trained&lt;/a&gt; model along with the training processes followed to replicate the paper results as closely as possible.&lt;/p&gt;

&lt;h2 id=&quot;training-process&quot;&gt;Training process&lt;/h2&gt;

&lt;p&gt;All of the hyperparameters and scripts used to train the model on the COCO dataset can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/blob/e35793a1a4000db1f9f99673437c514e24e65451/references/detection/README.md#ssdlite320-mobilenetv3-large&quot;&gt;references&lt;/a&gt; folder. Here we discuss the most notable details of the training process.&lt;/p&gt;

&lt;h3 id=&quot;tuned-hyperparameters&quot;&gt;Tuned Hyperparameters&lt;/h3&gt;

&lt;p&gt;Though the papers don’t provide any information on the hyperparameters used for training the models (such as regularization, learning rate and the batch size), the parameters listed in the &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config&quot;&gt;configuration files&lt;/a&gt; on the official repo were good starting points and using cross validation we adjusted them to their optimal values. All the above gave us a significant boost over the baseline SSD configuration.&lt;/p&gt;

&lt;h3 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h3&gt;

&lt;p&gt;Key important difference of SSDlite comparing to SSD is that the backbone of the first has only a fraction of the weights of the latter. This is why in SSDlite, the Data Augmentation focuses more on making the model robust to objects of variable sizes than trying to avoid overfitting. Consequently, SSDlite &lt;a href=&quot;https://github.com/pytorch/vision/blob/43d772067fe77965ec8fc49c799de5cea44b8aa2/references/detection/presets.py#L19-L24&quot;&gt;uses only a subset&lt;/a&gt; of the SSD transformations and this way it avoids the over-regularization of the model.&lt;/p&gt;

&lt;h3 id=&quot;lr-scheme&quot;&gt;LR Scheme&lt;/h3&gt;

&lt;p&gt;Due to the reliance on Data Augmentation to make the model robust to small and medium sized objects, we found that it is particularly beneficial for the training recipe to use large number of epochs. More specifically by using roughly 3x more epochs than SSD we are able to increase our precision by 4.2mAP points and by using a 6x multiplier we improve by 4.9mAP. Increasing further the epochs seems to yield diminishing returns and makes the training too slow and impractical, nevertheless based on the &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config#L154&quot;&gt;model configuration&lt;/a&gt; it seems that the authors of the paper used an equivalent &lt;em&gt;16x multiplier&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;weight-initialization--input-scaling--relu6&quot;&gt;Weight Initialization &amp;amp; Input Scaling &amp;amp; ReLU6&lt;/h3&gt;

&lt;p&gt;A set of final optimizations that brought our implementation very close to the official one and helped us bridge the accuracy gap was training the backbone &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L139-L141&quot;&gt;from scratch&lt;/a&gt; instead of initializing from ImageNet, adapting our &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L57-L62&quot;&gt;weight initialization scheme&lt;/a&gt;, changing our &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L216-L219&quot;&gt;Input Scaling&lt;/a&gt; and replacing all standard ReLUs added on the SSDlite heads with ReLU6. Note that since we trained the model from random weights, we additionally applied the speed optimization described on the paper of using a &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L196-L197&quot;&gt;reduced tail&lt;/a&gt; on the backbone.&lt;/p&gt;

&lt;h3 id=&quot;implementation-differences&quot;&gt;Implementation Differences&lt;/h3&gt;

&lt;p&gt;Comparing the above implementation with the one on the official repo, we’ve identified a few differences. Most of them are minor and they are related to how we initialize the weights (for example &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L57-L62&quot;&gt;Normal initialization&lt;/a&gt; vs &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config#L104-L107&quot;&gt;Truncated Normal&lt;/a&gt;), how we parameterize the LR Scheduling (for example &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/references/detection/engine.py#L21-L22&quot;&gt;smaller&lt;/a&gt; vs &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config#L169-L170&quot;&gt;larger&lt;/a&gt; warmup rate, &lt;a href=&quot;https://github.com/pytorch/vision/tree/master/references/detection#ssdlite320-mobilenetv3-large&quot;&gt;shorter&lt;/a&gt; vs &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config#L154&quot;&gt;longer&lt;/a&gt; training) etc. The biggest known difference lies in the way we compute the Classification loss. More specifically the implementation of SSDlite with MobileNetV3 backbone on the official repo &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config#L121-L124&quot;&gt;doesn’t use the SSD’s Multibox loss&lt;/a&gt; but instead uses RetinaNet’s &lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;&gt;focal loss&lt;/a&gt;. This is a rather significant deviation from the paper and since TorchVision already offers a full implementation of RetinaNet, we decided to implement SSDlite using the normal Multi-box SSD loss.&lt;/p&gt;

&lt;h2 id=&quot;break-down-of-key-accuracy-improvements&quot;&gt;Break down of key accuracy improvements&lt;/h2&gt;

&lt;p&gt;As discussed in previous articles, reproducing research papers and porting them to code is not a journey of monotonically increasing accuracies, especially in cases where the full training and implementation details are not known. Typically the process involves lots of backtracking as one needs to identify those implementation details and parameters that have significant impact on the accuracy from those that don’t. Below we try to visualize the most important iterations that improved our accuracy from the baseline:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/mAP-of-SSD320-MobileNetV3-Large.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Iteration&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;mAP&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Baseline with “SSD-style” Hyperparams&lt;/td&gt;
      &lt;td&gt;10.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Tuned Hyperparams&lt;/td&gt;
      &lt;td&gt;14.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ SSDlite Data Augmentation&lt;/td&gt;
      &lt;td&gt;15.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ 3x LR Scheme&lt;/td&gt;
      &lt;td&gt;19.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ 6x LR Scheme&lt;/td&gt;
      &lt;td&gt;20.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Weight Initialization &amp;amp; Input Scaling &amp;amp; ReLU6&lt;/td&gt;
      &lt;td&gt;21.3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The order of optimizations presented above is accurate, though a bit idealized in some cases. For example, though different schedulers were tested during the Hyperparameter tuning phase, none of them provided significant improvements and thus we maintained the MultiStepLR which was used in the baseline. Nevertheless while later experimenting with different LR Schemes, we found it beneficial to switch to CosineAnnealingLR, as it required less configuration. Consequently, we believe that the main takeaway from the above summary should be that even by starting with a correct implementation and a set of optimal hyperparams from a model of the same family, there is always accuracy points to be found by optimizing the training recipe and tuning the implementation. Admittedly the above is a rather extreme case where the accuracy doubled, but still in many cases there is a large number of optimizations that can help us push the accuracy significantly.&lt;/p&gt;

&lt;h1 id=&quot;benchmarks&quot;&gt;Benchmarks&lt;/h1&gt;

&lt;p&gt;Here is how to initialize the two pre-trained models:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ssdlite&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssdlite320_mobilenet_v3_large&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssd300_vgg16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are the benchmarks between the new and selected previous detection models:&lt;/p&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;mAP&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Inference on CPU (sec)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;# Params (M)&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;SSDlite320 MobileNetV3-Large&lt;/td&gt;
      &lt;td&gt;21.3&lt;/td&gt;
      &lt;td&gt;0.0911&lt;/td&gt;
      &lt;td&gt;3.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SSD300 VGG16&lt;/td&gt;
      &lt;td&gt;25.1&lt;/td&gt;
      &lt;td&gt;0.8303&lt;/td&gt;
      &lt;td&gt;35.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SSD512 VGG16 (not released)&lt;/td&gt;
      &lt;td&gt;28.8&lt;/td&gt;
      &lt;td&gt;2.2494&lt;/td&gt;
      &lt;td&gt;37.08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SSD512 ResNet50 (not released)&lt;/td&gt;
      &lt;td&gt;30.2&lt;/td&gt;
      &lt;td&gt;1.1137&lt;/td&gt;
      &lt;td&gt;42.70&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN MobileNetV3-Large 320 FPN (Low-Res)&lt;/td&gt;
      &lt;td&gt;22.8&lt;/td&gt;
      &lt;td&gt;0.1679&lt;/td&gt;
      &lt;td&gt;19.39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN MobileNetV3-Large FPN (High-Res)&lt;/td&gt;
      &lt;td&gt;32.8&lt;/td&gt;
      &lt;td&gt;0.8409&lt;/td&gt;
      &lt;td&gt;19.39&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As we can see, the SSDlite320 MobileNetV3-Large model is by far the fastest and smallest model and thus it’s an excellent candidate for real-world mobile applications. Though its accuracy is lower than the pre-trained low-resolution Faster R-CNN equivalent, the SSDlite framework is adaptable and one can boost its accuracy by introducing heavier heads with more convolutions.&lt;/p&gt;

&lt;p&gt;On the other hand, the SSD300 VGG16 model is rather slow and less accurate. This is mainly because of its VGG16 backbone. Though extremely important and influential, the VGG architecture is nowadays quite outdated. Thus though the specific model has historical and research value and hence it’s included in TorchVision, we recommend to users who want high-resolution detectors for real world applications to either combine SSD with alternative backbones (see this &lt;a href=&quot;https://github.com/pytorch/vision/pull/3760&quot;&gt;example&lt;/a&gt; on how to create one) or use one of the Faster R-CNN pre-trained models.&lt;/p&gt;

&lt;p&gt;We hope you enjoyed the 2nd and final part of the SSD series. We are looking forward to your feedback.&lt;/p&gt;</content><author><name>Vasilis Vryniotis</name></author><summary type="html">In the previous article, we’ve discussed how the SSD algorithm works, covered its implementation details and presented its training process. If you have not read the previous blog post, I encourage you to check it out before continuing.</summary></entry><entry><title type="html">The torch.linalg module: Accelerated Linear Algebra with Autograd in PyTorch</title><link href="https://pytorch.org/blog/torch-linalg-autograd/" rel="alternate" type="text/html" title="The torch.linalg module: Accelerated Linear Algebra with Autograd in PyTorch" /><published>2021-06-23T00:00:00-07:00</published><updated>2021-06-23T00:00:00-07:00</updated><id>https://pytorch.org/blog/torch-linalg-autograd</id><content type="html" xml:base="https://pytorch.org/blog/torch-linalg-autograd/">&lt;p&gt;Linear algebra is essential to deep learning and scientific computing, and it’s always been a core part of PyTorch. PyTorch 1.9 extends PyTorch’s support for linear algebra operations with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; module. This module, documented &lt;a href=&quot;https://pytorch.org/docs/master/linalg.html?highlight=linalg#module-torch.linalg&quot;&gt;here&lt;/a&gt;, has 26 operators, including faster and easier to use versions of older PyTorch operators, every function from &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.linalg.html&quot;&gt;NumPy’s linear algebra module&lt;/a&gt; extended with accelerator and autograd support, and a few operators that are completely new. This makes the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; immediately familiar to NumPy users and an exciting update to PyTorch’s linear algebra support.&lt;/p&gt;

&lt;h1 id=&quot;numpy-like-linear-algebra-in-pytorch&quot;&gt;NumPy-like linear algebra in PyTorch&lt;/h1&gt;

&lt;p&gt;If you’re familiar with NumPy’s linear algebra module then it’ll be easy to start using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt;. In most cases it’s a drop-in replacement. Let’s looking at drawing samples from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_normal_distribution&quot;&gt;multivariate normal distribution&lt;/a&gt; using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cholesky_decomposition&quot;&gt;Cholesky decomposition&lt;/a&gt; as a motivating example to demonstrate this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Creates inputs
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mu_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Covariance matrix sigma is positive-definite
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;normal_noise_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;standard_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;multivariate_normal_sample_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cholesky&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Random sample: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
      &lt;span class=&quot;n&quot;&gt;multivariate_normal_sample_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Random&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.9502426&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.78518077&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.83168697&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.90798228&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now let’s see the same sampler implemented in PyTorch:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;multivariate_normal_sample_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cholesky&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The two functions are identical, and we can validate their behavior by calling the function with the same arguments wrapped as PyTorch tensors:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# NumPy arrays are wrapped as tensors and share their memory
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_torch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;normal_noise_torch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal_noise_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;multivariate_normal_sample_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.9502&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.7852&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.8317&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9080&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The only difference is in how PyTorch prints tensors by default.&lt;/p&gt;

&lt;p&gt;The Cholesky decomposition can also help us quickly compute the probability density function of the non-degenerate multivariate normal distribution. One of the expensive terms in that computation is the square root of the determinant of the covariance matrix. Using &lt;a href=&quot;https://en.wikipedia.org/wiki/Determinant#Properties_of_the_determinant&quot;&gt;properties of the determinant&lt;/a&gt; and the Cholesky decomposition we can calculate the same result faster than the naive computation, however. Here’s the NumPy program that demonstrates this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sqrt_sigma_det_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;det&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sqrt_L_det_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cholesky&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;|sigma|^0.5 = &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt_sigma_det_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|^&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.237127491242027&lt;/span&gt;
 
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;|L| = &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt_L_det_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.237127491242028&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And here’s the same validation in PyTorch:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sqrt_sigma_det_torch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;det&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sqrt_L_det_torch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cholesky&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;|sigma|^0.5 = &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt_sigma_det_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|^&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.2371&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;|L| = &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt_L_det_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.2371&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can measure the difference in run time using PyTorch’s built-in benchmark utility:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.benchmark&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stmt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'torch.sqrt(torch.linalg.det(sigma))'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sigma'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stmt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'torch.prod(torch.diag(torch.linalg.cholesky(sigma)))'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sigma'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;det&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;mf&quot;&gt;80.80&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cholesky&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
  &lt;span class=&quot;mf&quot;&gt;11.56&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Demonstrating that the approach using the Cholesky decomposition can be significantly faster. Behind the scenes, PyTorch’s linear algebra module uses OpenBLAS or MKL implementations of the LAPACK standard to maximize its CPU performance.&lt;/p&gt;

&lt;h1 id=&quot;autograd-support&quot;&gt;Autograd Support&lt;/h1&gt;

&lt;p&gt;PyTorch’s linear algebra module doesn’t just implement the same functions as NumPy’s linear algebra module (and a few more), it also extends them with autograd and CUDA support.&lt;/p&gt;

&lt;p&gt;Let’s look at a very simple program that just computes an inverse and the gradient of that operation to show how autograd works:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can mimic the same computation in NumPy by defining the autograd formula ourselves:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;inv_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;inv_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inv_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Of course, as programs become more complicated it’s convenient to have builtin autograd support, and PyTorch’s linear algebra module supports both real and complex autograd.&lt;/p&gt;

&lt;h1 id=&quot;cuda-support&quot;&gt;CUDA Support&lt;/h1&gt;

&lt;p&gt;Support for autograd and accelerators, like CUDA devices, is a core part of PyTorch. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; module was developed with NVIDIA’s PyTorch and cuSOLVER teams, who helped optimize its performance on CUDA devices with the cuSOLVER, cuBLAS, and MAGMA libraries. These improvements make PyTorch’s CUDA linear algebra operations faster than ever. For example, let’s look at the performance of PyTorch 1.9’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg.cholesky&lt;/code&gt; vs. PyTorch 1.8’s (now deprecated) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.cholesky&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/cholesky-decomposition.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;(The above charts were created using an Ampere A100 GPU with CUDA 11.3, cuSOLVER 11.1.1.58, and MAGMA 2.5.2. Matrices are in double precision.)&lt;/p&gt;

&lt;p&gt;These charts show that performance has increased significantly on larger matrices, and that batched performance is better across the board. Other linear algebra operations, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg.qr&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg.lstsq&lt;/code&gt;, have also had their CUDA performance improved.&lt;/p&gt;

&lt;h1 id=&quot;beyond-numpy&quot;&gt;Beyond NumPy&lt;/h1&gt;

&lt;p&gt;In addition to offering all the functions in NumPy’s linear algebra module with support for autograd and accelerators, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; has a few new functions of its own. NumPy’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linalg.norm&lt;/code&gt; does not allow users to compute vector norms over arbitrary subsets of dimensions, so to enable this functionality we added &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg.vector_norm&lt;/code&gt;. We’ve also started modernizing other linear algebra functionality in PyTorch, so we created &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg.householder_product&lt;/code&gt; to replace the older &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.orgqr&lt;/code&gt;, and we plan to continue adding more linear algebra functionality in the future, too.&lt;/p&gt;

&lt;h1 id=&quot;the-future-of-linear-algebra-in-pytorch&quot;&gt;The Future of Linear Algebra in PyTorch&lt;/h1&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; module is fast and familiar with great support for autograd and accelerators. It’s already being used in libraries like &lt;a href=&quot;https://github.com/pytorch/botorch&quot;&gt;botorch&lt;/a&gt;, too. But we’re not stopping here. We plan to continue updating more of PyTorch’s existing linear algebra functionality (like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.lobpcg&lt;/code&gt;) and offering more support for low rank and sparse linear algebra. We also want to hear your feedback on how we can improve, so start a conversation on the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;forum&lt;/a&gt; or file an issue on our &lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;Github&lt;/a&gt; and share your thoughts.&lt;/p&gt;

&lt;p&gt;We look forward to hearing from you and seeing what the community does with PyTorch’s new linear algebra functionality!&lt;/p&gt;</content><author><name>Mike Ruberry, Ivan Yashchuk, Xiao Wang, Mario Lezcano and Natalia Gimelshein</name></author><summary type="html">Linear algebra is essential to deep learning and scientific computing, and it’s always been a core part of PyTorch. PyTorch 1.9 extends PyTorch’s support for linear algebra operations with the torch.linalg module. This module, documented here, has 26 operators, including faster and easier to use versions of older PyTorch operators, every function from NumPy’s linear algebra module extended with accelerator and autograd support, and a few operators that are completely new. This makes the torch.linalg immediately familiar to NumPy users and an exciting update to PyTorch’s linear algebra support.</summary></entry><entry><title type="html">An Overview of the PyTorch Mobile Demo Apps</title><link href="https://pytorch.org/blog/mobile-demo-apps-overview/" rel="alternate" type="text/html" title="An Overview of the PyTorch Mobile Demo Apps" /><published>2021-06-18T10:00:00-07:00</published><updated>2021-06-18T10:00:00-07:00</updated><id>https://pytorch.org/blog/mobile-demo-apps-overview</id><content type="html" xml:base="https://pytorch.org/blog/mobile-demo-apps-overview/">&lt;p&gt;PyTorch Mobile provides a runtime environment to execute state-of-the-art machine learning models on mobile devices. Latency is reduced, privacy preserved, and models can run on mobile devices anytime, anywhere.&lt;/p&gt;

&lt;p&gt;In this blog post, we provide a quick overview of 10 currently available PyTorch Mobile powered demo apps running various state-of-the-art PyTorch 1.9 machine learning models spanning images, video, audio and text.&lt;/p&gt;

&lt;p&gt;It’s never been easier to deploy a state-of-the-art ML model to a phone. You don’t need any domain knowledge in Machine Learning and we hope one of the below examples resonates enough with you to be the starting point for your next project.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/mobile_app_code.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;computer-vision&quot;&gt;Computer Vision&lt;/h2&gt;
&lt;h3 id=&quot;image-classification&quot;&gt;Image Classification&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use PyTorch C++ libraries on iOS and Android to classify a static image with the MobileNetv2/3 model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld&quot;&gt;iOS #1&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/workshops/tree/master/PTMobileWalkthruIOS&quot;&gt;iOS #2&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp&quot;&gt;Android #1&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/workshops/tree/master/PTMobileWalkthruAndroid&quot;&gt;Android #2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://youtu.be/amTepUIR93k&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://youtu.be/5Lxuu16_28o&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_helloworld.png&quot; width=&quot;50%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;live-image-classification&quot;&gt;Live Image Classification&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to run a quantized MobileNetV2 and Resnet18 models to classify images in real time with an iOS and Android device camera.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/PyTorchDemo&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/PyTorchDemoApp&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
&lt;img src=&quot;/assets/images/screenshot_live_image_classification1.png&quot; width=&quot;40%&quot; /&gt;
&lt;img src=&quot;/assets/images/screenshot_live_image_classification2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;image-segmentation&quot;&gt;Image Segmentation&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use the PyTorch DeepLabV3 model to segment images. The updated app for PyTorch 1.9 also demonstrates how to create the model using the Mobile Interpreter and load the model with the LiteModuleLoader API.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/ImageSegmentation&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/ImageSegmentation&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tutorial.png&quot; /&gt; &lt;a href=&quot;https://pytorch.org/tutorials/beginner/deeplabv3_on_ios.html&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/ImageSegmentation&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/deeplab1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/deeplab2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;vision-transformer-for-handwritten-digit-recognition&quot;&gt;Vision Transformer for Handwritten Digit Recognition&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use Facebook’s latest optimized Vision Transformer DeiT model to do image classification and handwritten digit recognition.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/ViT4MNIST&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/ViT4MNIST&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/11L5mIjrLn7B7VdwjQl5vJv3ZVK4hcYut/view?usp=sharing&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_digit_recognition1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_digit_recognition2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to convert the popular YOLOv5 model and use it on an iOS app that detects objects from pictures in your photos, taken with camera, or with live camera.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/ObjectDetection&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/ObjectDetection&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/1pIDrUDnCD5uF-mIz8nbSlZcXxPlRBKhl/view&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://drive.google.com/file/d/1-5AoRONUqZPZByM-fy0m7r8Ct11OnlIT/view&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_object_detection1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_object_detection2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;d2go&quot;&gt;D2Go&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to create and use a much lighter and faster Facebook D2Go model to detect objects from pictures in your photos, taken with camera, or with live camera.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/D2Go&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/D2Go&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/1GO2Ykfv5ut2Mfoc06Y3QUTFkS7407YA4/view&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://drive.google.com/file/d/18-2hLc-7JAKtd1q00X-5pHQCAdyJg7dZ/view?usp=sharing&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_d2go1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_d2go2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;
&lt;h3 id=&quot;video-classification&quot;&gt;Video Classification&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use a pre-trained PyTorchVideo model to perform video classification on tested videos, videos from the Photos library, or even real-time videos.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/TorchVideo&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/TorchVideo&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/1ijb4UIuF2VQiab4xfAsBwrQXCInvb9wd/view&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://drive.google.com/file/d/193tkZgt5Rlk7u-EQPcvkoFtmOQ14-zCC/view&quot;&gt;Android&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=Qb4vDm-ruwI&quot;&gt;Deep Dive&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_video1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_video2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;natural-language-processing&quot;&gt;Natural Language Processing&lt;/h2&gt;
&lt;h3 id=&quot;text-classification&quot;&gt;Text Classification&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use a pre-trained Reddit model to perform text classification.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/PyTorchDemo&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/PyTorchDemoApp&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_textclassification1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_textclassification2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;machine-translation&quot;&gt;Machine Translation&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to convert a sequence-to-sequence neural machine translation model trained with the code in the PyTorch NMT tutorial for french to english translation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/Seq2SeqNMT&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/Seq2SeqNMT&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/17Edk-yAyfzijHPR_2ZDAIX7VY-TkQnLf/view&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://drive.google.com/file/d/110KN3Pa9DprkBWnzj8Ppa8KMymhmBI61/view?usp=sharing&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_machinetranslation1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_machinetranslation2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;question-answering&quot;&gt;Question Answering&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use the DistilBERT Hugging Face transformer model to answer questions about Pytorch Mobile itself.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/QuestionAnswering&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/QuestionAnswering&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/1QIB3yoP4I3zUU0bLCpvUqPV5Kv8f8JvB/view&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://drive.google.com/file/d/10hwGNFo5tylalKwut_CWFPJmV7JRdDKF/view?usp=sharing&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_qa1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_qa2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;audio&quot;&gt;Audio&lt;/h2&gt;
&lt;h3 id=&quot;speech-recognition&quot;&gt;Speech Recognition&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to convert Facebook AI’s torchaudio-powered wav2vec 2.0, one of the leading models in speech recognition to TorchScript before deploying it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/SpeechRecognition&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/SpeechRecognition&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_asr1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_asr2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We really hope one of these demo apps stood out for you. For the full list, make sure to visit the &lt;a href=&quot;https://github.com/pytorch/ios-demo-app&quot;&gt;iOS&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/android-demo-app&quot;&gt;Android&lt;/a&gt; demo app repos. You should also definitely check out the video &lt;a href=&quot;https://www.youtube.com/watch?v=Qb4vDm-ruwI&quot;&gt;An Overview of the PyTorch Mobile Demo Apps&lt;/a&gt; which provides both an overview of the PyTorch mobile demo apps and a deep dive into the PyTorch Video app for iOS and Android.&lt;/p&gt;</content><author><name>Jeff Tang and Mark Saroufim</name></author><summary type="html">PyTorch Mobile provides a runtime environment to execute state-of-the-art machine learning models on mobile devices. Latency is reduced, privacy preserved, and models can run on mobile devices anytime, anywhere.</summary></entry><entry><title type="html">Everything You Need To Know About Torchvision’s SSD Implementation</title><link href="https://pytorch.org/blog/torchvision-ssd-implementation/" rel="alternate" type="text/html" title="Everything You Need To Know About Torchvision’s SSD Implementation" /><published>2021-06-16T00:00:00-07:00</published><updated>2021-06-16T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchvision-ssd-implementation</id><content type="html" xml:base="https://pytorch.org/blog/torchvision-ssd-implementation/">&lt;p&gt;In TorchVision v0.10, we’ve released two new Object Detection models based on the SSD architecture. Our plan is to cover the key implementation details of the algorithms along with information on how they were trained in a two-part article.&lt;/p&gt;

&lt;p&gt;In part 1 of the series, we will focus on the original implementation of the SSD algorithm as described on the &lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot;&gt;Single Shot MultiBox Detector paper&lt;/a&gt;. We will briefly give a high-level description of how the algorithm works, then go through its main components, highlight key parts of its code, and finally discuss how we trained the released model. Our goal is to cover all the necessary details to reproduce the model including those optimizations which are not covered on the paper but are part on the &lt;a href=&quot;https://github.com/weiliu89/caffe/tree/ssd&quot;&gt;original implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;how-does-ssd-work&quot;&gt;How Does SSD Work?&lt;/h1&gt;

&lt;p&gt;Reading the aforementioned paper is highly recommended but here is a quick oversimplified refresher. Our target is to detect the locations of objects in an image along with their categories. Here is the Figure 5 from the &lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot;&gt;SSD paper&lt;/a&gt; with prediction examples of the model:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/prediction examples.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The SSD algorithm uses a CNN backbone, passes the input image through it and takes the convolutional outputs from different levels of the network. The list of these outputs are called feature maps. These feature maps are then passed through the Classification and Regression heads which are responsible for predicting the class and the location of the boxes.&lt;/p&gt;

&lt;p&gt;Since the feature maps of each image contain outputs from different levels of the network, their size varies and thus they can capture objects of different dimensions. On top of each, we tile several default boxes which can be thought as our rough prior guesses. For each default box, we predict whether there is an object (along with its class) and its offset (correction over the original location). During training time, we need to first match the ground truth to the default boxes and then we use those matches to estimate our loss. During inference, similar prediction boxes are combined to estimate the final predictions.&lt;/p&gt;

&lt;h1 id=&quot;the-ssd-network-architecture&quot;&gt;The SSD Network Architecture&lt;/h1&gt;

&lt;p&gt;In this section, we will discuss the key components of SSD. Our code follows closely &lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot;&gt;the paper&lt;/a&gt; and makes use of many of the undocumented optimizations included in &lt;a href=&quot;https://github.com/weiliu89/caffe/tree/ssd&quot;&gt;the official implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;defaultboxgenerator&quot;&gt;DefaultBoxGenerator&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L134&quot;&gt;DefaultBoxGenerator class&lt;/a&gt; is responsible for generating the default boxes of SSD and operates similarly to the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L9&quot;&gt;AnchorGenerator&lt;/a&gt; of FasterRCNN (for more info on their differences see pages 4-6 of the paper). It produces a set of predefined boxes of specific width and height which are tiled across the image and serve as the first rough prior guesses of where objects might be located. Here is Figure 1 from the SSD paper with a visualization of ground truths and default boxes:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/visualization of ground truths.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The class is parameterized by a set of hyperparameters that control &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L139&quot;&gt;their shape&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L140-L149&quot;&gt;tiling&lt;/a&gt;. The implementation will provide &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L162-L171&quot;&gt;automatically good guesses&lt;/a&gt; with the default parameters for those who want to experiment with new backbones/datasets but one can also pass &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L144-L147&quot;&gt;optimized custom values&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;ssdmatcher&quot;&gt;SSDMatcher&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L348&quot;&gt;SSDMatcher class&lt;/a&gt; extends the standard &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L227&quot;&gt;Matcher&lt;/a&gt; used by FasterRCNN and it is responsible for matching the default boxes to the ground truth. After estimating the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L349&quot;&gt;IoUs of all combinations&lt;/a&gt;, we use the matcher to find for each default box the best &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L296&quot;&gt;candidate&lt;/a&gt; ground truth with overlap higher than the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L350-L351&quot;&gt;IoU threshold&lt;/a&gt;. The SSD version of the matcher has an extra step to ensure that each ground truth is matched with the default box that has the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L356-L360&quot;&gt;highest overlap&lt;/a&gt;. The results of the matcher are used in the loss estimation during the training process of the model.&lt;/p&gt;

&lt;h3 id=&quot;classification-and-regression-heads&quot;&gt;Classification and Regression Heads&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L38&quot;&gt;SSDHead class&lt;/a&gt; is responsible for initializing the Classification and Regression parts of the network. Here are a few notable details about their code:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Both the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L90&quot;&gt;Classification&lt;/a&gt; and the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L99&quot;&gt;Regression&lt;/a&gt; head inherit from the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L51&quot;&gt;same class&lt;/a&gt; which is responsible for making the predictions for each feature map.&lt;/li&gt;
  &lt;li&gt;Each level of the feature map uses a separate 3x3 Convolution to estimate the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L92-L94&quot;&gt;class logits&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L101-L103&quot;&gt;box locations&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L79&quot;&gt;number of predictions&lt;/a&gt; that each head makes per level depends on the number of default boxes and the sizes of the feature maps.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;backbone-feature-extractor&quot;&gt;Backbone Feature Extractor&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L413&quot;&gt;feature extractor&lt;/a&gt; reconfigures and enhances a standard VGG backbone with extra layers as depicted on the Figure 2 of the SSD paper:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/feature extractor.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The class supports all VGG models of TorchVision and one can create a similar extractor class for other types of CNNs (see &lt;a href=&quot;https://github.com/pytorch/vision/blob/644bdcdc438c1723714950d0771da76333b53954/torchvision/models/detection/ssd.py#L600&quot;&gt;this example for ResNet&lt;/a&gt;). Here are a few implementation details of the class:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L419-L420&quot;&gt;Patching&lt;/a&gt; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ceil_mode parameter&lt;/code&gt; of the 3rd Maxpool layer is necessary to get the same feature map sizes as the paper. This is due to small differences between PyTorch and the original Caffe implementation of the model.&lt;/li&gt;
  &lt;li&gt;It adds a series of &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L430-L456&quot;&gt;extra feature layers&lt;/a&gt;on top of VGG. If the highres parameter is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt; during its construction, it will append an &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L457-L464&quot;&gt;extra convolution&lt;/a&gt;. This is useful for the SSD512 version of the model.&lt;/li&gt;
  &lt;li&gt;As discussed on section 3 of the paper, the fully connected layers of the original VGG are converted to convolutions with the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L469&quot;&gt;first one using Atrous&lt;/a&gt;. Moreover maxpool5’s stride and kernel size is &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L468&quot;&gt;modified&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;As described on section 3.1, L2 normalization is used on the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L484&quot;&gt;output of conv4_3&lt;/a&gt; and a set of &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L422-L423&quot;&gt;learnable weights&lt;/a&gt; are introduced to control its scaling.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ssd-algorithm&quot;&gt;SSD Algorithm&lt;/h3&gt;

&lt;p&gt;The final key piece of the implementation is on the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L108&quot;&gt;SSD class&lt;/a&gt;. Here are some notable details:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The algorithm is &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L167-L176&quot;&gt;parameterized&lt;/a&gt; by a set of arguments similar to other detection models. The mandatory parameters are: the backbone which is responsible for &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L137-L139&quot;&gt;estimating the feature maps&lt;/a&gt;, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anchor_generator&lt;/code&gt; which should be a &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L140-L141&quot;&gt;configured instance&lt;/a&gt; of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DefaultBoxGenerator&lt;/code&gt; class, the size to which the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L142-L143&quot;&gt;input images&lt;/a&gt; will be resized and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_classes&lt;/code&gt; for classification &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L144&quot;&gt;excluding&lt;/a&gt; the background.&lt;/li&gt;
  &lt;li&gt;If a &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L150-L151&quot;&gt;head&lt;/a&gt; is not provided, the constructor will &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L194&quot;&gt;initialize&lt;/a&gt; the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SSDHead&lt;/code&gt;. To do so, we need to know the number of output channels for each feature map produced by the backbone. Initially we try to &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L186&quot;&gt;retrieve this information&lt;/a&gt; from the backbone but if not available we will &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L189&quot;&gt;dynamically estimate it&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The algorithm &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L183&quot;&gt;reuses&lt;/a&gt; the standard &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L129&quot;&gt;BoxCoder class&lt;/a&gt; used by other Detection models. The class is responsible for &lt;a href=&quot;https://leimao.github.io/blog/Bounding-Box-Encoding-Decoding/&quot;&gt;encoding and decoding&lt;/a&gt; the bounding boxes and is configured to use the same prior variances as the &lt;a href=&quot;https://github.com/weiliu89/caffe/blob/2c4e4c2899ad7c3a997afef2c1fbac76adca1bad/examples/ssd/ssd_coco.py#L326&quot;&gt;original implementation&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Though we reuse the standard &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/transform.py#L64&quot;&gt;GeneralizedRCNNTransform class&lt;/a&gt; to resize and normalize the input images, the SSD algorithm &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L203-L204&quot;&gt;configures&lt;/a&gt; it to ensure that the image size will remain fixed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are the two core methods of the implementation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_loss&lt;/code&gt; method estimates the standard Multi-box loss as described on page 5 of the SSD paper. It uses the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L244&quot;&gt;smooth L1 loss&lt;/a&gt; for regression and the standard &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L262-L266&quot;&gt;cross-entropy loss&lt;/a&gt; with &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L268-L276&quot;&gt;hard-negative sampling&lt;/a&gt; for classification.&lt;/li&gt;
  &lt;li&gt;As in all detection models, the forward method currently has different behaviour depending on whether the model is on training or eval mode. It starts by &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L309-L310&quot;&gt;resizing &amp;amp; normalizing the input images&lt;/a&gt; and then &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L324-L325&quot;&gt;passes them through the backbone&lt;/a&gt; to get the feature maps. The feature maps are then &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L331-L332&quot;&gt;passed through the head&lt;/a&gt; to get the predictions and then the method &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L334-L335&quot;&gt;generates the default boxes&lt;/a&gt;.
    &lt;ul&gt;
      &lt;li&gt;If the model is on &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L339-L352&quot;&gt;training mode&lt;/a&gt;, the forward will estimate the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L349&quot;&gt;IoUs of the default boxes with the ground truth&lt;/a&gt;, use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SSDmatcher&lt;/code&gt; to &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L350&quot;&gt;produce matches&lt;/a&gt; and finally &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L352&quot;&gt;estimate the losses&lt;/a&gt; by calling the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_loss method&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;If the model is on &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L353-L355&quot;&gt;eval mode&lt;/a&gt;, we first select the best detections by keeping only the ones that &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L384&quot;&gt;pass the score threshold&lt;/a&gt;, select the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L388-L391&quot;&gt;most promising boxes&lt;/a&gt; and run NMS to &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L401-L403&quot;&gt;clean up and select&lt;/a&gt; the best predictions. Finally we &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L355&quot;&gt;postprocess the predictions&lt;/a&gt; to resize them to the original image size.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;the-ssd300-vgg16-model&quot;&gt;The SSD300 VGG16 Model&lt;/h1&gt;

&lt;p&gt;The SSD is a family of models because it can be configured with different backbones and different Head configurations. In this section, we will focus on the provided &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L522-L523&quot;&gt;SSD pre-trained model&lt;/a&gt;. We will discuss the details of its configuration and the training process used to reproduce the reported results.&lt;/p&gt;

&lt;h3 id=&quot;training-process&quot;&gt;Training process&lt;/h3&gt;

&lt;p&gt;The model was trained using the COCO dataset and all of its hyper-parameters and scripts can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/blob/e35793a1a4000db1f9f99673437c514e24e65451/references/detection/README.md#ssd300-vgg16&quot;&gt;references&lt;/a&gt; folder. Below we provide details on the most notable aspects of the training process.&lt;/p&gt;

&lt;h3 id=&quot;paper-hyperparameters&quot;&gt;Paper Hyperparameters&lt;/h3&gt;

&lt;p&gt;In order to achieve the best possible results on COCO, we adopted the hyperparameters described on the section 3 of the paper concerning the optimizer configuration, the weight regularization etc. Moreover we found it useful to adopt the optimizations that appear in the &lt;a href=&quot;https://github.com/weiliu89/caffe/blob/ssd/examples/ssd/ssd_coco.py#L310-L321&quot;&gt;official implementation&lt;/a&gt; concerning the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L579-L581&quot;&gt;tiling configuration&lt;/a&gt; of the DefaultBox generator. This optimization was not described in the paper but it was crucial for improving the detection precision of smaller objects.&lt;/p&gt;

&lt;h3 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h3&gt;

&lt;p&gt;Implementing the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/references/detection/transforms.py#L20-L239&quot;&gt;SSD Data Augmentation strategy&lt;/a&gt; as described on page 6 and page 12 of the paper was critical to reproducing the results. More specifically the use of random “Zoom In” and “Zoom Out” transformations make the model robust to various input sizes and improve its precision on the small and medium objects. Finally since the VGG16 has quite a few parameters, the photometric distortions &lt;a href=&quot;https://github.com/pytorch/vision/blob/43d772067fe77965ec8fc49c799de5cea44b8aa2/references/detection/presets.py#L11-L18&quot;&gt;included in the augmentations&lt;/a&gt; have a regularization effect and help avoid the overfitting.&lt;/p&gt;

&lt;h3 id=&quot;weight-initialization--input-scaling&quot;&gt;Weight Initialization &amp;amp; Input Scaling&lt;/h3&gt;

&lt;p&gt;Another aspect that we found beneficial was to follow the &lt;a href=&quot;https://github.com/intel/caffe/blob/master/models/intel_optimized_models/ssd/VGGNet/coco/SSD_300x300/train.prototxt&quot;&gt;weight initialization scheme&lt;/a&gt; proposed by the paper. To do that, we had to adapt our input scaling method by &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L583-L587&quot;&gt;undoing the 0-1 scaling&lt;/a&gt; performed by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ToTensor()&lt;/code&gt; and use &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L24-L26&quot;&gt;pre-trained ImageNet weights&lt;/a&gt; fitted with this scaling (shoutout to &lt;a href=&quot;https://github.com/amdegroot&quot;&gt;Max deGroot&lt;/a&gt; for providing them in his repo). All the weights of new convolutions were &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L30-L35&quot;&gt;initialized using Xavier&lt;/a&gt; and their biases were set to zero. After initialization, the network was &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L571-L572&quot;&gt;trained end-to-end&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;lr-scheme&quot;&gt;LR Scheme&lt;/h3&gt;

&lt;p&gt;As reported on the paper, after applying aggressive data augmentations it’s necessary to train the models for longer. Our experiments confirm this and we had to tweak the Learning rate, batch sizes and overall steps to achieve the best results. Our &lt;a href=&quot;https://github.com/pytorch/vision/blob/e35793a1a4000db1f9f99673437c514e24e65451/references/detection/README.md#ssd300-vgg16&quot;&gt;proposed learning scheme&lt;/a&gt; is configured to be rather on the safe side, showed signs of plateauing between the steps and thus one is likely to be able to train a similar model by doing only 66% of our epochs.&lt;/p&gt;

&lt;h1 id=&quot;breakdown-of-key-accuracy-improvements&quot;&gt;Breakdown of Key Accuracy Improvements&lt;/h1&gt;

&lt;p&gt;It is important to note that implementing a model directly from a paper is an iterative process that circles between coding, training, bug fixing and adapting the configuration until we match the accuracies reported on the paper. Quite often it also involves simplifying the training recipe or enhancing it with more recent methodologies. It is definitely not a linear process where incremental accuracy improvements are achieved by improving a single direction at a time but instead involves exploring different hypothesis, making incremental improvements in different aspects and doing a lot of backtracking.&lt;/p&gt;

&lt;p&gt;With that in mind, below we try to summarize the optimizations that affected our accuracy the most. We did this by grouping together the various experiments in 4 main groups and attributing the experiment improvements to the closest match. Note that the Y-axis of the graph starts from 18 instead from 0 to make the difference between optimizations more visible:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/Key optimizations for improving the mAP of SSD300 VGG16.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model Configuration&lt;/th&gt;
      &lt;th&gt;mAP delta&lt;/th&gt;
      &lt;th&gt;mAP&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Baseline with “FasterRCNN-style” Hyperparams&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;19.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Paper Hyperparams&lt;/td&gt;
      &lt;td&gt;1.6&lt;/td&gt;
      &lt;td&gt;21.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Data Augmentation&lt;/td&gt;
      &lt;td&gt;1.8&lt;/td&gt;
      &lt;td&gt;22.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Weight Initialization &amp;amp; Input Scaling&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;23.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ LR scheme&lt;/td&gt;
      &lt;td&gt;1.2&lt;/td&gt;
      &lt;td&gt;25.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Our final model achieves an mAP of 25.1 and reproduces exactly the COCO results reported on the paper. Here is a &lt;a href=&quot;https://github.com/pytorch/vision/pull/3403&quot;&gt;detailed breakdown&lt;/a&gt; of the accuracy metrics.&lt;/p&gt;

&lt;p&gt;We hope you found the part 1 of the series interesting. On the part 2, we will focus on the implementation of SSDlite and discuss its differences from SSD. Until then, we are looking forward to your feedback.&lt;/p&gt;</content><author><name>Vasilis Vryniotis</name></author><summary type="html">In TorchVision v0.10, we’ve released two new Object Detection models based on the SSD architecture. Our plan is to cover the key implementation details of the algorithms along with information on how they were trained in a two-part article.</summary></entry><entry><title type="html">New PyTorch Library Releases in PyTorch 1.9, including TorchVision, TorchAudio, and more</title><link href="https://pytorch.org/blog/pytorch-1.9-new-library-releases/" rel="alternate" type="text/html" title="New PyTorch Library Releases in PyTorch 1.9, including TorchVision, TorchAudio, and more" /><published>2021-06-15T00:00:00-07:00</published><updated>2021-06-15T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.9-new-library-releases</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.9-new-library-releases/">&lt;p&gt;Today, we are announcing updates to a number of PyTorch libraries, alongside the &lt;a href=&quot;https://pytorch.org/blog/pytorch-1.9-released/&quot;&gt;PyTorch 1.9 release&lt;/a&gt;. The updates include new releases for the domain libraries including TorchVision, TorchText and TorchAudio. These releases, along with the PyTorch 1.9 release, include a number of new features and improvements that will provide a broad set of updates for the PyTorch community.&lt;/p&gt;

&lt;p&gt;Some highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;TorchVision&lt;/strong&gt; - Added new SSD and SSDLite models, quantized kernels for object detection, GPU Jpeg decoding, and iOS support. See &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;release notes&lt;/a&gt; here.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchAudio&lt;/strong&gt; - Added wav2vec 2.0 model deployable in non-Python environments (including C++, Android, and iOS). Many performance improvements in lfilter, spectral operations, resampling. Added options for quality control in sampling (i.e. Kaiser window support). Initiated the migration of complex tensors operations. Improved autograd support. See &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;release notes&lt;/a&gt; here.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchText&lt;/strong&gt; - Added a new high-performance Vocab module that provides common functional APIs for NLP workflows. See &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;release notes&lt;/a&gt; here.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’d like to thank the community for their support and work on this latest release.&lt;/p&gt;

&lt;p&gt;Features in PyTorch releases are classified as Stable, Beta, and Prototype. You can learn more about the definitions in &lt;a href=&quot;https://pytorch.org/blog/pytorch-feature-classification-changes/&quot;&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;torchvision-010&quot;&gt;TorchVision 0.10&lt;/h1&gt;

&lt;h3 id=&quot;stable-quantized-kernels-for-object-detection&quot;&gt;(Stable) Quantized kernels for object detection&lt;/h3&gt;
&lt;p&gt;The forward pass of the nms and roi_align operators now support tensors with a quantized dtype, which can help lower the memory footprint of object detection models, particularly on mobile environments. For more details, refer to &lt;a href=&quot;https://pytorch.org/vision/stable/ops.html#torchvision.ops.roi_align&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stable-speed-optimizations-for-tensor-transforms&quot;&gt;(Stable) Speed optimizations for Tensor transforms&lt;/h3&gt;
&lt;p&gt;The resize and flip transforms have been optimized and its runtime improved by up to 5x on the CPU.&lt;/p&gt;

&lt;h3 id=&quot;stable-documentation-improvements&quot;&gt;(Stable) Documentation improvements&lt;/h3&gt;
&lt;p&gt;Significant improvements were made to the documentation. In particular, a new gallery of examples is available. These examples visually illustrate how each transform acts on an image, and also properly documents and illustrates the output of the segmentation models.&lt;/p&gt;

&lt;p&gt;The example gallery will be extended in the future to provide more comprehensive examples and serve as a reference for common torchvision tasks. For more details, refer to &lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/index.html&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-new-models-for-detection&quot;&gt;(Beta) New models for detection&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot;&gt;SSD&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;SSDlite&lt;/a&gt; are two popular object detection architectures that are efficient in terms of speed and provide good results for low resolution pictures. In this release, we provide implementations for the original SSD model with VGG16 backbone and for its mobile-friendly variant SSDlite with MobileNetV3-Large backbone.&lt;/p&gt;

&lt;p&gt;The models were pre-trained on COCO train2017 and can be used as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Original SSD variant
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssd300_vgg16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Mobile-friendly SSDlite variant
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;320&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;320&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssdlite320_mobilenet_v3_large&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following accuracies can be obtained on COCO val2017 (full results available in &lt;a href=&quot;https://github.com/pytorch/vision/pull/3403&quot;&gt;#3403&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/vision/pull/3757&quot;&gt;#3757&lt;/a&gt;):&lt;/p&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;mAP&lt;/th&gt;
      &lt;th&gt;mAP@50&lt;/th&gt;
      &lt;th&gt;mAP@75&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;SSD300 VGG16&lt;/td&gt;
      &lt;td&gt;25.1&lt;/td&gt;
      &lt;td&gt;41.5&lt;/td&gt;
      &lt;td&gt;26.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SSDlite320 MobileNetV3-Large&lt;/td&gt;
      &lt;td&gt;21.3&lt;/td&gt;
      &lt;td&gt;34.3&lt;/td&gt;
      &lt;td&gt;22.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;For more details, refer to &lt;a href=&quot;https://pytorch.org/vision/stable/models.html#id37&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-jpeg-decoding-on-the-gpu&quot;&gt;(Beta) JPEG decoding on the GPU&lt;/h3&gt;
&lt;p&gt;Decoding jpegs is now possible on GPUs with the use of &lt;a href=&quot;https://developer.nvidia.com/nvjpeg&quot;&gt;nvjpeg&lt;/a&gt;, which should be readily available in your CUDA setup. The decoding time of a single image should be about 2 to 3 times faster than with libjpeg on CPU. While the resulting tensor will be stored on the GPU device, the input raw tensor still needs to reside on the host (CPU), because the first stages of the decoding process take place on the host:
from torchvision.io.image import read_file, decode_jpeg&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'path_to_image.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# raw data is on CPU
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decode_jpeg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# decoded image in on GPU
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For more details, see &lt;a href=&quot;https://pytorch.org/vision/stable/io.html#torchvision.io.decode_jpeg&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-ios-support&quot;&gt;(Beta) iOS support&lt;/h3&gt;
&lt;p&gt;TorchVision 0.10 now provides pre-compiled iOS binaries for its C++ operators, which means you can run Faster R-CNN and Mask R-CNN on iOS. An example app on how to build a program leveraging those ops can be found &lt;a href=&quot;https://github.com/pytorch/vision/tree/master/ios/VisionTestApp&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;torchaudio-090&quot;&gt;TorchAudio 0.9.0&lt;/h1&gt;

&lt;h3 id=&quot;stable-complex-tensor-migration&quot;&gt;(Stable) Complex Tensor Migration&lt;/h3&gt;
&lt;p&gt;TorchAudio has functions that handle complex-valued tensors. These functions follow a convention to use an extra dimension to represent real and imaginary parts. In PyTorch 1.6, the native complex type was introduced. As its API is getting stable, torchaudio has started to migrate to the native complex type.&lt;/p&gt;

&lt;p&gt;In this release, we added support for native complex tensors, and you can opt-in to use them. Using the native complex types, we have verified that affected functions continue to support autograd and TorchScript, moreover, switching to native complex types improves their performance. For more details, refer to &lt;a href=&quot;https://github.com/pytorch/audio/issues/1337&quot;&gt;pytorch/audio#1337&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stable-filtering-improvement&quot;&gt;(Stable) Filtering Improvement&lt;/h3&gt;
&lt;p&gt;In release 0.8, we added the C++ implementation of the core part of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lfilter&lt;/code&gt; for CPU, which improved the performance. In this release, we optimized some internal operations of the CPU implementation for further performance improvement. We also added autograd support to both CPU and GPU. Now &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lfilter&lt;/code&gt; and all the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;biquad&lt;/code&gt; filters (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;band_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bass_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;treble_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;allpass_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lowpass_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;highpass_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bandpass_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;equalizer_biquad&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bandrefect_biquad&lt;/code&gt;) benefit from the performance improvement and support autograd. We also moved the implementation of overdrive to C++ for performance improvement. For more details, refer to &lt;a href=&quot;https://pytorch.org/audio/0.9.0/functional.html#lfilter&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stable-improved-autograd-support&quot;&gt;(Stable) Improved Autograd Support&lt;/h3&gt;
&lt;p&gt;Along with the work of Complex Tensor Migration and Filtering Improvement, we also added autograd tests to transforms. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lfilter&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;biquad&lt;/code&gt; and its variants, and most transforms are now guaranteed to support autograd. For more details, refer to &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;the release note&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stable-improved-windows-support&quot;&gt;(Stable) Improved Windows Support&lt;/h3&gt;
&lt;p&gt;Torchaudio implements some operations in C++ for reasons such as performance and integration with third-party libraries. These C++ components were only available on Linux and macOS. In this release, we have added support to Windows. With this, the efficient filtering implementation mentioned above is also available on Windows.&lt;/p&gt;

&lt;p&gt;However, please note that not all the C++ components are available for Windows. “sox_io” backend and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.functional.compute_kaldi_pitch&lt;/code&gt; are not supported.&lt;/p&gt;

&lt;h3 id=&quot;stable-io-functions-migration&quot;&gt;(Stable) I/O Functions Migration&lt;/h3&gt;
&lt;p&gt;Since the 0.6 release, we have continuously improved I/O functionality. Specifically, in 0.8 we changed the default backend from “sox” to “sox_io” and applied the same switch to API of the “soundfile” backend. The 0.9 release concludes this migration by removing the deprecated backends. For more details, please refer to &lt;a href=&quot;https://github.com/pytorch/audio/issues/903&quot;&gt;#903&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-wav2vec20-model&quot;&gt;(Beta) Wav2Vec2.0 Model&lt;/h3&gt;
&lt;p&gt;We have added the model architectures from &lt;a href=&quot;https://arxiv.org/abs/2006.11477&quot;&gt;Wav2Vec2.0&lt;/a&gt;. You can import fine-tuned models parameters published on &lt;a href=&quot;https://github.com/pytorch/fairseq/tree/master/examples/wav2vec&quot;&gt;fairseq&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/models?filter=wav2vec2&quot;&gt;Hugging Face Hub&lt;/a&gt;. Our model definition supports TorchScript, and it is possible to deploy the model to non-Python environments, such as C++, &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/SpeechRecognition&quot;&gt;Android&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/SpeechRecognition&quot;&gt;iOS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The following code snippet illustrates such a use case. Please check out our &lt;a href=&quot;https://github.com/pytorch/audio/tree/master/examples/libtorchaudio&quot;&gt;c++ example directory&lt;/a&gt; for the complete example. Currently, it is designed for running inference. If you would like more support for training, please file a feature request.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Import fine-tuned model from Hugging Face Hub
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchaudio.models.wav2vec2.utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import_huggingface_model&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;original&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Wav2Vec2ForCTC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;facebook/wav2vec2-base-960h&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imported&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import_huggingface_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Import fine-tuned model from fairseq
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fairseq&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchaudio.models.wav2vec2.utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import_fairseq_model&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fairseq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint_utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_model_ensemble_and_task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;wav2vec_small_960h.pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg_overrides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;data_dir&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imported&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import_fairseq_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2v_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Build uninitialized model and load state dict
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchaudio.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wav2vec2_base&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wav2vec2_base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imported&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Quantize / script / optimize for mobile
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantized_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantize_dynamic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qconfig_spec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qint8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scripted_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantized_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimized_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimize_for_mobile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scripted_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimized_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;model_for_deployment.pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more details, see &lt;a href=&quot;https://pytorch.org/audio/0.9.0/models.html#wav2vec2-0&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-resampling-improvement&quot;&gt;(Beta) Resampling Improvement&lt;/h3&gt;
&lt;p&gt;In release 0.8, we vectorized the operation in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.compliance.kaldi.resample_waveform&lt;/code&gt;, which improved the performance of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resample_waveform&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.transforms.Resample&lt;/code&gt;. In this release, we have further revised the way the resampling algorithm is implemented.&lt;/p&gt;

&lt;p&gt;We have:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Added Kaiser Window support for a wider range of resampling quality.&lt;/li&gt;
  &lt;li&gt;Added &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rolloff&lt;/code&gt; parameter for anti-aliasing control.&lt;/li&gt;
  &lt;li&gt;Added the mechanism to precompute the kernel and cache it in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.transforms.Resample&lt;/code&gt; for even faster operation.&lt;/li&gt;
  &lt;li&gt;Moved the implementation from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.compliance.kaldi.resample_waveform&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.functional.resample&lt;/code&gt; and deprecated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.compliance.kaldi.resample_waveform&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more details, see &lt;a href=&quot;https://pytorch.org/audio/0.9.0/transforms.html#resample&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-rnn-transducer-loss&quot;&gt;(Prototype) RNN Transducer Loss&lt;/h3&gt;
&lt;p&gt;The RNN transducer loss is used in training RNN transducer models, which is a popular architecture for speech recognition tasks. The prototype loss in torchaudio currently supports autograd, torchscript, float16 and float32, and can also be run on both CPU and CUDA. For more details, please refer to &lt;a href=&quot;https://pytorch.org/audio/master/rnnt_loss.html&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;torchtext-0100&quot;&gt;TorchText 0.10.0&lt;/h1&gt;

&lt;h3 id=&quot;beta-new-vocab-module&quot;&gt;(Beta) New Vocab Module&lt;/h3&gt;
&lt;p&gt;In this release, we introduce a new Vocab module that replaces the current Vocab class. The new Vocab provides common functional APIs for NLP workflows. This module is backed by an efficient C++ implementation that reduces batch look-up time by up-to ~85% (refer to summary of &lt;a href=&quot;https://github.com/pytorch/text/pull/1248&quot;&gt;#1248&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/text/pull/1290&quot;&gt;#1290&lt;/a&gt; for further information on benchmarks), and provides support for TorchScript. We provide accompanying factory functions that can be used to build the Vocab object either through a python ordered dictionary or an Iterator that yields lists of tokens.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#creating Vocab from text file
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;io&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchtext.vocab&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_vocab_from_iterator&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#generator that yield list of tokens
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;yield_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'utf-8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#get Vocab object
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_vocab_from_iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yield_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;specials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;unk&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#creating Vocab through ordered dict
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchtext.vocab&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OrderedDict&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;counter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sorted_by_freq_tuples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ordered_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OrderedDict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sorted_by_freq_tuples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ordered_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#common API usage
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#look-up index
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#batch look-up indices
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;looup_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#support forward API of PyTorch nn Modules
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#batch look-up tokens
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lookup_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#set default index to return when token not found
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_default_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;out_of_vocabulary&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#prints 0
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more details, refer to &lt;a href=&quot;https://pytorch.org/text/stable/vocab.html&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks for reading. If you’re interested in these updates and want to join the PyTorch community, we encourage you to join &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;the discussion&lt;/a&gt; forums and &lt;a href=&quot;https://github.com/pytorch/pytorch/issues&quot;&gt;open GitHub issues&lt;/a&gt;. To get the latest news from PyTorch, follow us on &lt;a href=&quot;https://www.facebook.com/pytorch/&quot;&gt;Facebook&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/PyTorch&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;https://medium.com/pytorch&quot;&gt;Medium&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/pytorch&quot;&gt;YouTube&lt;/a&gt; or &lt;a href=&quot;https://www.linkedin.com/company/pytorch&quot;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;-Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we are announcing updates to a number of PyTorch libraries, alongside the PyTorch 1.9 release. The updates include new releases for the domain libraries including TorchVision, TorchText and TorchAudio. These releases, along with the PyTorch 1.9 release, include a number of new features and improvements that will provide a broad set of updates for the PyTorch community.</summary></entry><entry><title type="html">PyTorch 1.9 Release, including torch.linalg and Mobile Interpreter</title><link href="https://pytorch.org/blog/pytorch-1.9-released/" rel="alternate" type="text/html" title="PyTorch 1.9 Release, including torch.linalg and Mobile Interpreter" /><published>2021-06-15T00:00:00-07:00</published><updated>2021-06-15T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.9-released</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.9-released/">&lt;p&gt;We are excited to announce the release of PyTorch 1.9. The release is composed of more than 3,400 commits since 1.8, made by 398 contributors. The release notes are available &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;. Highlights include:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Major improvements to support scientific computing, including &lt;em&gt;torch.linalg&lt;/em&gt;, &lt;em&gt;torch.special&lt;/em&gt;, and Complex Autograd&lt;/li&gt;
  &lt;li&gt;Major improvements in on-device binary size with Mobile Interpreter&lt;/li&gt;
  &lt;li&gt;Native support for elastic-fault tolerance training through the upstreaming of TorchElastic into PyTorch Core&lt;/li&gt;
  &lt;li&gt;Major updates to the PyTorch RPC framework to support large scale distributed training with GPU support&lt;/li&gt;
  &lt;li&gt;New APIs to optimize performance and packaging for model inference deployment&lt;/li&gt;
  &lt;li&gt;Support for Distributed training, GPU utilization and SM efficiency in the PyTorch Profiler&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Along with 1.9, we are also releasing major updates to the PyTorch libraries, which you can read about in &lt;a href=&quot;https://pytorch.org/blog/pytorch-1.9-new-library-releases/&quot;&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We’d like to thank the community for their support and work on this latest release. We’d especially like to thank Quansight and Microsoft for their contributions.&lt;/p&gt;

&lt;p&gt;Features in PyTorch releases are classified as Stable, Beta, and Prototype. You can learn more about the definitions in &lt;a href=&quot;https://pytorch.org/blog/pytorch-feature-classification-changes/&quot;&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;frontend-apis&quot;&gt;Frontend APIs&lt;/h1&gt;

&lt;h3 id=&quot;stable-torchlinalg&quot;&gt;(Stable) &lt;em&gt;torch.linalg&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;In 1.9, the &lt;em&gt;torch.linalg&lt;/em&gt; module is moving to a stable release. Linear algebra is essential to deep learning and scientific computing, and the &lt;em&gt;torch.linalg&lt;/em&gt; module extends PyTorch’s support for it with implementations of every function from &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.linalg.html&quot;&gt;NumPy’s linear algebra module&lt;/a&gt; (now with support for accelerators and autograd) and more, like &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.linalg.matrix_norm.html?highlight=matrix_norm#torch.linalg.matrix_norm&quot;&gt;&lt;em&gt;torch.linalg.matrix_norm&lt;/em&gt;&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.linalg.householder_product.html?highlight=householder_product#torch.linalg.householder_product&quot;&gt;&lt;em&gt;torch.linalg.householder_product&lt;/em&gt;&lt;/a&gt;. This makes the module immediately familiar to users who have worked with NumPy. Refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/linalg.html?highlight=linalg#module-torch.linalg&quot;&gt;the documentation&lt;/a&gt; here.&lt;/p&gt;

&lt;p&gt;We plan to publish another blog post with more details on the &lt;em&gt;torch.linalg&lt;/em&gt; module next week!&lt;/p&gt;

&lt;h3 id=&quot;stable-complex-autograd&quot;&gt;(Stable) Complex Autograd&lt;/h3&gt;

&lt;p&gt;The Complex Autograd feature, released as a beta in PyTorch 1.8, is now stable. Since the beta release, we have extended support for Complex Autograd for over 98% operators in PyTorch 1.9, improved testing for complex operators by adding more OpInfos, and added greater validation through TorchAudio migration to native complex tensors (refer to &lt;a href=&quot;https://github.com/pytorch/audio/issues/1337&quot;&gt;this issue&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This feature provides users the functionality to calculate complex gradients and optimize real valued loss functions with complex variables. This is a required feature for multiple current and downstream prospective users of complex numbers in PyTorch like TorchAudio, ESPNet, Asteroid, and FastMRI. Refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/notes/autograd.html#autograd-for-complex-numbers&quot;&gt;the documentation&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&quot;stable-torchuse_deterministic_algorithms&quot;&gt;(Stable) torch.use_deterministic_algorithms()&lt;/h3&gt;

&lt;p&gt;To help with debugging and writing reproducible programs, PyTorch 1.9 includes a &lt;em&gt;torch.use_determinstic_algorithms&lt;/em&gt; option. When this setting is enabled, operations will behave deterministically, if possible, or throw a runtime error if they might behave nondeterministically. Here are a couple examples:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sparse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Sparse-dense CUDA bmm is usually nondeterministic
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;use_deterministic_algorithms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Now torch.bmm gives the same result each time, but with reduced performance
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# CUDA kthvalue has no deterministic algorithm, so it throws a runtime error
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kthvalue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;RuntimeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kthvalue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CUDA&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;does&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;have&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;implementation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch 1.9 adds deterministic implementations for a number of indexing operations, too, including &lt;em&gt;index_add&lt;/em&gt;, &lt;em&gt;index_copy&lt;/em&gt;, and &lt;em&gt;index_put with accum=False&lt;/em&gt;. For more details, refer to the &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.use_deterministic_algorithms.html?highlight=use_deterministic#torch.use_deterministic_algorithms&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/1.9.0/notes/randomness.html?highlight=reproducibility&quot;&gt;reproducibility note&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchspecial&quot;&gt;(Beta) &lt;em&gt;torch.special&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;A &lt;em&gt;torch.special&lt;/em&gt; module, analogous to &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/special.html&quot;&gt;SciPy’s special module&lt;/a&gt;, is now available in beta. This module contains many functions useful for scientific computing and working with distributions such as &lt;em&gt;iv&lt;/em&gt;, &lt;em&gt;ive&lt;/em&gt;, &lt;em&gt;erfcx&lt;/em&gt;, &lt;em&gt;logerfc&lt;/em&gt;, and &lt;em&gt;logerfcx&lt;/em&gt;. Refer to &lt;a href=&quot;https://pytorch.org/docs/master/special.html&quot;&gt;the documentation&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&quot;beta-nnmodule-parameterization&quot;&gt;(Beta) nn.Module parameterization&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt; parameterization allows users to parametrize any parameter or buffer of an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt; without modifying the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt; itself. It allows you to constrain the space in which your parameters live without the need for special optimization methods.&lt;/p&gt;

&lt;p&gt;This also contains a new implementation of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spectral_norm&lt;/code&gt; parametrization for PyTorch 1.9. More parametrization will be added to this feature (weight_norm, matrix constraints and part of pruning) for the feature to become stable in 1.10. For more details, refer to the &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.nn.utils.parametrizations.spectral_norm.html?highlight=parametrize&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/parametrizations.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;pytorch-mobile&quot;&gt;PyTorch Mobile&lt;/h1&gt;

&lt;h3 id=&quot;beta-mobile-interpreter&quot;&gt;(Beta) Mobile Interpreter&lt;/h3&gt;

&lt;p&gt;We are releasing Mobile Interpreter, a streamlined version of the PyTorch runtime, in beta. The Interpreter will execute PyTorch programs in edge devices, with reduced binary size footprint.&lt;/p&gt;

&lt;p&gt;Mobile Interpreter is one of the top requested features for PyTorch Mobile. This new release will significantly reduce binary size compared with the current on-device runtime. In order for you to get the binary size improvements with our interpreter (which can reduce the binary size up to ~75% for a typical application) follow these instructions. As an example, using Mobile Interpreter, we can reach 2.6 MB compressed with MobileNetV2 in arm64-v7a Android. With this latest release we are making it much simpler to integrate the interpreter by providing pre-built libraries for iOS and Android.&lt;/p&gt;

&lt;h3 id=&quot;torchvision-library&quot;&gt;TorchVision Library&lt;/h3&gt;

&lt;p&gt;Starting from 1.9, users can use the TorchVision library on their iOS/Android apps. The Torchvision library contains the C++ TorchVision ops and needs to be linked together with the main PyTorch library for iOS, for Android it can be added as a gradle dependency. This allows using TorchVision prebuilt MaskRCNN operators for object detections and segmentation. To learn more about the library, please refer to our tutorials and &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/D2Go&quot;&gt;demo apps&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;demo-apps&quot;&gt;Demo apps&lt;/h3&gt;

&lt;p&gt;We are releasing a new video app based on &lt;a href=&quot;https://pytorchvideo.org/&quot;&gt;PyTorch Video&lt;/a&gt; library and an updated speech recognition app based on the latest torchaudio, wave2vec model. Both are available on &lt;a href=&quot;https://github.com/pytorch/ios-demo-app&quot;&gt;iOS&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/android-demo-app&quot;&gt;Android&lt;/a&gt;. In addition, we have updated the seven Computer Vision and three Natural Language Processing demo apps, including the HuggingFace DistilBERT, and the DeiT vision transformer models, with PyTorch Mobile v1.9. With the addition of these two apps, we now offer a full suite of demo apps covering image, text, audio, and video. To get started check out our &lt;a href=&quot;https://github.com/pytorch/ios-demo-app&quot;&gt;iOS demo apps&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/android-demo-app&quot;&gt;Android demo apps&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/android-demo-app.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h1 id=&quot;distributed-training&quot;&gt;Distributed Training&lt;/h1&gt;

&lt;h3 id=&quot;beta-torchelastic-is-now-part-of-core&quot;&gt;(Beta) TorchElastic is now part of core&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/50621&quot;&gt;TorchElastic&lt;/a&gt;, which was open sourced over a year ago in the &lt;a href=&quot;https://github.com/pytorch/elastic&quot;&gt;pytorch/elastic&lt;/a&gt; github repository, is a runner and coordinator for PyTorch worker processes. Since then, it has been adopted by various distributed torch use-cases: 1) &lt;a href=&quot;https://medium.com/pytorch/training-deepspeech-using-torchelastic-ad013539682&quot;&gt;deepspeech.pytorch&lt;/a&gt; 2) &lt;a href=&quot;https://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html#torchelastic&quot;&gt;pytorch-lightning&lt;/a&gt; 3) &lt;a href=&quot;https://github.com/pytorch/elastic/blob/master/kubernetes/README.md&quot;&gt;Kubernetes CRD&lt;/a&gt;. Now, it is part of PyTorch core.&lt;/p&gt;

&lt;p&gt;As its name suggests, the core function of TorcheElastic is to gracefully handle scaling events. A notable corollary of elasticity is that peer discovery and rank assignment are built into TorchElastic enabling users to run distributed training on preemptible instances without requiring a gang scheduler. As a side note, &lt;a href=&quot;https://etcd.io/&quot;&gt;etcd&lt;/a&gt; used to be a hard dependency of TorchElastic. With the upstream, this is no longer the case since we have added a “standalone” rendezvous based on c10d::Store. For more details, refer to the &lt;a href=&quot;https://pytorch.org/docs/1.9.0/distributed.elastic.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-distributed-training-updates&quot;&gt;(Beta) Distributed Training Updates&lt;/h3&gt;

&lt;p&gt;In addition to TorchElastic, there are a number of beta features available in the distributed package:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;(Beta) CUDA support is available in RPC&lt;/strong&gt;: Compared to CPU RPC and general-purpose RPC frameworks, CUDA RPC is a much more efficient way for P2P Tensor communication. It is built on top of TensorPipe which can automatically choose a communication channel for each Tensor based on Tensor device type and channel availability on both the caller and the callee. Existing TensorPipe channels cover NVLink, InfiniBand, SHM, CMA, TCP, etc. See &lt;a href=&quot;https://pytorch.org/tutorials/recipes/cuda_rpc.html&quot;&gt;this recipe&lt;/a&gt; for how CUDA RPC helps to attain 34x speedup compared to CPU RPC.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;(Beta) ZeroRedundancyOptimizer&lt;/strong&gt;: ZeroRedundancyOptimizer can be used in conjunction with DistributedDataParallel to reduce the size of per-process optimizer states. The idea of ZeroRedundancyOptimizer comes from &lt;a href=&quot;https://github.com/microsoft/DeepSpeed&quot;&gt;DeepSpeed/ZeRO project&lt;/a&gt; and &lt;a href=&quot;https://github.com/marian-nmt/marian-dev&quot;&gt;Marian&lt;/a&gt;, where the optimizer in each process owns a shard of model parameters and their corresponding optimizer states. When running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step()&lt;/code&gt;, each optimizer only updates its own parameters, and then uses collective communication to synchronize updated parameters across all processes. Refer to &lt;a href=&quot;https://pytorch.org/docs/master/distributed.optim.html&quot;&gt;this documentation&lt;/a&gt; and this &lt;a href=&quot;https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html&quot;&gt;tutorial&lt;/a&gt; to learn more.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;(Beta) Support for profiling distributed collectives&lt;/strong&gt;: PyTorch’s profiler tools, &lt;em&gt;torch.profiler&lt;/em&gt; and &lt;em&gt;torch.autograd.profiler&lt;/em&gt;, are able to profile distributed collectives and point to point communication primitives including allreduce, alltoall, allgather, send/recv, etc. This is enabled for all backends supported natively by PyTorch: gloo, mpi, and nccl. This can be used to debug performance issues, analyze traces that contain distributed communication, and gain insight into performance of applications that use distributed training. To learn more, refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/distributed.html#profiling-collective-communication&quot;&gt;this documentation&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;performance-optimization-and-tooling&quot;&gt;Performance Optimization and Tooling&lt;/h1&gt;

&lt;h3 id=&quot;stable-freezing-api&quot;&gt;(Stable) Freezing API&lt;/h3&gt;

&lt;p&gt;Module Freezing is the process of inlining module parameters and attributes values as constants into the TorchScript internal representation. This allows further optimization and specialization of your program, both for TorchScript optimizations and lowering to other backends. It is used by &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/utils/mobile_optimizer.py&quot;&gt;optimize_for_mobile API&lt;/a&gt;, ONNX, and others.&lt;/p&gt;

&lt;p&gt;Freezing is recommended for model deployment. It helps TorchScript JIT optimizations optimize away overhead and bookkeeping that is necessary for training, tuning, or debugging PyTorch models. It enables graph fusions that are not semantically valid on non-frozen graphs - such as fusing Conv-BN. For more details, refer to the &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.jit.freeze.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-pytorch-profiler&quot;&gt;(Beta) PyTorch Profiler&lt;/h3&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/pytorch-profiler.gif&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The new PyTorch Profiler graduates to beta and leverages &lt;a href=&quot;https://github.com/pytorch/kineto/&quot;&gt;Kineto&lt;/a&gt; for GPU profiling, TensorBoard for visualization and is now the standard across our tutorials and documentation.&lt;/p&gt;

&lt;p&gt;PyTorch 1.9 extends support for the new &lt;em&gt;torch.profiler&lt;/em&gt; API to more builds, including Windows and Mac and is recommended in most cases instead of the previous &lt;em&gt;torch.autograd.profiler&lt;/em&gt; API. The new API supports existing profiler features, integrates with CUPTI library (Linux-only) to trace on-device CUDA kernels and provides support for long-running jobs, e.g.:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;trace_handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;self_cuda_time_total&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_limit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;export_chrome_trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/tmp/trace_&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;activities&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ProfilerActivity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ProfilerActivity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CUDA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# schedule argument specifies the iterations on which the profiler is active
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;schedule&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;warmup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# on_trace_ready argument specifies the handler for the traces
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;on_trace_ready&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace_handler&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# profiler will trace iterations 2 and 3, and then 6 and 7 (counting from zero)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;More usage examples can be found on the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html&quot;&gt;profiler recipe page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The PyTorch Profiler Tensorboard plugin has new features for:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Distributed Training summary view with communications overview for NCCL&lt;/li&gt;
  &lt;li&gt;GPU Utilization and SM Efficiency in Trace view and GPU operators view&lt;/li&gt;
  &lt;li&gt;Memory Profiling view&lt;/li&gt;
  &lt;li&gt;Jump to source when launched from Microsoft VSCode&lt;/li&gt;
  &lt;li&gt;Ability for load traces from cloud object storage systems&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;beta-inference-mode-api&quot;&gt;(Beta) Inference Mode API&lt;/h3&gt;

&lt;p&gt;Inference Mode API allows significant speed-up for inference workloads while remaining safe and ensuring no incorrect gradients can ever be computed. It offers the best possible performance when no autograd is required. For more details, refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.inference_mode.html?highlight=inference%20mode#torch.inference_mode&quot;&gt;the documentation for inference mode itself&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/1.9.0/notes/autograd.html#locally-disabling-gradient-computation&quot;&gt;the documentation explaining when to use it and the difference with no_grad mode&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchpackage&quot;&gt;(Beta) &lt;em&gt;torch.package&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;torch.package&lt;/em&gt; is a new way to package PyTorch models in a self-contained, stable format. A package will include both the model’s data (e.g. parameters, buffers) and its code (model architecture). Packaging a model with its full set of Python dependencies, combined with a description of a conda environment with pinned versions, can be used to easily reproduce training. Representing a model in a self-contained artifact will also allow it to be published and transferred throughout a production ML pipeline while retaining the flexibility of a pure-Python representation. For more details, refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/package.html&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-prepare_for_inference&quot;&gt;(Prototype) prepare_for_inference&lt;/h3&gt;

&lt;p&gt;prepare_for_inference is a new prototype feature that takes in a module and performs graph-level optimizations to improve inference performance, depending on the device. It is meant to be a PyTorch-native option that requires minimal changes to user’s workflows. For more details, see &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/jit/_freeze.py#L168&quot;&gt;the documentation&lt;/a&gt; for the Torchscript version &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/jit/_freeze.py#L168&quot;&gt;here&lt;/a&gt; or the FX version &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/fx/experimental/optimization.py#L234&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-profile-directed-typing-in-torchscript&quot;&gt;(Prototype) Profile-directed typing in TorchScript&lt;/h3&gt;

&lt;p&gt;TorchScript has a hard requirement for source code to have type annotations in order for compilation to be successful. For a long time, it was only possible to add missing or incorrect type annotations through trial and error (i.e., by fixing the type-checking errors generated by &lt;em&gt;torch.jit.script&lt;/em&gt; one by one), which was inefficient and time consuming. Now, we have enabled profile directed typing for &lt;em&gt;torch.jit.script&lt;/em&gt; by leveraging existing tools like MonkeyType, which makes the process much easier, faster, and more efficient. For more details, refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/jit.html&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks for reading. If you’re interested in these updates and want to join the PyTorch community, we encourage you to join the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;discussion forums&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/issues&quot;&gt;open GitHub issues&lt;/a&gt;. To get the latest news from PyTorch, follow us on &lt;a href=&quot;https://www.facebook.com/pytorch/&quot;&gt;Facebook&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/PyTorch&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;https://medium.com/pytorch&quot;&gt;Medium&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/pytorch&quot;&gt;YouTube&lt;/a&gt;, or &lt;a href=&quot;https://www.linkedin.com/company/pytorch&quot;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">We are excited to announce the release of PyTorch 1.9. The release is composed of more than 3,400 commits since 1.8, made by 398 contributors. The release notes are available here. Highlights include: Major improvements to support scientific computing, including torch.linalg, torch.special, and Complex Autograd Major improvements in on-device binary size with Mobile Interpreter Native support for elastic-fault tolerance training through the upstreaming of TorchElastic into PyTorch Core Major updates to the PyTorch RPC framework to support large scale distributed training with GPU support New APIs to optimize performance and packaging for model inference deployment Support for Distributed training, GPU utilization and SM efficiency in the PyTorch Profiler</summary></entry><entry><title type="html">Overview of PyTorch Autograd Engine</title><link href="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/" rel="alternate" type="text/html" title="Overview of PyTorch Autograd Engine" /><published>2021-06-08T00:00:00-07:00</published><updated>2021-06-08T00:00:00-07:00</updated><id>https://pytorch.org/blog/overview-of-pytorch-autograd-engine</id><content type="html" xml:base="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/">&lt;p&gt;This blog post is based on PyTorch version 1.8, although it should apply for older versions too, since most of the mechanics have remained constant.&lt;/p&gt;

&lt;p&gt;To help understand the concepts explained here, it is recommended that you read the awesome blog post by &lt;a href=&quot;https://twitter.com/ezyang&quot;&gt;@ezyang&lt;/a&gt;: &lt;a href=&quot;http://blog.ezyang.com/2019/05/pytorch-internals/&quot;&gt;PyTorch internals&lt;/a&gt; if you are not familiar with PyTorch architecture components such as ATen or c10d.&lt;/p&gt;

&lt;h3 id=&quot;what-is-autograd&quot;&gt;What is autograd?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;PyTorch computes the gradient of a function with respect to the inputs by using automatic differentiation. Automatic differentiation is a technique that, given a computational graph, calculates the gradients of the inputs. Automatic differentiation can be performed in two different ways; forward and reverse mode. Forward mode means that we calculate the gradients along with the result of the function, while reverse mode requires us to evaluate the function first, and then we calculate the gradients starting from the output. While both modes have their pros and cons, the reverse mode is the de-facto choice since the number of outputs is smaller than the number of inputs, which allows a much more efficient computation. Check [3] to learn more about this.&lt;/p&gt;

&lt;p&gt;Automatic differentiation relies on a classic calculus formula known as the chain-rule. The chain rule allows us to calculate very complex derivatives by splitting them and recombining them later.&lt;/p&gt;

&lt;p&gt;Formally speaking, given a composite function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(g(x))&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(g(x))&quot; title=&quot;f(g(x))&quot; /&gt;&lt;/a&gt;, we can calculate its derivative as &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial}{\partial&amp;space;x}&amp;space;f(g(x))&amp;space;=&amp;space;f'(g(x))g'(x)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial}{\partial&amp;space;x}&amp;space;f(g(x))&amp;space;=&amp;space;f'(g(x))g'(x)&quot; title=&quot;\frac{\partial}{\partial x} f(g(x)) = f'(g(x))g'(x)&quot; /&gt;&lt;/a&gt;. This result is what makes automatic differentiation work.
By combining the derivatives of the simpler functions that compose a larger one, such as a neural network, it is possible to compute the exact value of the gradient at a given point rather than relying on the numerical approximation, which would require multiple perturbations in the input to obtain a value.&lt;/p&gt;

&lt;p&gt;To get the intuition of how the reverse mode works, let’s look at a simple function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(x,&amp;space;y)&amp;space;=&amp;space;log(x*y)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x,&amp;space;y)&amp;space;=&amp;space;log(x*y)&quot; title=&quot;f(x, y) = log(x*y)&quot; /&gt;&lt;/a&gt;. Figure 1 shows its computational graph where the inputs x, y in the left, flow through a series of operations to generate the output z.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/f_x_y_graph.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 1: Computational graph of f(x, y) = log(x*y)&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The automatic differentiation engine will normally execute this graph. It will also extend it to calculate the derivatives of w with respect to the inputs x, y, and the intermediate result v.&lt;/p&gt;

&lt;p&gt;The example function can be decomposed in f and g, where &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(x,&amp;space;y)&amp;space;=&amp;space;log(g(x,&amp;space;y))&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x,&amp;space;y)&amp;space;=&amp;space;log(g(x,&amp;space;y))&quot; title=&quot;f(x, y) = log(g(x, y))&quot; /&gt;&lt;/a&gt; and &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=g(x,&amp;space;y)&amp;space;=&amp;space;xy&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?g(x,&amp;space;y)&amp;space;=&amp;space;xy&quot; title=&quot;g(x, y) = xy&quot; /&gt;&lt;/a&gt;.  Every time the engine executes an operation in the graph, the derivative of that operation is added to the graph to be executed later in the backward pass. Note, that the engine knows the derivatives of the basic functions.&lt;/p&gt;

&lt;p&gt;In the example above, when multiplying x and y to obtain v, the engine will extend the graph to calculate the partial derivatives of the multiplication by using the multiplication derivative definition that it already knows. &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial}{\partial&amp;space;x}&amp;space;g(x,&amp;space;y)&amp;space;=&amp;space;y&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial}{\partial&amp;space;x}&amp;space;g(x,&amp;space;y)&amp;space;=&amp;space;y&quot; title=&quot;\frac{\partial}{\partial x} g(x, y) = y&quot; /&gt;&lt;/a&gt; and &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial}{\partial&amp;space;y}&amp;space;g(x,&amp;space;y)&amp;space;=&amp;space;x&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial}{\partial&amp;space;y}&amp;space;g(x,&amp;space;y)&amp;space;=&amp;space;x&quot; title=&quot;\frac{\partial}{\partial y} g(x, y) = x&quot; /&gt;&lt;/a&gt; . The resulting extended graph is shown in Figure 2, where the &lt;em&gt;MultDerivative&lt;/em&gt; node also calculates the product of the resulting gradients by an input gradient to apply the chain rule; this will be explicitly seen in the following operations. Note that the backward graph (green nodes) will not be executed until all the forward steps are completed.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/multi_derivative_graph.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 2: Computational graph extended after executing the logarithm&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Continuing, the engine now calculates the &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=log(v)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?log(v)&quot; title=&quot;log(v)&quot; /&gt;&lt;/a&gt; operation and extends the graph again with the log derivative that it knows to be &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{1}{v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{1}{v}&quot; title=&quot;\frac{1}{v}&quot; /&gt;&lt;/a&gt;. This is shown in figure 3. This operation generates the result &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;w}{\partial&amp;space;v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;w}{\partial&amp;space;v}&quot; title=&quot;\frac{\partial w}{\partial v}&quot; /&gt;&lt;/a&gt; that when propagated backward and multiplied by the multiplication derivative as in the chain rule, generates the derivatives &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;w}{\partial&amp;space;x}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;w}{\partial&amp;space;x}&quot; title=&quot;\frac{\partial w}{\partial x}&quot; /&gt;&lt;/a&gt;, &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;w}{\partial&amp;space;x}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;w}{\partial&amp;space;x}&quot; title=&quot;\frac{\partial w}{\partial x}&quot; /&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/extended_computational_graph.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 3: Computational graph extended after executing the logarithm&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The original computation graph is extended with a new dummy variable z that is the same w. The derivative of z with respect to w is 1 as they are the same variable, this trick allows us to apply the chain rule to calculate the derivatives of the inputs. After the forward pass is complete, we start the backward pass, by supplying the initial value of 1.0 for &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;w}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;w}&quot; title=&quot;\frac{\partial z}{\partial w}&quot; /&gt;&lt;/a&gt;. This is shown in Figure 4.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/computational_graph_reverse_auto_differentiation.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 4: Computational graph extended for reverse auto differentiation&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Then following the green graph we execute the LogDerivative operation &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{1}{v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{1}{v}&quot; title=&quot;\frac{1}{v}&quot; /&gt;&lt;/a&gt; that the auto differentiation engine introduced, and multiply its result by &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;w}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;w}&quot; title=&quot;\frac{\partial z}{\partial w}&quot; /&gt;&lt;/a&gt; to obtain the gradient &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;v}&quot; title=&quot;\frac{\partial z}{\partial v}&quot; /&gt;&lt;/a&gt; as per the chain rule states. Next, the multiplication derivative is executed in the same way, and the desired derivatives &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;x}&amp;space;\frac{\partial&amp;space;z}{\partial&amp;space;y}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;x}&amp;space;\frac{\partial&amp;space;z}{\partial&amp;space;y}&quot; title=&quot;\frac{\partial z}{\partial x} \frac{\partial z}{\partial y}&quot; /&gt;&lt;/a&gt; are finally obtained.&lt;/p&gt;

&lt;p&gt;Formally, what we are doing here, and PyTorch autograd engine also does, is computing a Jacobian-vector product (Jvp) to calculate the gradients of the model parameters, since the model parameters and inputs are vectors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Jacobian-vector product&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When we calculate the gradient of a vector-valued function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(\overline{x})&amp;space;=&amp;space;\overline{y}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(\overline{x})&amp;space;=&amp;space;\overline{y}&quot; title=&quot;f(\overline{x}) = \overline{y}&quot; /&gt;&lt;/a&gt; (a function whose inputs and outputs are vectors), we are essentially constructing a Jacobian matrix .&lt;/p&gt;

&lt;p&gt;Thanks to the chain rule, multiplying the Jacobian matrix of a function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(\overline{x})&amp;space;=&amp;space;\overline{y}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(\overline{x})&amp;space;=&amp;space;\overline{y}&quot; title=&quot;f(\overline{x}) = \overline{y}&quot; /&gt;&lt;/a&gt; by a vector &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=v&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?v&quot; title=&quot;v&quot; /&gt;&lt;/a&gt; with the previously calculated gradients of a scalar function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=z&amp;space;=&amp;space;g(\overline{y})&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?z&amp;space;=&amp;space;g(\overline{y})&quot; title=&quot;z = g(\overline{y})&quot; /&gt;&lt;/a&gt; results in the gradients &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;x_1}&amp;space;\cdots&amp;space;\frac{\partial&amp;space;z}{\partial&amp;space;x_n}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;x_1}&amp;space;\cdots&amp;space;\frac{\partial&amp;space;z}{\partial&amp;space;x_n}&quot; title=&quot;\frac{\partial z}{\partial x_1} \cdots \frac{\partial z}{\partial x_n}&quot; /&gt;&lt;/a&gt; of the scalar output with respect to the vector-valued function inputs.&lt;/p&gt;

&lt;p&gt;As an example, let’s look at some functions in python notation to show how the chain rule applies.&lt;/p&gt;
&lt;div class=&quot;outlined-code-block&quot;&gt;
  &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(x_1,&amp;space;x_2)&amp;space;=&amp;space;(log(x_1&amp;space;x_2),&amp;space;sin(x_2))&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x_1,&amp;space;x_2)&amp;space;=&amp;space;(log(x_1&amp;space;x_2),&amp;space;sin(x_2))&quot; title=&quot;f(x_1, x_2) = (log(x_1 x_2), sin(x_2))&quot; /&gt;&lt;/a&gt;

  &lt;pre&gt;def f(x1, x2):
      a = x1 * x2
      y1 = log(a)
      y2 = sin(x2)
      return (y1, y2)
  &lt;/pre&gt;

  &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=g(y_1,&amp;space;y_2)&amp;space;=&amp;space;y_1&amp;space;y_2&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?g(y_1,&amp;space;y_2)&amp;space;=&amp;space;y_1&amp;space;y_2&quot; title=&quot;g(y_1, y_2) = y_1 y_2&quot; /&gt;&lt;/a&gt;

  &lt;pre&gt;def g(y1, y2):
      return y1 * y2
  &lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;Now, if we derive this by hand using the chain rule and the definition of the derivatives, we obtain the following set of identities that we can directly plug into the Jacobian matrix of &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(x_1,&amp;space;x_2)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x_1,&amp;space;x_2)&quot; title=&quot;f(x_1, x_2)&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;outlined-code-block&quot;&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y_1}{\partial&amp;space;x_1}&amp;space;=&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;a}\frac{\partial&amp;space;a}{\partial&amp;space;x_1}&amp;space;=&amp;space;\frac{1}{x_1}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y_1}{\partial&amp;space;x_1}&amp;space;=&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;a}\frac{\partial&amp;space;a}{\partial&amp;space;x_1}&amp;space;=&amp;space;\frac{1}{x_1}&quot; title=&quot;\frac{\partial y_1}{\partial x_1} = \frac{\partial y_1}{\partial a}\frac{\partial a}{\partial x_1} = \frac{1}{x_1}&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y_1}{\partial&amp;space;x_2}&amp;space;=&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;a}\frac{\partial&amp;space;a}{\partial&amp;space;x_2}&amp;space;=&amp;space;\frac{1}{x_2}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y_1}{\partial&amp;space;x_2}&amp;space;=&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;a}\frac{\partial&amp;space;a}{\partial&amp;space;x_2}&amp;space;=&amp;space;\frac{1}{x_2}&quot; title=&quot;\frac{\partial y_1}{\partial x_2} = \frac{\partial y_1}{\partial a}\frac{\partial a}{\partial x_2} = \frac{1}{x_2}&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y_2}{\partial&amp;space;x_1}&amp;space;=&amp;space;0&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y_2}{\partial&amp;space;x_1}&amp;space;=&amp;space;0&quot; title=&quot;\frac{\partial y_2}{\partial x_1} = 0&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y_2}{\partial&amp;space;x_2}&amp;space;=&amp;space;cos(x_2)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y_2}{\partial&amp;space;x_2}&amp;space;=&amp;space;cos(x_2)&quot; title=&quot;\frac{\partial y_2}{\partial x_2} = cos(x_2)&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Next, let’s consider the gradients for the scalar function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=z&amp;space;=&amp;space;g(y_1,&amp;space;y_2)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?z&amp;space;=&amp;space;g(y_1,&amp;space;y_2)&quot; title=&quot;z = g(y_1, y_2)&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;outlined-code-block&quot;&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{y_1}&amp;space;=&amp;space;y_2&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{y_1}&amp;space;=&amp;space;y_2&quot; title=&quot;\frac{\partial z}{y_1} = y_2&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{y_2}&amp;space;=&amp;space;y_1&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{y_2}&amp;space;=&amp;space;y_1&quot; title=&quot;\frac{\partial z}{y_2} = y_1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;If we now calculate the transpose-Jacobian vector product obeying the chain rule, we obtain the following expression:&lt;/p&gt;
&lt;div style=&quot;overflow:scroll&quot;&gt;
  &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\begin{pmatrix}\frac{\partial&amp;space;y_1}{\partial&amp;space;x_1}&amp;space;&amp;amp;&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;x_2}&amp;space;\\\\&amp;space;\frac{\partial&amp;space;y_2}{\partial&amp;space;x_1}&amp;space;&amp;amp;&amp;space;\frac{\partial&amp;space;y_2}{\partial&amp;space;x_2}&amp;space;\end{pmatrix}^{t}&amp;space;\begin{pmatrix}&amp;space;y_2\\y_1&amp;space;\end{pmatrix}&amp;space;=&amp;space;\begin{pmatrix}\frac{1}{x_1}&amp;space;&amp;amp;&amp;space;\frac{1}{x_2}&amp;space;\\\\&amp;space;0&amp;space;&amp;amp;&amp;space;cos(x_2))&amp;space;\end{pmatrix}^{t}&amp;space;\begin{pmatrix}&amp;space;y_2\\y_1&amp;space;\end{pmatrix}&amp;space;=&amp;space;\begin{pmatrix}&amp;space;\frac{1}{x_1}y_2\\\frac{1}{x_2}y_2&amp;plus;cos(x_2)y_1&amp;space;\end{pmatrix}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\begin{pmatrix}\frac{\partial&amp;space;y_1}{\partial&amp;space;x_1}&amp;space;&amp;amp;&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;x_2}&amp;space;\\\\&amp;space;\frac{\partial&amp;space;y_2}{\partial&amp;space;x_1}&amp;space;&amp;amp;&amp;space;\frac{\partial&amp;space;y_2}{\partial&amp;space;x_2}&amp;space;\end{pmatrix}^{t}&amp;space;\begin{pmatrix}&amp;space;y_2\\y_1&amp;space;\end{pmatrix}&amp;space;=&amp;space;\begin{pmatrix}\frac{1}{x_1}&amp;space;&amp;amp;&amp;space;\frac{1}{x_2}&amp;space;\\\\&amp;space;0&amp;space;&amp;amp;&amp;space;cos(x_2))&amp;space;\end{pmatrix}^{t}&amp;space;\begin{pmatrix}&amp;space;y_2\\y_1&amp;space;\end{pmatrix}&amp;space;=&amp;space;\begin{pmatrix}&amp;space;\frac{1}{x_1}y_2\\\frac{1}{x_2}y_2&amp;plus;cos(x_2)y_1&amp;space;\end{pmatrix}&quot; title=&quot;\begin{pmatrix}\frac{\partial y_1}{\partial x_1} &amp;amp; \frac{\partial y_1}{\partial x_2} \\\\ \frac{\partial y_2}{\partial x_1} &amp;amp; \frac{\partial y_2}{\partial x_2} \end{pmatrix}^{t} \begin{pmatrix} y_2\\y_1 \end{pmatrix} = \begin{pmatrix}\frac{1}{x_1} &amp;amp; \frac{1}{x_2} \\\\ 0 &amp;amp; cos(x_2)) \end{pmatrix}^{t} \begin{pmatrix} y_2\\y_1 \end{pmatrix} = \begin{pmatrix} \frac{1}{x_1}y_2\\\frac{1}{x_2}y_2+cos(x_2)y_1 \end{pmatrix}&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;Evaluating the Jvp for &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=(x_1,&amp;space;x_2)&amp;space;=&amp;space;(0.5,&amp;space;0.75)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?(x_1,&amp;space;x_2)&amp;space;=&amp;space;(0.5,&amp;space;0.75)&quot; title=&quot;(x_1, x_2) = (0.5, 0.75)&quot; /&gt;&lt;/a&gt; yields the result:
&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=(\frac{dy}{x_1},&amp;space;\frac{dy}{x_2})&amp;space;=&amp;space;(1.3633,&amp;space;0.1912)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?(\frac{dy}{x_1},&amp;space;\frac{dy}{x_2})&amp;space;=&amp;space;(1.3633,&amp;space;0.1912)&quot; title=&quot;(\frac{dy}{x_1}, \frac{dy}{x_2}) = (1.3633, 0.1912)&quot; /&gt;&lt;/a&gt;
We can execute the same expression in PyTorch and calculate the gradient of the input:&lt;/p&gt;
&lt;div class=&quot;outlined-code-block&quot;&gt;
  &lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; import torch&lt;/pre&gt;
  &lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; x = torch.tensor([0.5, 0.75], requires_grad=True)&lt;/pre&gt;
  &lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; y = torch.log(x[0] * x[1]) * torch.sin(x[1])&lt;/pre&gt;
  &lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; y.backward(1.0)&lt;/pre&gt;
  &lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; x.grad&lt;/pre&gt;
  tensor([1.3633,
          0.1912])&amp;lt;/pre&amp;gt;
&lt;/div&gt;

&lt;p&gt;The result is the same as our hand-calculated Jacobian-vector product!
However, PyTorch never constructed the matrix as it could grow prohibitively large but instead, created a graph of operations that traversed backward while applying the Jacobian-vector products defined in &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml&quot;&gt;tools/autograd/derivatives.yaml&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Going through the graph&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every time PyTorch executes an operation, the autograd engine constructs the graph to be traversed backward.
The reverse mode auto differentiation starts by adding a scalar variable at the end &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=z&amp;space;=&amp;space;w&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?z&amp;space;=&amp;space;w&quot; title=&quot;z = w&quot; /&gt;&lt;/a&gt; so that &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;w}&amp;space;=&amp;space;1&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;w}&amp;space;=&amp;space;1&quot; title=&quot;\frac{\partial z}{\partial w} = 1&quot; /&gt;&lt;/a&gt; as we saw in the introduction. This is the initial gradient value that is supplied to the Jvp engine calculation as we saw in the section above.&lt;/p&gt;

&lt;p&gt;In PyTorch, the initial gradient is explicitly set by the user when he calls the backward method.&lt;/p&gt;

&lt;p&gt;Then, the Jvp calculation starts but it never constructs the matrix. Instead, when PyTorch records the computational graph, the derivatives of the executed forward operations are added (Backward Nodes). Figure 5 shows a backward graph generated by the execution of the functions &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(x_1,&amp;space;x_2)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x_1,&amp;space;x_2)&quot; title=&quot;f(x_1, x_2)&quot; /&gt;&lt;/a&gt; and &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=g(y_1,&amp;space;y_2)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?g(y_1,&amp;space;y_2)&quot; title=&quot;g(y_1, y_2)&quot; /&gt;&lt;/a&gt; seen before.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/computational_graph_backward_pass.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 5: Computational Graph extended with the backward pass&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Once the forward pass is done, the results are used in the backward pass where the derivatives in the computational graph are executed. The basic derivatives are stored in the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml&quot;&gt;tools/autograd/derivatives.yaml&lt;/a&gt; file and they are not regular derivatives but the Jvp versions of them [3]. They take their primitive function inputs and outputs as parameters along with the gradient of the function outputs with respect to the final outputs. By repeatedly multiplying the resulting gradients by the next Jvp derivatives in the graph, the gradients up to the inputs will be generated following the chain rule.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/chain_rule_backward_differentiation.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 6: How the chain rule is applied in backward differentiation&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Figure 6 represents the process by showing the chain rule. We started with a value of 1.0 as detailed before which is the already calculated gradient &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y}{\partial&amp;space;u}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y}{\partial&amp;space;u}&quot; title=&quot;\frac{\partial y}{\partial u}&quot; /&gt;&lt;/a&gt; highlighted in green. And we move to the next node in the graph. The &lt;em&gt;backward&lt;/em&gt; function registered in &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/a0a7a2d648f05b0192e6943c9684406cdf404fbf/tools/autograd/derivatives.yaml#L635-L636&quot;&gt;derivatives.yaml&lt;/a&gt; will calculate the associated
&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;u}{\partial&amp;space;v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;u}{\partial&amp;space;v}&quot; title=&quot;\frac{\partial u}{\partial v}&quot; /&gt;&lt;/a&gt; value highlighted in red and multiply it by &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y}{\partial&amp;space;u}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y}{\partial&amp;space;u}&quot; title=&quot;\frac{\partial y}{\partial u}&quot; /&gt;&lt;/a&gt;. By the chain rule this results in &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y}{\partial&amp;space;v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y}{\partial&amp;space;v}&quot; title=&quot;\frac{\partial y}{\partial v}&quot; /&gt;&lt;/a&gt; which will be the already calculated gradient (green) when we process the next backward node in the graph.&lt;/p&gt;

&lt;p&gt;You may also have noticed that in Figure 5 there is a gradient generated from two different sources. When two different functions share an input, the gradients with respect to the output are aggregated for that input, and calculations using that gradient can’t proceed unless all the paths have been aggregated together.&lt;/p&gt;

&lt;p&gt;Let’s see an example of how the derivatives are stored in PyTorch.&lt;/p&gt;

&lt;p&gt;Suppose that we are currently processing the backward propagation of the &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=log&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?log&quot; title=&quot;log&quot; /&gt;&lt;/a&gt; function, in the &lt;em&gt;LogBackward&lt;/em&gt; node in Figure 2.  The derivative of &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=log&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?log&quot; title=&quot;log&quot; /&gt;&lt;/a&gt; in &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/a0a7a2d648f05b0192e6943c9684406cdf404fbf/tools/autograd/derivatives.yaml#L635-L636&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;derivatives.yaml&lt;/code&gt;&lt;/a&gt; is specified as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad.div(self.conj())&lt;/code&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad&lt;/code&gt; is the already calculated gradient &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;y1}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;y1}&quot; title=&quot;\frac{\partial z}{\partial y1}&quot; /&gt;&lt;/a&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.conj()&lt;/code&gt; is the complex conjugate of the input vector. For complex numbers PyTorch calculates a special derivative called the conjugate Wirtinger derivative [6]. This derivative takes the complex number and its conjugate and by operating some magic that is described in [6], they are the direction of steepest descent when plugged into optimizers.&lt;/p&gt;

&lt;p&gt;This code translates to &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=(\frac{\partial&amp;space;z}{\partial&amp;space;y1}&amp;space;\frac{1}{v})&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?(\frac{\partial&amp;space;z}{\partial&amp;space;y1}&amp;space;\frac{1}{v})&quot; title=&quot;(\frac{\partial z}{\partial y1} \frac{1}{v})&quot; /&gt;&lt;/a&gt;, the corresponding green, and red squares in Figure 3. Continuing, the autograd engine will execute the next operation; backward of the multiplication. As before, the inputs are the original function’s inputs and the gradient calculated from the &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=log&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?log&quot; title=&quot;log&quot; /&gt;&lt;/a&gt; backward step. This step will keep repeating until we reach the gradient with respect to the inputs and the computation will be finished. The gradient of &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{x2}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{x2}&quot; title=&quot;\frac{\partial z}{x2}&quot; /&gt;&lt;/a&gt; is only completed once the multiplication and sin gradients are added together. As you can see, we computed the equivalent of the Jvp but without constructing the matrix.&lt;/p&gt;

&lt;p&gt;In the next post we will dive inside PyTorch code to see how this graph is constructed and where are the relevant pieces should you want to experiment with it!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ol class=&quot;reference-list&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html&quot;&gt;https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf&quot;&gt;https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf&quot;&gt;https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mustafaghali11.medium.com/how-pytorch-backward-function-works-55669b3b7c62&quot;&gt;https://mustafaghali11.medium.com/how-pytorch-backward-function-works-55669b3b7c62&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://indico.cern.ch/event/708041/contributions/3308814/attachments/1813852/2963725/automatic_differentiation_and_deep_learning.pdf&quot;&gt;https://indico.cern.ch/event/708041/contributions/3308814/attachments/1813852/2963725/automatic_differentiation_and_deep_learning.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/autograd.html#complex-autograd-doc&quot;&gt;https://pytorch.org/docs/stable/notes/autograd.html#complex-autograd-doc&lt;/a&gt;&lt;/li&gt;
  &lt;p&gt;Recommended: shows why the backprop is formally expressed with the Jacobian&lt;/p&gt;
  &lt;li&gt;&lt;a href=&quot;cs.ubc.ca/~fwood/CS340/lectures/AD1.pdf&quot;&gt;cs.ubc.ca/~fwood/CS340/lectures/AD1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Preferred Networks, Inc.</name></author><summary type="html">This blog post is based on PyTorch version 1.8, although it should apply for older versions too, since most of the mechanics have remained constant.</summary></entry><entry><title type="html">Everything you need to know about TorchVision’s MobileNetV3 implementation</title><link href="https://pytorch.org/blog/torchvision-mobilenet-v3-implementation/" rel="alternate" type="text/html" title="Everything you need to know about TorchVision’s MobileNetV3 implementation" /><published>2021-05-26T00:00:00-07:00</published><updated>2021-05-26T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchvision-mobilenet-v3-implementation</id><content type="html" xml:base="https://pytorch.org/blog/torchvision-mobilenet-v3-implementation/">&lt;p&gt;In TorchVision v0.9, we released a series of &lt;a href=&quot;https://pytorch.org/blog/ml-models-torchvision-v0.9/&quot;&gt;new mobile-friendly models&lt;/a&gt; that can be used for Classification, Object Detection and Semantic Segmentation. In this article, we will dig deep into the code of the models, share notable implementation details, explain how we configured and trained them, and highlight important tradeoffs we made during their tuning. Our goal is to disclose technical details that typically remain undocumented in the original papers and repos of the models.&lt;/p&gt;

&lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt;

&lt;p&gt;The implementation of the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py&quot;&gt;MobileNetV3 architecture&lt;/a&gt; follows closely the &lt;a href=&quot;https://arxiv.org/abs/1905.02244&quot;&gt;original paper&lt;/a&gt;. It is customizable and offers different configurations for building Classification, Object Detection and Semantic Segmentation backbones. It was designed to follow a similar structure to MobileNetV2 and the two share &lt;a href=&quot;https://github.com/pytorch/vision/blob/cac8a97b0bd14eddeff56f87a890d5cc85776e18/torchvision/models/mobilenetv2.py#L32&quot;&gt;common building blocks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Off-the-shelf, we offer the two variants described on the paper: the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L196-L214&quot;&gt;Large&lt;/a&gt; and the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L215-L229&quot;&gt;Small&lt;/a&gt;. Both are constructed using the same code with the only difference being their configuration which describes the number of blocks, their sizes, their activation functions etc.&lt;/p&gt;

&lt;h3 id=&quot;configuration-parameters&quot;&gt;Configuration parameters&lt;/h3&gt;

&lt;p&gt;Even though one can write a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L105&quot;&gt;custom InvertedResidual setting&lt;/a&gt; and pass it to the MobileNetV3 class directly, for the majority of applications we can adapt the existing configs by passing parameters to the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L253&quot;&gt;model building methods&lt;/a&gt;. Some of the key configuration parameters are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;width_mult&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L188&quot;&gt;parameter&lt;/a&gt; is a multiplier that affects the number of channels of the model. The default value is 1 and by increasing or decreasing it one can change the number of filters of all convolutions, including the ones of the first and last layers. The implementation ensures that the number of filters is always a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L56-L57&quot;&gt;multiple of 8&lt;/a&gt;. This is a hardware optimization trick which allows for faster vectorization of operations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduced_tail&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L188&quot;&gt;parameter&lt;/a&gt; halves the number of channels on the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L210-L214&quot;&gt;last blocks&lt;/a&gt; of the network. This version is used by some Object Detection and Semantic Segmentation models. It’s a speed optimization which is described on the &lt;a href=&quot;https://arxiv.org/abs/1905.02244&quot;&gt;MobileNetV3 paper&lt;/a&gt; and reportedly leads to a 15% latency reduction without a significant negative effect on accuracy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dilated&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L188&quot;&gt;parameter&lt;/a&gt; affects the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L210-L212&quot;&gt;last 3&lt;/a&gt; InvertedResidual blocks of the model and turns their normal depthwise Convolutions to Atrous Convolutions. This is used to control the output stride of these blocks and has a &lt;a href=&quot;https://arxiv.org/abs/1706.05587&quot;&gt;significant positive effect&lt;/a&gt; on the accuracy of Semantic Segmentation models.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation details&lt;/h3&gt;

&lt;p&gt;Below we provide additional information on some notable implementation details of the architecture.
The &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L101&quot;&gt;MobileNetV3 class&lt;/a&gt; is responsible for building a network out of the provided configuration. Here are some implementation details of the class:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The last convolution block expands the output of the last InvertedResidual block by a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L149&quot;&gt;factor of 6&lt;/a&gt;. The implementation is aligned with the Large and Small configurations described on the paper and can adapt to different values of the multiplier parameter.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly to other models such as MobileNetV2, a dropout layer is placed just before the final Linear layer of the classifier.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L60&quot;&gt;InvertedResidual class&lt;/a&gt; is the main building block of the network. Here are some notable implementation details of the block along with its visualization which comes from Figure 4 of the paper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There is no &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L73-L76&quot;&gt;expansion step&lt;/a&gt; if the input channels and the expanded channels are the same. This happens on the first convolution block of the network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is always a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L86-L88&quot;&gt;projection step&lt;/a&gt; even when the expanded channels are the same as the output channels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The activation method of the depthwise block is placed &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L82-L84&quot;&gt;before&lt;/a&gt; the Squeeze-and-Excite layer as this improves marginally the accuracy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/mobilenet-v3-block.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;

&lt;p&gt;In this section we provide benchmarks of the pre-trained models and details on how they were configured, trained and quantized.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here is how to initialize the pre-trained models:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;large = torchvision.models.mobilenet_v3_large(pretrained=True, width_mult=1.0,  reduced_tail=False, dilated=False)
small = torchvision.models.mobilenet_v3_small(pretrained=True)
quantized = torchvision.models.quantization.mobilenet_v3_large(pretrained=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below we have the detailed benchmarks between new and selected previous models. As we can see MobileNetV3-Large is a viable replacement of ResNet50 for users who are willing to sacrifice a bit of accuracy for a roughly 6x speed-up:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@5&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# Params (M)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;74.042&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.340&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0411&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MobileNetV3-Small&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;67.668&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;87.402&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0165&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.54&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Quantized MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;73.004&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.858&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0162&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.96&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MobileNetV2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.880&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.290&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0608&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet50&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;76.150&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92.870&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2545&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;25.56&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet18&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;69.760&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89.080&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1032&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11.69&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Note that the inference times are measured on CPU. They are not absolute benchmarks, but they allow for relative comparisons between models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All pre-trained models are configured with a width multiplier of 1, have full tails, are non-dilated, and were fitted on ImageNet. Both the Large and Small variants were trained using the same hyper-parameters and scripts which can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/c2ab0c59f42babf9ad01aa616cd8a901daac86dd/references/classification#mobilenetv3-large--small&quot;&gt;references&lt;/a&gt; folder. Below we provide details on the most notable aspects of the training process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Achieving fast and stable training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/vision/blob/c2ab0c59f42babf9ad01aa616cd8a901daac86dd/references/classification/train.py#L172-L173&quot;&gt;Configuring RMSProp&lt;/a&gt; correctly was crucial to achieve fast training with numerical stability. The authors of the paper used TensorFlow in their experiments and in their runs they reported using &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet#v3&quot;&gt;quite high&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rmsprop_epsilon&lt;/code&gt; comparing to the default. Typically this hyper-parameter takes small values as it’s used to avoid zero denominators, but in this specific model choosing the right value seems important to avoid numerical instabilities in the loss.&lt;/p&gt;

&lt;p&gt;Another important detail is that though PyTorch’s and TensorFlow’s RMSProp implementations typically behave similarly, there are &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/32545&quot;&gt;a few differences&lt;/a&gt; with the most notable in our setup being how the epsilon hyperparameter is handled. More specifically, PyTorch adds the epsilon &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/training/rmsprop.py#L25&quot;&gt;outside of the square root calculation&lt;/a&gt; while TensorFlow &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/training/rmsprop.py#L25&quot;&gt;adds it inside&lt;/a&gt;. The result of this implementation detail is that one needs to adjust the epsilon value while porting the hyper parameter of the paper. A reasonable approximation can be taken with the formula &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PyTorch_eps = sqrt(TF_eps)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Increasing our accuracy by tuning hyperparameters &amp;amp; improving our training recipe&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After configuring the optimizer to achieve fast and stable training, we turned into optimizing the accuracy of the model. There are a few techniques that helped us achieve this. First of all, to avoid overfitting we augmented out data using the AutoAugment algorithm, followed by RandomErasing. Additionally we tuned parameters such as the weight decay using cross validation. We also found beneficial to perform &lt;a href=&quot;https://github.com/pytorch/vision/blob/674e8140042c2a3cbb1eb9ebad1fa49501599130/references/classification/utils.py#L259&quot;&gt;weight averaging&lt;/a&gt; across different epoch checkpoints after the end of the training. Finally, though not used in our published training recipe, we found that using Label Smoothing, Stochastic Depth and LR noise injection improve the overall accuracy by over &lt;a href=&quot;https://rwightman.github.io/pytorch-image-models/training_hparam_examples/#mobilenetv3-large-100-75766-top-1-92542-top-5&quot;&gt;1.5 points&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The graph and table depict a simplified summary of the most important iterations for improving the accuracy of the MobileNetV3 Large variant. Note that the actual number of iterations done while training the model was significantly larger and that the progress in accuracy was not always monotonically increasing. Also note that the Y-axis of the graph starts from 70% instead from 0% to make the difference between iterations more visible:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/key-iterations-for-improving-the-accuracyof-mobilenetV3.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Iteration&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@5&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Baseline with “MobileNetV2-style” Hyperparams&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.542&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.068&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ RMSProp with default eps&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;70.684&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ RMSProp with adjusted eps &amp;amp; LR scheme&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.764&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.178&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Data Augmentation &amp;amp; Tuned Hyperparams&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;73.86&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.292&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Checkpoint Averaging&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;74.028&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.382&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Label Smoothing &amp;amp; Stochastic Depth &amp;amp; LR noise&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;75.536&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92.368&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Note that once we’ve achieved an acceptable accuracy, we verified the model performance on the hold-out test dataset which hasn’t been used before for training or hyper-parameter tuning. This process helps us detect overfitting and is always performed for all pre-trained models prior their release.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Quantization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We currently offer quantized weights for the QNNPACK backend of the &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/quantization/mobilenetv3.py#L115&quot;&gt;MobileNetV3-Large variant&lt;/a&gt; which provides a speed-up of 2.5x. To quantize the model, Quantized Aware Training (QAT) was used. The hyper parameters and the scripts used to train the model can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/c2ab0c59f42babf9ad01aa616cd8a901daac86dd/references/classification#quantized&quot;&gt;references&lt;/a&gt; folder.&lt;/p&gt;

&lt;p&gt;Note that QAT allows us to model the effects of quantization and adjust the weights so that we can improve the model accuracy. This translates to an accuracy increase of 1.8 points comparing to simple post-training quantization:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Quantization Status&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@5&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Non-quantized&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;74.042&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.340&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Quantized Aware Training&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;73.004&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.858&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Post-training Quantization&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.160&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89.834&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h3&gt;

&lt;p&gt;In this section, we will first provide benchmarks of the released models, and then discuss how the MobileNetV3-Large backbone was used in a Feature Pyramid Network along with the FasterRCNN detector to perform Object Detection. We will also explain how the network was trained and tuned alongside with any tradeoffs we had to make. We will not cover details about how it was used with &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/detection/ssdlite.py&quot;&gt;SSDlite&lt;/a&gt; as this will be discussed on a future article.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here is how the models are initialized:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;high_res = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) 
low_res = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are some benchmarks between new and selected previous models. As we can see the high resolution Faster R-CNN with MobileNetV3-Large FPN backbone seems a viable replacement of the equivalent ResNet50 model for those users who are willing to sacrifice few accuracy points for a 5x speed-up:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mAP&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# Params (M)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN MobileNetV3-Large FPN (High-Res)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;32.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.8409&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;19.39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN MobileNetV3-Large 320 FPN (Low-Res)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;22.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1679&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;19.39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;37.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1514&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;41.76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RetinaNet ResNet-50 FPN&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;36.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.8825&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;34.01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Implementation details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Detector uses a FPN-style backbone which extracts features from different convolutions of the MobileNetV3 model. &lt;a href=&quot;https://github.com/pytorch/vision/blob/eca37cf735064702189ff5d5b1428cbe25ab2bcf/torchvision/models/detection/backbone_utils.py#L165-L166&quot;&gt;By default&lt;/a&gt; the pre-trained model uses the output of the 13th InvertedResidual block and the output of the Convolution prior to the pooling layer but the implementation supports using the outputs of &lt;a href=&quot;https://github.com/pytorch/vision/blob/eca37cf735064702189ff5d5b1428cbe25ab2bcf/torchvision/models/detection/backbone_utils.py#L147-L150&quot;&gt;more stages&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All feature maps extracted from the network have their output projected down to &lt;a href=&quot;https://github.com/pytorch/vision/blob/eca37cf735064702189ff5d5b1428cbe25ab2bcf/torchvision/models/detection/backbone_utils.py#L160&quot;&gt;256 channels&lt;/a&gt; by the FPN block as this greatly improves the speed of the network. These feature maps provided by the FPN backbone are used by the FasterRCNN detector to provide box and class predictions at &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L382-L389&quot;&gt;different scales&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training &amp;amp; Tuning process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We currently offer two pre-trained models capable of doing object detection at different resolutions. Both models were trained on the COCO dataset using the same hyper-parameters and scripts which can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/e35793a1a4000db1f9f99673437c514e24e65451/references/detection#faster-r-cnn-mobilenetv3-large-fpn&quot;&gt;references&lt;/a&gt; folder.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L398-L399&quot;&gt;High Resolution detector&lt;/a&gt; was trained with images of 800-1333px, while the mobile-friendly &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L398-L399&quot;&gt;Low Resolution detector&lt;/a&gt; was trained with images of 320-640px. The reason why we provide two separate sets of pre-trained weights is because training a detector directly on the smaller images leads to a 5 mAP increase in precision comparing to passing small images to the pre-trained high-res model. Both backbones were initialized with weights fitted on ImageNet and the &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L377-L378&quot;&gt;3 last stages&lt;/a&gt; of their weights where fined-tuned during the training process.&lt;/p&gt;

&lt;p&gt;An additional speed optimization can be applied on the mobile-friendly model by &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L423-L424&quot;&gt;tuning the RPN NMS thresholds&lt;/a&gt;. By sacrificing only 0.2 mAP of precision we were able to improve the CPU speed of the model by roughly 45%. The details of the optimization can be seen below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Tuning Status&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mAP&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Before&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;23.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2904&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;After&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;22.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1679&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Below we provide some examples of visualizing the predictions of the Faster R-CNN MobileNetV3-Large FPN model:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/detection.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;semantic-segmentation&quot;&gt;Semantic Segmentation&lt;/h3&gt;

&lt;p&gt;In this section we will start by providing some benchmarks of the released pre-trained models. Then we will discuss how a MobileNetV3-Large backbone was combined with segmentation heads such as &lt;a href=&quot;https://arxiv.org/abs/1905.02244&quot;&gt;LR-ASPP&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1706.05587&quot;&gt;DeepLabV3&lt;/a&gt; and the &lt;a href=&quot;https://arxiv.org/abs/1411.4038&quot;&gt;FCN&lt;/a&gt; to conduct Semantic Segmentation. We will also explain how the network was trained and propose a few optional optimization techniques for speed critical applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is how to initialize the pre-trained models:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(pretrained=True) 
deeplabv3 = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are the detailed benchmarks between new and selected existing models. As we can see, the DeepLabV3 with a MobileNetV3-Large backbone is a viable replacement of FCN with ResNet50 for the majority of applications as it achieves similar accuracy with a 8.5x speed-up. We also observe that the LR-ASPP network supersedes the equivalent FCN in all metrics:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mIoU&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Global Pixel Acc&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# Params (M)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;LR-ASPP MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;57.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.3278&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;60.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5869&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11.03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN MobileNetV3-Large (not released)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;57.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.3702&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 ResNet50&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;66.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.3531&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;39.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN ResNet50&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;60.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.0146&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;32.96&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;implementation-details-1&quot;&gt;Implementation details&lt;/h3&gt;

&lt;p&gt;In this section we will discuss important implementation details of tested segmentation heads. Note that all models described in this section use a dilated MobileNetV3-Large backbone.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LR-ASPP&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The LR-ASPP is the Lite variant of the Reduced Atrous Spatial Pyramid Pooling model proposed by the authors of the MobileNetV3 paper. Unlike the other segmentation models in TorchVision, it does not make use of an &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L185-L186&quot;&gt;auxiliary loss&lt;/a&gt;. Instead it uses &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L92-L100&quot;&gt;low and high-level features&lt;/a&gt; with output strides of 8 and 16 respectively.&lt;/p&gt;

&lt;p&gt;Unlike the paper where a 49x49 AveragePooling layer with variable strides is used, &lt;a href=&quot;https://github.com/pytorch/vision/blob/e2db2eddbb1699a59fbb5ccbec912979048ef3bf/torchvision/models/segmentation/lraspp.py#L53&quot;&gt;our implementation&lt;/a&gt; uses an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AdaptiveAvgPool2d&lt;/code&gt; layer to process the global features. This is because the authors of the paper tailored the head to the Cityscapes dataset while our focus is to provide a general purpose implementation that can work on multiple datasets. Finally our implementation always has a bilinear interpolation &lt;a href=&quot;https://github.com/pytorch/vision/blob/e2db2eddbb1699a59fbb5ccbec912979048ef3bf/torchvision/models/segmentation/lraspp.py#L35&quot;&gt;before returning the output&lt;/a&gt; to ensure that the sizes of the input and output images match exactly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DeepLabV3 &amp;amp; FCN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The combination of MobileNetV3 with DeepLabV3 and FCN follows closely the ones of other models and the stage estimation for these methods is identical to LR-ASPP. The only notable difference is that instead of using high and low level features, &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L37-L45&quot;&gt;we attach&lt;/a&gt; the normal loss to the feature map with output stride 16 and an auxiliary loss on the feature map with output stride 8.&lt;/p&gt;

&lt;p&gt;Finally we should note that the FCN version of the model was not released because it was completely superseded by the LR-ASPP both in terms of speed and accuracy. The &lt;a href=&quot;https://github.com/pytorch/vision/pull/3276/commits/1641d5f4c7d41f534444fab340c598d61a91bd12#diff-ccff7af514d99eeb40416c8b9ec30f032d1a3f450aaa4057958ca39ab174452eL17&quot;&gt;pre-trained weights&lt;/a&gt; are still available and can be used with minimal changes to the code.&lt;/p&gt;

&lt;h3 id=&quot;training--tuning-process&quot;&gt;Training &amp;amp; Tuning process&lt;/h3&gt;

&lt;p&gt;We currently offer two MobileNetV3 pre-trained models capable of doing semantic segmentation: the LR-ASPP and the DeepLabV3. The backbones of the models were &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L89-L90&quot;&gt;initialized with ImageNet weights&lt;/a&gt; and trained end-to-end. Both architectures were trained on the COCO dataset using the same scripts with similar hyper-parameters. Their details can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/a78d0d83d0a499fe8480d7a9f493676e746c4699/references/segmentation#deeplabv3_mobilenet_v3_large&quot;&gt;references&lt;/a&gt; folder.&lt;/p&gt;

&lt;p&gt;Normally, during inference the images are &lt;a href=&quot;https://github.com/pytorch/vision/blob/a78d0d83d0a499fe8480d7a9f493676e746c4699/references/segmentation/train.py#L30-L33&quot;&gt;resized to 520 pixels&lt;/a&gt;. An optional speed optimization is to construct a Low Res configuration of the model by using the High-Res pre-trained weights and reducing the inference resizing to 320 pixels. This will improve the CPU execution times by roughly 60% while sacrificing a couple of mIoU points. The detailed numbers of this optimization can be found on the table below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Low-Res Configuration&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mIoU Difference&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Speed Improvement&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mIoU&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Global Pixel Acc&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;LR-ASPP MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-2.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;65.26%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;55.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1139&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-3.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;63.86%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;56.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2121&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN MobileNetV3-Large (not released)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;57.57%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;54.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1571&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here are some examples of visualizing the predictions of the LR-ASPP MobileNetV3-Large model:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/segmentation.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We hope that you found this article interesting. We are looking forward to your feedback to see if this is the type of content you would like us to publish more often. If the community finds that such posts are useful, we will be happy to publish more articles that cover the implementation details of newly introduced Machine Learning models.&lt;/p&gt;</content><author><name>Vasilis Vryniotis and Francisco Massa</name></author><summary type="html">In TorchVision v0.9, we released a series of new mobile-friendly models that can be used for Classification, Object Detection and Semantic Segmentation. In this article, we will dig deep into the code of the models, share notable implementation details, explain how we configured and trained them, and highlight important tradeoffs we made during their tuning. Our goal is to disclose technical details that typically remain undocumented in the original papers and repos of the models.</summary></entry><entry><title type="html">Announcing the PyTorch Enterprise Support Program</title><link href="https://pytorch.org/blog/announcing-pytorch-enterprise/" rel="alternate" type="text/html" title="Announcing the PyTorch Enterprise Support Program" /><published>2021-05-25T00:00:00-07:00</published><updated>2021-05-25T00:00:00-07:00</updated><id>https://pytorch.org/blog/announcing-pytorch-enterprise</id><content type="html" xml:base="https://pytorch.org/blog/announcing-pytorch-enterprise/">&lt;p&gt;Today, we are excited to announce the &lt;a href=&quot;http://pytorch.org/enterprise-support-program&quot;&gt;PyTorch Enterprise Support Program&lt;/a&gt;, a participatory program that enables service providers to develop and offer tailored enterprise-grade support to their customers. This new offering, built in collaboration between Facebook and Microsoft, was created in direct response to feedback from PyTorch enterprise users who are developing models in production at scale for mission-critical applications.&lt;/p&gt;

&lt;p&gt;The PyTorch Enterprise Support Program is available to any service provider. It is designed to mutually benefit all program Participants by sharing and improving PyTorch long-term support (LTS), including contributions of hotfixes and other improvements found while working closely with customers and on their systems.&lt;/p&gt;

&lt;p&gt;To benefit the open source community, all hotfixes developed by Participants will be tested and fed back to the LTS releases of PyTorch regularly through PyTorch’s standard pull request process. To participate in the program, a service provider must apply and meet a set of program terms and certification requirements. Once accepted, the service provider becomes a program Participant and can offer a packaged PyTorch Enterprise support service with LTS, prioritized troubleshooting, useful integrations, and more.&lt;/p&gt;

&lt;div class=&quot;col-md-5 enterprise-azure-logo-container&quot;&gt;
  &lt;img src=&quot;/assets/images/microsoft-azure.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;As one of the founding members and an inaugural member of the PyTorch Enterprise Support Program, Microsoft is launching &lt;a href=&quot;https://Aka.ms/PyTorchEnterpriseHeroBlog&quot;&gt;PyTorch Enterprise on Microsoft Azure&lt;/a&gt; to deliver a reliable production experience for PyTorch users. Microsoft will support each PyTorch release for as long as it is current. In addition, it will support selected releases for two years, enabling a stable production experience. Microsoft Premier and Unified Support customers can access prioritized troubleshooting for hotfixes, bugs, and security patches at no additional cost. Microsoft will extensively test PyTorch releases for performance regression. The latest release of PyTorch will be integrated with &lt;a href=&quot;https://azure.microsoft.com/en-us/services/machine-learning/&quot;&gt;Azure Machine Learning&lt;/a&gt; and other PyTorch add-ons including &lt;a href=&quot;https://www.onnxruntime.ai/&quot;&gt;ONNX Runtime&lt;/a&gt; for faster inference.&lt;/p&gt;

&lt;p&gt;PyTorch Enterprise on Microsoft Azure not only benefits its customers, but also the PyTorch community users. All improvements will be tested and fed back to the future release for PyTorch so everyone in the community can use them.&lt;/p&gt;

&lt;p&gt;As an organization or PyTorch user, the standard way of researching and deploying with different release versions of PyTorch does not change. If your organization is looking for the managed long-term support, prioritized patches, bug fixes, and additional enterprise-grade support, then you should reach out to service providers participating in the program.&lt;/p&gt;

&lt;p&gt;To learn more and participate in the program as a service provider, visit the &lt;a href=&quot;http://pytorch.org/enterprise-support-program&quot;&gt;PyTorch Enterprise Support Program&lt;/a&gt;. If you want to learn more about Microsoft’s offering, visit &lt;a href=&quot;https://Aka.ms/PyTorchEnterpriseHeroBlog&quot;&gt;PyTorch Enterprise on Microsoft Azure&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thank you,&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we are excited to announce the PyTorch Enterprise Support Program, a participatory program that enables service providers to develop and offer tailored enterprise-grade support to their customers. This new offering, built in collaboration between Facebook and Microsoft, was created in direct response to feedback from PyTorch enterprise users who are developing models in production at scale for mission-critical applications.</summary></entry><entry><title type="html">PyTorch Ecosystem Day 2021 Recap and New Contributor Resources</title><link href="https://pytorch.org/blog/ecosystem-day-2021-recap/" rel="alternate" type="text/html" title="PyTorch Ecosystem Day 2021 Recap and New Contributor Resources" /><published>2021-05-10T00:00:00-07:00</published><updated>2021-05-10T00:00:00-07:00</updated><id>https://pytorch.org/blog/ecosystem-day-2021-recap</id><content type="html" xml:base="https://pytorch.org/blog/ecosystem-day-2021-recap/">&lt;p&gt;Thank you to our incredible community for making the first ever PyTorch Ecosystem Day a success! The day was filled with discussions on new developments, trends and challenges showcased through 71 posters, 32 breakout sessions and 6 keynote speakers.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/ecosystem-day-thank-you.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Special thanks to our keynote speakers: Piotr Bialecki, Ritchie Ng, Miquel Farré, Joe Spisak, Geeta Chauhan, and Suraj Subramanian who shared updates from the latest release of PyTorch, exciting work being done with partners, use case example from Disney, the growth and development of the PyTorch community in Asia Pacific, and latest contributor highlights.&lt;/p&gt;

&lt;p&gt;If you missed the opening talks, you rewatch them here:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=MYE01-XaSZA&quot;&gt;Morning/EMEA Opening Talks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=CjU_6OaYKpw&quot;&gt;Evening/APAC Opening Talks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to the talks, we had 71 posters covering various topics such as multimodal, NLP, compiler, distributed training, researcher productivity tools, AI accelerators, and more. From the event, it was clear that an underlying thread that ties all of these different projects together is the cross-collaboration of the PyTorch community. Thank you for continuing to push the state of the art with PyTorch!&lt;/p&gt;

&lt;p&gt;To view the full catalogue of poster, please visit &lt;strong&gt;&lt;a href=&quot;https://pytorch.org/ecosystem/pted/2021&quot;&gt;PyTorch Ecosystem Day 2021 Event Page&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;new-contributor-resources&quot;&gt;New Contributor Resources&lt;/h3&gt;
&lt;p&gt;Today, we are also sharing new contributor resources that we are trying out to give you the most access to up-to-date news, networking opportunities and more.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/resources/contributors/&quot;&gt;Contributor Newsletter&lt;/a&gt; - Includes curated news including RFCs, feature roadmaps, notable PRs, editorials from developers, and more to support keeping track of everything that’s happening in our community.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev-discuss.pytorch.org/&quot;&gt;Contributors Discussion Forum&lt;/a&gt; - Designed for contributors to learn and collaborate on the latest development across PyTorch.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch-dev-podcast.simplecast.com/&quot;&gt;PyTorch Developer Podcast (Beta)&lt;/a&gt; - Edward Yang, PyTorch Research Scientist, at Facebook AI shares bite-sized (10 to 20 mins) podcast episodes discussing topics about all sorts of internal development topics in PyTorch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you,&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Thank you to our incredible community for making the first ever PyTorch Ecosystem Day a success! The day was filled with discussions on new developments, trends and challenges showcased through 71 posters, 32 breakout sessions and 6 keynote speakers.</summary></entry></feed>