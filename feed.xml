<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pytorch.org/" rel="alternate" type="text/html" /><updated>2020-06-18T21:52:41-07:00</updated><id>https://pytorch.org/</id><title type="html">PyTorch Website</title><subtitle>Scientific Computing...</subtitle><author><name>Facebook</name></author><entry><title type="html">Updates &amp;amp; Improvements to PyTorch Tutorials</title><link href="https://pytorch.org/blog/updates-improvements-to-pytorch-tutorials/" rel="alternate" type="text/html" title="Updates &amp; Improvements to PyTorch Tutorials" /><published>2020-05-05T00:00:00-07:00</published><updated>2020-05-05T00:00:00-07:00</updated><id>https://pytorch.org/blog/updates-improvements-to-pytorch-tutorials</id><content type="html" xml:base="https://pytorch.org/blog/updates-improvements-to-pytorch-tutorials/">&lt;p&gt;PyTorch.org provides researchers and developers with documentation, installation instructions, latest news, community projects, tutorials, and more. Today, we are introducing usability and content improvements including tutorials in additional categories, a new recipe format for quickly referencing common topics, sorting using tags, and an updated homepage.&lt;/p&gt;

&lt;p&gt;Let’s take a look at them in detail.&lt;/p&gt;

&lt;h2 id=&quot;tutorials-home-page-update&quot;&gt;TUTORIALS HOME PAGE UPDATE&lt;/h2&gt;
&lt;p&gt;The tutorials home page now provides clear actions that developers can take. For new PyTorch users, there is an easy-to-discover button to take them directly to “A 60 Minute Blitz”. Right next to it, there is a button to view all recipes which are designed to teach specific features quickly with examples.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/tutorialhomepage.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;In addition to the existing left navigation bar, tutorials can now be quickly filtered by multi-select tags. Let’s say you want to view all tutorials related to “Production” and “Quantization”. You can select the “Production” and “Quantization” filters as shown in the image shown below:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/blockfiltering.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The following additional resources can also be found at the bottom of the Tutorials homepage:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/ptcheat.html&quot;&gt;PyTorch Cheat Sheet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/examples&quot;&gt;PyTorch Examples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/tutorials&quot;&gt;Tutorial on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pytorch-recipes&quot;&gt;PYTORCH RECIPES&lt;/h2&gt;
&lt;p&gt;Recipes are new bite-sized, actionable examples designed to teach researchers and developers how to use specific PyTorch features. Some notable new recipes include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html&quot;&gt;Loading Data in PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/Captum_Recipe.html&quot;&gt;Model Interpretability Using Captum&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html&quot;&gt;How to Use TensorBoard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;View the full recipes &lt;a href=&quot;http://pytorch.org/tutorials/recipes/recipes_index.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;learning-pytorch&quot;&gt;LEARNING PYTORCH&lt;/h2&gt;
&lt;p&gt;This section includes tutorials designed for users new to PyTorch. Based on community feedback, we have made updates to the current &lt;a href=&quot;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&quot;&gt;Deep Learning with PyTorch: A 60 Minute Blitz&lt;/a&gt; tutorial, one of our most popular tutorials for beginners. Upon completion, one can understand what PyTorch and neural networks are, and be able to build and train a simple image classification network. Updates include adding explanations to clarify output meanings and linking back to where users can read more in the docs, cleaning up confusing syntax errors, and reconstructing and explaining new concepts for easier readability.&lt;/p&gt;

&lt;h2 id=&quot;deploying-models-in-production&quot;&gt;DEPLOYING MODELS IN PRODUCTION&lt;/h2&gt;
&lt;p&gt;This section includes tutorials for developers looking to take their PyTorch models to production. The tutorials include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html&quot;&gt;Deploying PyTorch in Python via a REST API with Flask&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&quot;&gt;Introduction to TorchScript&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;Loading a TorchScript Model in C++&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html&quot;&gt;Exploring a Model from PyTorch to ONNX and Running it using ONNX Runtime&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;frontend-apis&quot;&gt;FRONTEND APIS&lt;/h2&gt;
&lt;p&gt;PyTorch provides a number of frontend API features that can help developers to code, debug, and validate their models more efficiently. This section includes tutorials that teach what these features are and how to use them. Some tutorials to highlight:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html&quot;&gt;Introduction to Named Tensors in PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_frontend.html&quot;&gt;Using the PyTorch C++ Frontend&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html&quot;&gt;Extending TorchScript with Custom C++ Operators&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html&quot;&gt;Extending TorchScript with Custom C++ Classes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_autograd.html&quot;&gt;Autograd in C++ Frontend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-optimization&quot;&gt;MODEL OPTIMIZATION&lt;/h2&gt;
&lt;p&gt;Deep learning models often consume large amounts of memory, power, and compute due to their complexity. This section provides tutorials for model optimization:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/pruning_tutorial.html&quot;&gt;Pruning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;Dynamic Quantization on BERT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;Static Quantization with Eager Mode in PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parallel-and-distributed-training&quot;&gt;PARALLEL AND DISTRIBUTED TRAINING&lt;/h2&gt;
&lt;p&gt;PyTorch provides features that can accelerate performance in research and production such as native support for asynchronous execution of collective operations and peer-to-peer communication that is accessible from Python and C++. This section includes tutorials on parallel and distributed training:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html&quot;&gt;Single-Machine Model Parallel Best Practices&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/ddp_tutorial.html&quot;&gt;Getting started with Distributed Data Parallel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/rpc_tutorial.html&quot;&gt;Getting started with Distributed RPC Framework&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html&quot;&gt;Implementing a Parameter Server Using Distributed RPC Framework&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Making these improvements are just the first step of improving PyTorch.org for the community. Please submit your suggestions &lt;a href=&quot;https://github.com/pytorch/tutorials/pulls&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">PyTorch.org provides researchers and developers with documentation, installation instructions, latest news, community projects, tutorials, and more. Today, we are introducing usability and content improvements including tutorials in additional categories, a new recipe format for quickly referencing common topics, sorting using tags, and an updated homepage.</summary></entry><entry><title type="html">PyTorch library updates including new model serving library</title><link href="https://pytorch.org/blog/pytorch-library-updates-new-model-serving-library/" rel="alternate" type="text/html" title="PyTorch library updates including new model serving library " /><published>2020-04-21T00:00:00-07:00</published><updated>2020-04-21T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-library-updates-new-model-serving-library</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-library-updates-new-model-serving-library/">&lt;p&gt;Along with the PyTorch 1.5 release, we are announcing new libraries for high-performance PyTorch model serving and tight integration with TorchElastic and Kubernetes. Additionally, we are releasing updated packages for torch_xla (Google Cloud TPUs), torchaudio, torchvision, and torchtext. All of these new libraries and enhanced capabilities are available today and accompany all of the core features &lt;a href=&quot;https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis&quot;&gt;released in PyTorch 1.5&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchserve-experimental&quot;&gt;TorchServe (Experimental)&lt;/h2&gt;

&lt;p&gt;TorchServe is a flexible and easy to use library for serving PyTorch models in production performantly at scale. It is cloud and environment agnostic and supports features such as multi-model serving, logging, metrics, and the creation of RESTful endpoints for application integration. TorchServe was jointly developed by engineers from Facebook and AWS with feedback and engagement from the broader PyTorch community. The experimental release of TorchServe is available today. Some of the highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support for both Python-based and TorchScript-based models&lt;/li&gt;
  &lt;li&gt;Default handlers for common use cases (e.g., image segmentation, text classification) as well as the ability to write custom handlers for other use cases&lt;/li&gt;
  &lt;li&gt;Model versioning, the ability to run multiple versions of a model at the same time, and the ability to roll back to an earlier version&lt;/li&gt;
  &lt;li&gt;The ability to package a model, learning weights, and supporting files (e.g., class mappings, vocabularies) into a single, persistent artifact (a.k.a. the “model archive”)&lt;/li&gt;
  &lt;li&gt;Robust management capability, allowing full configuration of models, versions, and individual worker threads via command line, config file, or run-time API&lt;/li&gt;
  &lt;li&gt;Automatic batching of individual inferences across HTTP requests&lt;/li&gt;
  &lt;li&gt;Logging including common metrics, and the ability to incorporate custom metrics&lt;/li&gt;
  &lt;li&gt;Ready-made Dockerfile for easy deployment&lt;/li&gt;
  &lt;li&gt;HTTPS support for secure deployment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To learn more about the APIs and the design of this feature, see the links below:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;See &lt;here&gt; for a full multi-node deployment reference architecture.&lt;/here&gt;&lt;/li&gt;
  &lt;li&gt;The full documentation can be found &lt;a href=&quot;https://pytorch.org/serve&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;torchelastic-integration-with-kubernetes-experimental&quot;&gt;TorchElastic integration with Kubernetes (Experimental)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/elastic&quot;&gt;TorchElastic&lt;/a&gt; is a proven library for training large scale deep neural networks at scale within companies like Facebook, where having the ability to dynamically adapt to server availability and scale as new compute resources come online is critical. Kubernetes enables customers using machine learning frameworks like PyTorch to run training jobs distributed across fleets of powerful GPU instances like the Amazon EC2 P3. Distributed training jobs, however, are not fault-tolerant, and a job cannot continue if a node failure or reclamation interrupts training. Further, jobs cannot start without acquiring all required resources, or scale up and down without being restarted. This lack of resiliency and flexibility results in increased training time and costs from idle resources. TorchElastic addresses these limitations by enabling distributed training jobs to be executed in a fault-tolerant and elastic manner. Until today, Kubernetes users needed to manage Pods and Services required for TorchElastic training jobs manually.&lt;/p&gt;

&lt;p&gt;Through the joint collaboration of engineers at Facebook and AWS, TorchElastic, adding elasticity and fault tolerance, is now supported using vanilla Kubernetes and through the managed EKS service from AWS.&lt;/p&gt;

&lt;p&gt;To learn more see the &lt;a href=&quot;http://pytorch.org/elastic/0.2.0rc0/kubernetes.html&quot;&gt;TorchElastic repo&lt;/a&gt; for the controller implementation and docs on how to use it.&lt;/p&gt;

&lt;h2 id=&quot;torch_xla-15-now-available&quot;&gt;torch_xla 1.5 now available&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://pytorch.org/xla/&quot;&gt;torch_xla&lt;/a&gt; is a Python package that uses the &lt;a href=&quot;https://www.tensorflow.org/xla&quot;&gt;XLA linear algebra compiler&lt;/a&gt; to accelerate the &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch deep learning framework&lt;/a&gt; on &lt;a href=&quot;https://cloud.google.com/tpu/&quot;&gt;Cloud TPUs&lt;/a&gt; and &lt;a href=&quot;https://cloud.google.com/tpu/docs/tutorials/pytorch-pod&quot;&gt;Cloud TPU Pods&lt;/a&gt;. torch_xla aims to give PyTorch users the ability to do everything they can do on GPUs on Cloud TPUs as well while minimizing changes to the user experience. The project began with a conversation at NeurIPS 2017 and gathered momentum in 2018 when teams from Facebook and Google came together to create a proof of concept. We announced this collaboration at PTDC 2018 and made the PyTorch/XLA integration broadly available at PTDC 2019. The project already has 28 contributors, nearly 2k commits, and a repo that has been forked more than 100 times.&lt;/p&gt;

&lt;p&gt;This release of &lt;a href=&quot;http://pytorch.org/xla/&quot;&gt;torch_xla&lt;/a&gt; is aligned and tested with PyTorch 1.5 to reduce friction for developers and to provide a stable and mature PyTorch/XLA stack for training models using Cloud TPU hardware. You can &lt;a href=&quot;https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc&quot;&gt;try it for free&lt;/a&gt; in your browser on an 8-core Cloud TPU device with &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Google Colab&lt;/a&gt;, and you can use it at a much larger scaleon &lt;a href=&quot;https://cloud.google.com/gcp&quot;&gt;Google Cloud&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See the full torch_xla release notes &lt;a href=&quot;https://github.com/pytorch/xla/releases&quot;&gt;here&lt;/a&gt;. Full docs and tutorials can be found &lt;a href=&quot;https://pytorch.org/xla/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://cloud.google.com/tpu/docs/tutorials&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-domain-libraries&quot;&gt;PyTorch Domain Libraries&lt;/h2&gt;

&lt;p&gt;torchaudio, torchvision, and torchtext complement PyTorch with common datasets, models, and transforms in each domain area. We’re excited to share new releases for all three domain libraries alongside PyTorch 1.5 and the rest of the library updates. For this release, all three domain libraries are removing support for Python2 and will support Python3 only.&lt;/p&gt;

&lt;h3 id=&quot;torchaudio-05&quot;&gt;torchaudio 0.5&lt;/h3&gt;
&lt;p&gt;The torchaudio 0.5 release includes new transforms, functionals, and datasets. Highlights for the release include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Added the Griffin-Lim functional and transform, &lt;code class=&quot;highlighter-rouge&quot;&gt;InverseMelScale&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Vol&lt;/code&gt; transforms, and &lt;code class=&quot;highlighter-rouge&quot;&gt;DB_to_amplitude&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Added support for &lt;code class=&quot;highlighter-rouge&quot;&gt;allpass&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fade&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;bandpass&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;bandreject&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;band&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;treble&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;deemph&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;riaa&lt;/code&gt; filters and transformations.&lt;/li&gt;
  &lt;li&gt;New datasets added including &lt;code class=&quot;highlighter-rouge&quot;&gt;LJSpeech&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;SpeechCommands&lt;/code&gt; datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the release full notes &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt; and full docs can be found &lt;a href=&quot;https://pytorch.org/audio/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchvision-06&quot;&gt;torchvision 0.6&lt;/h3&gt;
&lt;p&gt;The torchvision 0.6 release includes updates to datasets, models and a significant number of bug fixes. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Faster R-CNN now supports negative samples which allows the feeding of images without annotations at training time.&lt;/li&gt;
  &lt;li&gt;Added &lt;code class=&quot;highlighter-rouge&quot;&gt;aligned&lt;/code&gt; flag to &lt;code class=&quot;highlighter-rouge&quot;&gt;RoIAlign&lt;/code&gt; to match Detectron2.&lt;/li&gt;
  &lt;li&gt;Refactored abstractions for C++ video decoder&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the release full notes &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt; and full docs can be found &lt;a href=&quot;https://pytorch.org/docs/stable/torchvision/index.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchtext-06&quot;&gt;torchtext 0.6&lt;/h3&gt;
&lt;p&gt;The torchtext 0.6 release includes a number of bug fixes and improvements to documentation. Based on user’s feedback, dataset abstractions are currently being redesigned also. Highlights for the release include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fixed an issue related to the SentencePiece dependency in conda package.&lt;/li&gt;
  &lt;li&gt;Added support for the experimental IMDB dataset to allow a custom vocab.&lt;/li&gt;
  &lt;li&gt;A number of documentation updates including adding a code of conduct and a deduplication of the docs on the torchtext site.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Your feedback and discussions on the experimental datasets API are welcomed. You can send them to &lt;a href=&quot;https://github.com/pytorch/text/issues/664&quot;&gt;issue #664&lt;/a&gt;. We would also like to highlight the pull request &lt;a href=&quot;https://github.com/pytorch/text/pull/701&quot;&gt;here&lt;/a&gt; where the latest dataset abstraction is applied to the text classification datasets. The feedback can be beneficial to finalizing this abstraction.&lt;/p&gt;

&lt;p&gt;See the release full notes &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt; and full docs can be found &lt;a href=&quot;https://pytorch.org/text/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team, the Amazon team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Along with the PyTorch 1.5 release, we are announcing new libraries for high-performance PyTorch model serving and tight integration with TorchElastic and Kubernetes. Additionally, we are releasing updated packages for torch_xla (Google Cloud TPUs), torchaudio, torchvision, and torchtext. All of these new libraries and enhanced capabilities are available today and accompany all of the core features released in PyTorch 1.5.</summary></entry><entry><title type="html">PyTorch 1.5 released, new and updated APIs including C++ frontend API parity with Python</title><link href="https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis/" rel="alternate" type="text/html" title="PyTorch 1.5 released, new and updated APIs including C++ frontend API parity with Python" /><published>2020-04-21T00:00:00-07:00</published><updated>2020-04-21T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis/">&lt;p&gt;Today, we’re announcing the availability of PyTorch 1.5, along with new and updated libraries. This release includes several major new API additions and improvements. PyTorch now includes a significant update to the C++ frontend, ‘channels last’ memory format for computer vision models, and a stable release of the distributed RPC framework used for model-parallel training. The release also has new APIs for autograd for hessians and jacobians, and an API that allows the creation of Custom C++ Classes that was inspired by pybind.&lt;/p&gt;

&lt;p&gt;You can find the detailed release notes &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;c-frontend-api-stable&quot;&gt;C++ Frontend API (Stable)&lt;/h2&gt;

&lt;p&gt;The C++ frontend API is now at parity with Python, and the features overall have been moved to ‘stable’ (previously tagged as experimental). Some of the major highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Now with ~100% coverage and docs for C++ torch::nn module/functional, users can easily translate their model from Python API to C++ API, making the model authoring experience much smoother.&lt;/li&gt;
  &lt;li&gt;Optimizers in C++ had deviated from the Python equivalent: C++ optimizers can’t take parameter groups as input while the Python ones can. Additionally, step function implementations were not exactly the same. With the 1.5 release, C++ optimizers will always behave the same as the Python equivalent.&lt;/li&gt;
  &lt;li&gt;The lack of tensor multi-dim indexing API in C++ is a well-known issue and had resulted in many posts in PyTorch Github issue tracker and forum. The previous workaround was to use a combination of &lt;code class=&quot;highlighter-rouge&quot;&gt;narrow&lt;/code&gt; / &lt;code class=&quot;highlighter-rouge&quot;&gt;select&lt;/code&gt; / &lt;code class=&quot;highlighter-rouge&quot;&gt;index_select&lt;/code&gt; / &lt;code class=&quot;highlighter-rouge&quot;&gt;masked_select&lt;/code&gt;, which was clunky and error-prone compared to the Python API’s elegant &lt;code class=&quot;highlighter-rouge&quot;&gt;tensor[:, 0, ..., mask]&lt;/code&gt; syntax. With the 1.5 release, users can use &lt;code class=&quot;highlighter-rouge&quot;&gt;tensor.index({Slice(), 0, &quot;...&quot;, mask})&lt;/code&gt; to achieve the same purpose.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;channels-last-memory-format-for-computer-vision-models-experimental&quot;&gt;‘Channels last’ memory format for Computer Vision models (Experimental)&lt;/h2&gt;

&lt;p&gt;‘Channels last’ memory layout unlocks ability to use performance efficient convolution algorithms and hardware (NVIDIA’s Tensor Cores, FBGEMM, QNNPACK). Additionally, it is designed to automatically propagate through the operators, which allows easy switching between memory layouts.&lt;/p&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators&quot;&gt;here&lt;/a&gt; on how to write memory format aware operators.&lt;/p&gt;

&lt;h2 id=&quot;custom-c-classes-experimental&quot;&gt;Custom C++ Classes (Experimental)&lt;/h2&gt;

&lt;p&gt;This release adds a new API, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch::class_&lt;/code&gt;, for binding custom C++ classes into TorchScript and Python simultaneously. This API is almost identical in syntax to &lt;a href=&quot;https://pybind11.readthedocs.io/en/stable/&quot;&gt;pybind11&lt;/a&gt;. It allows users to expose their C++ class and its methods to the TorchScript type system and runtime system such that they can instantiate and manipulate arbitrary C++ objects from TorchScript and Python. An example C++ binding:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CustomClassHolder&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testStack&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;myclasses&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;MyStackClass&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;push&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pop&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intrusive_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which exposes a class you can use in Python and TorchScript like so:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nd&quot;&gt;@torch.jit.script&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;do_stacks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myclasses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myclasses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hi&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;mom&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# &quot;mom&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;foobar&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [&quot;hi&quot;, &quot;foobar&quot;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can try it out in the tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;distributed-rpc-framework-apis-now-stable&quot;&gt;Distributed RPC framework APIs (Now Stable)&lt;/h2&gt;

&lt;p&gt;The Distributed &lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html&quot;&gt;RPC framework&lt;/a&gt; was launched as experimental in the 1.4 release and the proposal is to mark Distributed RPC framework as stable and no longer experimental. This work involves a lot of enhancements and bug fixes to make the distributed RPC framework more reliable and robust overall, as well as adding a couple of new features, including profiling support, using TorchScript functions in RPC, and several enhancements for ease of use. Below is an overview of the various APIs within the framework:&lt;/p&gt;

&lt;h3 id=&quot;rpc-api&quot;&gt;RPC API&lt;/h3&gt;
&lt;p&gt;The RPC API allows users to specify functions to run and objects to be instantiated on remote nodes. These functions are transparently recorded so that gradients can backpropagate through remote nodes using Distributed Autograd.&lt;/p&gt;

&lt;h3 id=&quot;distributed-autograd&quot;&gt;Distributed Autograd&lt;/h3&gt;
&lt;p&gt;Distributed Autograd connects the autograd graph across several nodes and allows gradients to flow through during the backwards pass. Gradients are accumulated into a context (as opposed to the .grad field as with Autograd) and users must specify their model’s forward pass under a with &lt;code class=&quot;highlighter-rouge&quot;&gt;dist_autograd.context()&lt;/code&gt; manager in order to ensure that all RPC communication is recorded properly. Currently, only FAST mode is implemented (see &lt;a href=&quot;https://pytorch.org/docs/stable/rpc/distributed_autograd.html#distributed-autograd-design&quot;&gt;here&lt;/a&gt; for the difference between FAST and SMART modes).&lt;/p&gt;

&lt;h3 id=&quot;distributed-optimizer&quot;&gt;Distributed Optimizer&lt;/h3&gt;
&lt;p&gt;The distributed optimizer creates RRefs to optimizers on each worker with parameters that require gradients, and then uses the RPC API to run the optimizer remotely. The user must collect all remote parameters and wrap them in an &lt;code class=&quot;highlighter-rouge&quot;&gt;RRef&lt;/code&gt;, as this is required input to the distributed optimizer. The user must also specify the distributed autograd &lt;code class=&quot;highlighter-rouge&quot;&gt;context_id&lt;/code&gt; so that the optimizer knows in which context to look for gradients.&lt;/p&gt;

&lt;p&gt;Learn more about distributed RPC framework APIs &lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;new-high-level-autograd-api-experimental&quot;&gt;New High level autograd API (Experimental)&lt;/h2&gt;

&lt;p&gt;PyTorch 1.5 brings new functions including jacobian, hessian, jvp, vjp, hvp and vhp to the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.autograd.functional&lt;/code&gt; submodule. This feature builds on the current API and allows the user to easily perform these functions.&lt;/p&gt;

&lt;p&gt;Detailed design discussion on GitHub can be found &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/30632&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;python-2-no-longer-supported&quot;&gt;Python 2 no longer supported&lt;/h2&gt;

&lt;p&gt;Starting PyTorch 1.5.0, we will no longer support Python 2, specifically version 2.7. Going forward support for Python will be limited to Python 3, specifically Python 3.5, 3.6, 3.7 and 3.8 (first enabled in PyTorch 1.4.0).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we’re announcing the availability of PyTorch 1.5, along with new and updated libraries. This release includes several major new API additions and improvements. PyTorch now includes a significant update to the C++ frontend, ‘channels last’ memory format for computer vision models, and a stable release of the distributed RPC framework used for model-parallel training. The release also has new APIs for autograd for hessians and jacobians, and an API that allows the creation of Custom C++ Classes that was inspired by pybind.</summary></entry><entry><title type="html">Introduction to Quantization on PyTorch</title><link href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/" rel="alternate" type="text/html" title="Introduction to Quantization on PyTorch" /><published>2020-03-26T00:00:00-07:00</published><updated>2020-03-26T00:00:00-07:00</updated><id>https://pytorch.org/blog/introduction-to-quantization-on-pytorch</id><content type="html" xml:base="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/">&lt;p&gt;It’s important to make efficient use of both server-side and on-device compute resources when developing machine learning applications. To support more efficient deployment on servers and edge devices, PyTorch added a support for model quantization using the familiar eager mode Python API.&lt;/p&gt;

&lt;p&gt;Quantization leverages 8bit integer (int8) instructions to reduce the model size and run the inference faster (reduced latency) and can be the difference between a model achieving quality of service goals or even fitting into the resources available on a mobile device. Even when resources aren’t quite so constrained it may enable you to deploy a larger and more accurate model. Quantization is available in PyTorch starting in version 1.3 and with the release of PyTorch 1.4 we published quantized models for ResNet, ResNext, MobileNetV2, GoogleNet, InceptionV3 and ShuffleNetV2 in the PyTorch torchvision 0.5 library.&lt;/p&gt;

&lt;p&gt;This blog post provides an overview of the quantization support on PyTorch and its incorporation with the TorchVision domain library.&lt;/p&gt;

&lt;h2 id=&quot;what-is-quantization&quot;&gt;&lt;strong&gt;What is Quantization?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Quantization refers to techniques for doing both computations and memory accesses with lower precision data, usually int8 compared to floating point implementations. This enables performance gains in several important areas:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;4x reduction in model size;&lt;/li&gt;
  &lt;li&gt;2-4x reduction in memory bandwidth;&lt;/li&gt;
  &lt;li&gt;2-4x faster inference due to savings in memory bandwidth and faster compute with int8 arithmetic (the exact speed up varies depending on the hardware, the runtime, and the model).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quantization does not however come without additional cost. Fundamentally quantization means introducing approximations and the resulting networks have slightly less accuracy. These techniques attempt to minimize the gap between the full floating point accuracy and the quantized accuracy.&lt;/p&gt;

&lt;p&gt;We designed quantization to fit into the PyTorch framework. The means that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;PyTorch has data types corresponding to &lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor&quot;&gt;quantized tensors&lt;/a&gt;, which share many of the features of tensors.&lt;/li&gt;
  &lt;li&gt;One can write kernels with quantized tensors, much like kernels for floating point tensors to customize their implementation. PyTorch supports quantized modules for common operations as part of the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.quantized&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.quantized.dynamic&lt;/code&gt; name-space.&lt;/li&gt;
  &lt;li&gt;Quantization is compatible with the rest of PyTorch: quantized models are traceable and scriptable. The quantization method is virtually identical for both server and mobile backends. One can easily mix quantized and floating point operations in a model.&lt;/li&gt;
  &lt;li&gt;Mapping of floating point tensors to quantized tensors is customizable with user defined observer/fake-quantization blocks. PyTorch provides default implementations that should work for most use cases.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/torch_stack1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We developed three techniques for quantizing neural networks in PyTorch as part of quantization tooling in the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization&lt;/code&gt; name-space.&lt;/p&gt;

&lt;h2 id=&quot;the-three-modes-of-quantization-supported-in-pytorch-starting-version-13&quot;&gt;&lt;strong&gt;The Three Modes of Quantization Supported in PyTorch starting version 1.3&lt;/strong&gt;&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;dynamic-quantization&quot;&gt;&lt;strong&gt;Dynamic Quantization&lt;/strong&gt;&lt;/h3&gt;
    &lt;p&gt;The easiest method of quantization PyTorch supports is called &lt;strong&gt;dynamic quantization&lt;/strong&gt;. This involves not just converting the weights to int8 - as happens in all quantization variants - but also converting the activations to int8 on the fly, just before doing the computation (hence “dynamic”). The computations will thus be performed using efficient int8 matrix multiplication and convolution implementations, resulting in faster compute. However, the activations are read and written to memory in floating point format.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;PyTorch API&lt;/strong&gt;: we have a simple API for dynamic quantization in PyTorch. &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.quantize_dynamic&lt;/code&gt; takes in a model, as well as a couple other arguments, and produces a quantized model! Our &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;end-to-end tutorial&lt;/a&gt; illustrates this for a BERT model; while the tutorial is long and contains sections on loading pre-trained models and other concepts unrelated to quantization, the part the quantizes the BERT model is simply:&lt;/li&gt;
    &lt;/ul&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.quantization&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;quantized_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantize_dynamic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qint8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;See the documentation for the function &lt;a href=&quot;https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic&quot;&gt;here&lt;/a&gt; an end-to-end example in our tutorials &lt;a href=&quot;https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;post-training-static-quantization&quot;&gt;&lt;strong&gt;Post-Training Static Quantization&lt;/strong&gt;&lt;/h3&gt;

    &lt;p&gt;One can further improve the performance (latency) by converting networks to use both integer arithmetic and int8 memory accesses. Static quantization performs the additional step of first feeding batches of data through the network and computing the resulting distributions of the different activations (specifically, this is done by inserting “observer” modules at different points that record these distributions). This information is used to determine how specifically the different activations should be quantized at inference time (a simple technique would be to simply divide the entire range of activations into 256 levels, but we support more sophisticated methods as well). Importantly, this additional step allows us to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.&lt;/p&gt;

    &lt;p&gt;With this release, we’re supporting several features that allow users to optimize their static quantization:&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Observers: you can customize observer modules which specify how statistics are collected prior to quantization to try out more advanced methods to quantize your data.&lt;/li&gt;
      &lt;li&gt;Operator fusion: you can fuse multiple operations into a single operation, saving on memory access while also improving the operation’s numerical accuracy.&lt;/li&gt;
      &lt;li&gt;Per-channel quantization: we can independently quantize weights for each output channel in a convolution/linear layer, which can lead to higher accuracy with almost the same speed.&lt;/li&gt;
    &lt;/ol&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;h3 id=&quot;pytorch-api&quot;&gt;&lt;strong&gt;PyTorch API&lt;/strong&gt;:&lt;/h3&gt;
        &lt;ul&gt;
          &lt;li&gt;To fuse modules, we have &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.fuse_modules&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;Observers are inserted using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.prepare&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;Finally, quantization itself is done using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.convert&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;We have a tutorial with an end-to-end example of quantization (this same tutorial also covers our third quantization method, quantization-aware training), but because of our simple API, the three lines that perform post-training static quantization on the pre-trained model &lt;code class=&quot;highlighter-rouge&quot;&gt;myModel&lt;/code&gt; are:&lt;/p&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# set quantization config for server (x86)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;deploymentmyModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fbgemm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# insert observers&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Calibrate the model and collect statistics&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# convert to quantized version&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;quantization-aware-training&quot;&gt;&lt;strong&gt;Quantization Aware Training&lt;/strong&gt;&lt;/h3&gt;
    &lt;p&gt;&lt;strong&gt;Quantization-aware training(QAT)&lt;/strong&gt; is the third method, and the one that typically results in highest accuracy of these three. With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method usually yields higher accuracy than the other two methods.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;h3 id=&quot;pytorch-api-1&quot;&gt;&lt;strong&gt;PyTorch API&lt;/strong&gt;:&lt;/h3&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.prepare_qat&lt;/code&gt; inserts fake quantization modules to model quantization.&lt;/li&gt;
          &lt;li&gt;Mimicking the static quantization API, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.convert&lt;/code&gt; actually quantizes the model once training is complete.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;For example, in &lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;the end-to-end example&lt;/a&gt;, we load in a pre-trained model as &lt;code class=&quot;highlighter-rouge&quot;&gt;qat_model&lt;/code&gt;, then we simply perform quantization-aware training using:&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# specify quantization config for QAT&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;qat_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_qat_qconfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fbgemm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# prepare QAT&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare_qat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qat_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# convert to quantized version, removing dropout, to check for accuracy on each&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epochquantized_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qat_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;device-and-operator-support&quot;&gt;&lt;strong&gt;Device and Operator Support&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Quantization support is restricted to a subset of available operators, depending on the method being used, for a list of supported operators, please see the documentation at &lt;a href=&quot;https://pytorch.org/docs/stable/quantization.html&quot;&gt;https://pytorch.org/docs/stable/quantization.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The set of available operators and the quantization numerics also depend on the backend being used to run quantized models. Currently quantized operators are supported only for CPU inference in the following backends: x86 and ARM. Both the quantization configuration (how tensors should be quantized and the quantized kernels (arithmetic with quantized tensors) are backend dependent. One can specify the backend by doing:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchbackend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fbgemm'&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 'fbgemm' for server, 'qnnpack' for mobile&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_qconfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# prepare and convert model&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Set the backend on which the quantized kernels need to be run&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backends&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, quantization aware training occurs in full floating point and can run on either GPU or CPU. Quantization aware training is typically only used in CNN models when post training static or dynamic quantization doesn’t yield sufficient accuracy. This can occur with models that are highly optimized to achieve small size (such as Mobilenet).&lt;/p&gt;

&lt;h4 id=&quot;integration-in-torchvision&quot;&gt;&lt;strong&gt;Integration in torchvision&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We’ve also enabled quantization for some of the most popular models in &lt;a href=&quot;https://github.com/pytorch/vision/tree/master/torchvision/models/quantization&quot;&gt;torchvision&lt;/a&gt;: Googlenet, Inception, Resnet, ResNeXt, Mobilenet and Shufflenet. We have upstreamed these changes to torchvision in three forms:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Pre-trained quantized weights so that you can use them right away.&lt;/li&gt;
  &lt;li&gt;Quantization ready model definitions so that you can do post-training quantization or quantization aware training.&lt;/li&gt;
  &lt;li&gt;A script for doing quantization aware training — which is available for any of these model though, as you will learn below, we only found it necessary for achieving accuracy with Mobilenet.&lt;/li&gt;
  &lt;li&gt;We also have a &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html&quot;&gt;tutorial&lt;/a&gt; showing how you can do transfer learning with quantization using one of the torchvision models.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;choosing-an-approach&quot;&gt;&lt;strong&gt;Choosing an approach&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The choice of which scheme to use depends on multiple factors:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Model/Target requirements: Some models might be sensitive to quantization, requiring quantization aware training.&lt;/li&gt;
  &lt;li&gt;Operator/Backend support: Some backends require fully quantized operators.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Currently, operator coverage is limited and may restrict the choices listed in the table below:
The table below provides a guideline.&lt;/p&gt;

&lt;style type=&quot;text/css&quot;&gt;
.tg  {border-collapse:collapse;border-spacing:0;border-color:black;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top;font-weight:bold;color:black;}
article.pytorch-article table tr th:first-of-type, article.pytorch-article table tr td:first-of-type{padding-left:5px}
&lt;/style&gt;

&lt;table class=&quot;tg&quot;&gt;
  &lt;tr&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;Model Type&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;Preferred scheme&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;Why&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;LSTM/RNN&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Dynamic Quantization&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Throughput dominated by compute/memory bandwidth for weights&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;BERT/Transformer&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Dynamic Quantization&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Throughput dominated by compute/memory bandwidth for weights&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;CNN&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Static Quantization&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Throughput limited by memory bandwidth for activations&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;CNN&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Quantization Aware Training&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;In the case where accuracy can't be achieved with static quantization&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;performance-results&quot;&gt;&lt;strong&gt;Performance Results&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Quantization provides a 4x reduction in the model size and a speedup of 2x to 3x compared to floating point implementations depending on the hardware platform and the model being benchmarked. Some sample results are:&lt;/p&gt;

&lt;div class=&quot;table-responsive&quot;&gt;
  &lt;table class=&quot;tg&quot;&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Model&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Float Latency (ms)&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Quantized Latency (ms)&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Inference Performance Gain&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Device&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Notes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;BERT&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;581&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;313&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;1.8x&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Xeon-D2191 (1.6GHz)&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Batch size = 1, Maximum sequence length= 128, Single thread, x86-64, Dynamic quantization&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Resnet-50&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;214&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;103&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;2x&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Xeon-D2191 (1.6GHz)&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Single thread, x86-64, Static quantization&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Mobilenet-v2&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;97&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;17&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;5.7x&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Samsung S9&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Static quantization, Floating point numbers are based on Caffe2 run-time and are not optimized&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;accuracy-results&quot;&gt;&lt;strong&gt;Accuracy results&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;We also compared the accuracy of static quantized models with the floating point models on Imagenet. For dynamic quantization, we &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/run_glue.py&quot;&gt;compared&lt;/a&gt; the F1 score of BERT on the GLUE benchmark for MRPC.&lt;/p&gt;

&lt;h4 id=&quot;computer-vision-model-accuracy&quot;&gt;&lt;strong&gt;Computer Vision Model accuracy&lt;/strong&gt;&lt;/h4&gt;

&lt;table class=&quot;tg&quot;&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Model&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Top-1 Accuracy (Float)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Top-1 Accuracy (Quantized)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Quantization scheme&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Googlenet&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;69.8&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;69.7&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Inception-v3&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;77.5&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;77.1&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;ResNet-18&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;69.8&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;69.4&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Resnet-50&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;76.1&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;75.9&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;ResNext-101 32x8d&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;79.3&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;79&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Mobilenet-v2&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;71.9&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;71.6&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Quantization Aware Training&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Shufflenet-v2&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;69.4&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;68.4&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;speech-and-nlp-model-accuracy&quot;&gt;&lt;strong&gt;Speech and NLP Model accuracy&lt;/strong&gt;&lt;/h4&gt;

&lt;div class=&quot;table-responsive&quot;&gt;
  &lt;table class=&quot;tg&quot;&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Model&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;F1 (GLUEMRPC) Float&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;F1 (GLUEMRPC) Quantized&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Quantization scheme&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;BERT&lt;/td&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;0.902&lt;/td&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;0.895&lt;/td&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;Dynamic quantization&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;To get started on quantizing your models in PyTorch, start with &lt;a href=&quot;https://pytorch.org/tutorials/#model-optimization&quot;&gt;the tutorials on the PyTorch website&lt;/a&gt;. If you are working with sequence data start with &lt;a href=&quot;https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html&quot;&gt;dynamic quantization for LSTM&lt;/a&gt;, or &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;BERT&lt;/a&gt;. If you are working with image data then we recommend starting with the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html&quot;&gt;transfer learning with quantization&lt;/a&gt; tutorial. Then you can explore &lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;static post training quantization&lt;/a&gt;. If you find that the accuracy drop with post training quantization is too high, then try &lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;quantization aware training&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you run into issues you can get community help by posting in at &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;discuss.pytorch.org&lt;/a&gt;, use the quantization category for quantization related issues.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is authored by Raghuraman Krishnamoorthi, James Reed, Min Ni, Chris Gottbrath and Seth Weidman. Special thanks to Jianyu Huang, Lingyi Liu and Haixin Liu for producing quantization metrics included in this post.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;PyTorch quantization presentation at Neurips: &lt;a href=&quot;https://research.fb.com/wp-content/uploads/2019/12/2.-Quantization.pptx&quot;&gt;(https://research.fb.com/wp-content/uploads/2019/12/2.-Quantization.pptx)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Quantized Tensors &lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor&quot;&gt;(https://github.com/pytorch/pytorch/wiki/
Introducing-Quantized-Tensor)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Quantization RFC on Github &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/18318&quot;&gt;(https://github.com/pytorch/pytorch/
issues/18318)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Raghuraman Krishnamoorthi, James Reed, Min Ni, Chris Gottbrath, and Seth Weidman</name></author><summary type="html">It’s important to make efficient use of both server-side and on-device compute resources when developing machine learning applications. To support more efficient deployment on servers and edge devices, PyTorch added a support for model quantization using the familiar eager mode Python API.</summary></entry><entry><title type="html">PyTorch 1.4 released, domain libraries updated</title><link href="https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated/" rel="alternate" type="text/html" title="PyTorch 1.4 released, domain libraries updated" /><published>2020-01-15T00:00:00-08:00</published><updated>2020-01-15T00:00:00-08:00</updated><id>https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated/">&lt;p&gt;Today, we’re announcing the availability of PyTorch 1.4, along with updates to the PyTorch domain libraries. These releases build on top of the announcements from &lt;a href=&quot;https://pytorch.org/blog/pytorch-adds-new-tools-and-libraries-welcomes-preferred-networks-to-its-community/&quot;&gt;NeurIPS 2019&lt;/a&gt;, where we shared the availability of PyTorch Elastic, a new classification framework for image and video, and the addition of Preferred Networks to the PyTorch community. For those that attended the workshops at NeurIPS, the content can be found &lt;a href=&quot;https://research.fb.com/neurips-2019-expo-workshops/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-14&quot;&gt;PyTorch 1.4&lt;/h2&gt;

&lt;p&gt;The 1.4 release of PyTorch adds new capabilities, including the ability to do fine grain build level customization for PyTorch Mobile, and new experimental features including support for model parallel training and Java language bindings.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-mobile---build-level-customization&quot;&gt;PyTorch Mobile - Build level customization&lt;/h3&gt;

&lt;p&gt;Following the open sourcing of &lt;a href=&quot;https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/&quot;&gt;PyTorch Mobile in the 1.3 release&lt;/a&gt;, PyTorch 1.4 adds additional mobile support including the ability to customize build scripts at a fine-grain level. This allows mobile developers to optimize library size by only including the operators used by their models and, in the process, reduce their on device footprint significantly. Initial results show that, for example, a customized MobileNetV2 is 40% to 50% smaller than the prebuilt PyTorch mobile library. You can learn more &lt;a href=&quot;https://pytorch.org/mobile/home/&quot;&gt;here&lt;/a&gt; about how to create your own custom builds and, as always, please engage with the community on the &lt;a href=&quot;https://discuss.pytorch.org/c/mobile&quot;&gt;PyTorch forums&lt;/a&gt; to provide any feedback you have.&lt;/p&gt;

&lt;p&gt;Example code snippet for selectively compiling only the operators needed for MobileNetV2:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Dump list of operators used by MobileNetV2:&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yaml&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'MobileNetV2.pt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;export_opnames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'MobileNetV2.yaml'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;yaml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;#&lt;/span&gt; Build PyTorch Android library customized &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;MobileNetV2:
&lt;span class=&quot;go&quot;&gt;SELECTED_OP_LIST=MobileNetV2.yaml scripts/build_pytorch_android.sh arm64-v8a

&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;#&lt;/span&gt; Build PyTorch iOS library customized &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;MobileNetV2:
&lt;span class=&quot;go&quot;&gt;SELECTED_OP_LIST=MobileNetV2.yaml BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 scripts/build_ios.sh
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;distributed-model-parallel-training-experimental&quot;&gt;Distributed model parallel training (Experimental)&lt;/h3&gt;

&lt;p&gt;With the scale of models, such as RoBERTa, continuing to increase into the billions of parameters, model parallel training has become ever more important to help researchers push the limits. This release provides a distributed RPC framework to support distributed model parallel training. It allows for running functions remotely and referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backwards and update parameters across RPC boundaries.&lt;/p&gt;

&lt;p&gt;To learn more about the APIs and the design of this feature, see the links below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html&quot;&gt;API documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/distributed_autograd.html&quot;&gt;Distributed Autograd design doc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/rref.html&quot;&gt;Remote Reference design doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the full tutorials, see the links below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/rpc_tutorial.html&quot;&gt;A full RPC tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/examples/tree/master/distributed/rpc&quot;&gt;Examples using model parallel training for reinforcement learning and with an LSTM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As always, you can connect with community members and discuss more on the &lt;a href=&quot;https://discuss.pytorch.org/c/distributed/distributed-rpc&quot;&gt;forums&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;java-bindings-experimental&quot;&gt;Java bindings (Experimental)&lt;/h3&gt;

&lt;p&gt;In addition to supporting Python and C++, this release adds experimental support for Java bindings. Based on the interface developed for Android in PyTorch Mobile, the new bindings allow you to invoke TorchScript models from any Java program. Note that the Java bindings are only available for Linux for this release, and for inference only. We expect support to expand in subsequent releases. See the code snippet below for how to use PyTorch within Java:&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;demo-model.pt1&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;fromBlob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// data&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// shape&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IValue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toTensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;shape: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Arrays&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Arrays&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getDataAsFloatArray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Learn more about how to use PyTorch from Java &lt;a href=&quot;https://github.com/pytorch/java-demo&quot;&gt;here&lt;/a&gt;, and see the full Javadocs API documentation &lt;a href=&quot;https://pytorch.org/javadoc/1.4.0/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For the full 1.4 release notes, see &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;domain-libraries&quot;&gt;Domain Libraries&lt;/h2&gt;

&lt;p&gt;PyTorch domain libraries like torchvision, torchtext, and torchaudio complement PyTorch with common datasets, models, and transforms. We’re excited to share new releases for all three domain libraries alongside the PyTorch 1.4 core release.&lt;/p&gt;

&lt;h3 id=&quot;torchvision-05&quot;&gt;torchvision 0.5&lt;/h3&gt;

&lt;p&gt;The improvements to torchvision 0.5 mainly focus on adding support for production deployment including quantization, TorchScript, and ONNX. Some of the highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All models in torchvision are now torchscriptable making them easier to ship into non-Python production environments&lt;/li&gt;
  &lt;li&gt;ResNets, MobileNet, ShuffleNet, GoogleNet and InceptionV3 now have quantized counterparts with pre-trained models, and also include scripts for quantization-aware training.&lt;/li&gt;
  &lt;li&gt;In partnership with the Microsoft team, we’ve added ONNX support for all models including Mask R-CNN.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about torchvision 0.5 &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchaudio-04&quot;&gt;torchaudio 0.4&lt;/h3&gt;

&lt;p&gt;Improvements in torchaudio 0.4 focus on enhancing the currently available transformations, datasets, and backend support. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SoX is now optional, and a new extensible backend dispatch mechanism exposes SoundFile as an alternative to SoX.&lt;/li&gt;
  &lt;li&gt;The interface for datasets has been unified. This enables the addition of two large datasets: LibriSpeech and Common Voice.&lt;/li&gt;
  &lt;li&gt;New filters such as biquad, data augmentation such as time and frequency masking, transforms such as MFCC, gain and dither, and new feature computation such as deltas, are now available.&lt;/li&gt;
  &lt;li&gt;Transformations now support batches and are jitable.&lt;/li&gt;
  &lt;li&gt;An interactive speech recognition demo with voice activity detection is available for experimentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about torchaudio 0.4 &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchtext-05&quot;&gt;torchtext 0.5&lt;/h3&gt;

&lt;p&gt;torchtext 0.5 focuses mainly on improvements to the dataset loader APIs, including compatibility with core PyTorch APIs, but also adds support for unsupervised text tokenization. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Added bindings for SentencePiece for unsupervised text tokenization .&lt;/li&gt;
  &lt;li&gt;Added a new unsupervised learning dataset - enwik9.&lt;/li&gt;
  &lt;li&gt;Made revisions to PennTreebank, WikiText103, WikiText2, IMDb to make them compatible with torch.utils.data. Those datasets are in an experimental folder and we welcome your feedback.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about torchtext 0.5 &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we’re announcing the availability of PyTorch 1.4, along with updates to the PyTorch domain libraries. These releases build on top of the announcements from NeurIPS 2019, where we shared the availability of PyTorch Elastic, a new classification framework for image and video, and the addition of Preferred Networks to the PyTorch community. For those that attended the workshops at NeurIPS, the content can be found here.</summary></entry><entry><title type="html">PyTorch adds new tools and libraries, welcomes Preferred Networks to its community</title><link href="https://pytorch.org/blog/pytorch-adds-new-tools-and-libraries-welcomes-preferred-networks-to-its-community/" rel="alternate" type="text/html" title="PyTorch adds new tools and libraries, welcomes Preferred Networks to its community" /><published>2019-12-06T00:00:00-08:00</published><updated>2019-12-06T00:00:00-08:00</updated><id>https://pytorch.org/blog/pytorch-adds-new-tools-and-libraries-welcomes-preferred-networks-to-its-community</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-adds-new-tools-and-libraries-welcomes-preferred-networks-to-its-community/">&lt;p&gt;PyTorch continues to be used for the latest state-of-the-art research on display at the NeurIPS conference next week, making up nearly &lt;a href=&quot;https://chillee.github.io/pytorch-vs-tensorflow/&quot;&gt;70% of papers&lt;/a&gt; that cite a framework. In addition, we’re excited to welcome Preferred Networks, the maintainers of the Chainer framework, to the PyTorch community. Their teams are moving fully over to PyTorch for developing their ML capabilities and services.&lt;/p&gt;

&lt;p&gt;This growth underpins PyTorch’s focus on building for the needs of the research community, and increasingly, supporting the full workflow from research to production deployment. To further support researchers and developers, we’re launching a number of new tools and libraries for large scale computer vision and elastic fault tolerant training. Learn more on GitHub and at our NeurIPS booth.&lt;/p&gt;

&lt;h2 id=&quot;preferred-networks-joins-the-pytorch-community&quot;&gt;Preferred Networks joins the PyTorch community&lt;/h2&gt;

&lt;p&gt;Preferred Networks, Inc. (PFN) announced plans to move its deep learning framework from Chainer to PyTorch. As part of this change, PFN will collaborate with the PyTorch community and contributors, including people from Facebook, Microsoft, CMU, and NYU, to participate in the development of PyTorch.&lt;/p&gt;

&lt;p&gt;PFN developed Chainer, a deep learning framework that introduced the concept of define-by-run (also referred to as eager execution), to support and speed up its deep learning development. Chainer has been used at PFN since 2015 to rapidly solve real-world problems with the latest, cutting-edge technology. Chainer was also one of the inspirations for PyTorch’s initial design, as outlined in the &lt;a href=&quot;https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library&quot;&gt;PyTorch NeurIPS&lt;/a&gt; paper.&lt;/p&gt;

&lt;p&gt;PFN has driven innovative work with &lt;a href=&quot;https://cupy.chainer.org/&quot;&gt;CuPy&lt;/a&gt;, ImageNet in 15 minutes, &lt;a href=&quot;https://optuna.org/&quot;&gt;Optuna&lt;/a&gt;, and other projects that have pushed the boundaries of design and engineering. As part of the PyTorch community, PFN brings with them creative engineering capabilities and experience to help take the framework forward. In addition, PFN’s migration to PyTorch will allow it to efficiently incorporate the latest research results to accelerate its R&amp;amp;D activities, &lt;a href=&quot;https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/&quot;&gt;given PyTorch’s broad adoption with researchers&lt;/a&gt;, and to collaborate with the community to add support for PyTorch on MN-Core, a deep learning processor currently in development.&lt;/p&gt;

&lt;p&gt;We are excited to welcome PFN to the PyTorch community, and to jointly work towards the common goal of furthering advances in deep learning technology. Learn more about the PFN’s migration to PyTorch &lt;a href=&quot;https://preferred.jp/en/news/pr20191205/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;tools-for-elastic-training-and-large-scale-computer-vision&quot;&gt;Tools for elastic training and large scale computer vision&lt;/h2&gt;

&lt;h3 id=&quot;pytorch-elastic-experimental&quot;&gt;PyTorch Elastic (Experimental)&lt;/h3&gt;

&lt;p&gt;Large scale model training is becoming commonplace with architectures like BERT and the growth of model parameter counts into the billions or even tens of billions. To achieve convergence at this scale in a reasonable amount of time, the use of distributed training is needed.&lt;/p&gt;

&lt;p&gt;The current PyTorch Distributed Data Parallel (DDP) module enables data parallel training where each process trains the same model but on different shards of data. It enables bulk synchronous, multi-host, multi-GPU/CPU execution of ML training. However, DDP has several shortcomings; e.g. jobs cannot start without acquiring all the requested nodes; jobs cannot continue after a node fails due to error or transient issue; jobs cannot incorporate a node that joined later; and lastly; progress cannot be made with the presence of a slow/stuck node.&lt;/p&gt;

&lt;p&gt;The focus of &lt;a href=&quot;https://github.com/pytorch/elastic&quot;&gt;PyTorch Elastic&lt;/a&gt;, which uses Elastic Distributed Data Parallelism, is to address these issues and build a generic framework/APIs for PyTorch to enable reliable and elastic execution of these data parallel training workloads. It will provide better programmability, higher resilience to failures of all kinds, higher-efficiency and larger-scale training compared with pure DDP.&lt;/p&gt;

&lt;p&gt;Elasticity, in this case, means both: 1) the ability for a job to continue after node failure (by running with fewer nodes and/or by incorporating a new host and transferring state to it); and 2) the ability to add/remove nodes dynamically due to resource availability changes or bottlenecks.&lt;/p&gt;

&lt;p&gt;While this feature is still experimental, you can try it out on AWS EC2, with the instructions &lt;a href=&quot;https://github.com/pytorch/elastic/tree/master/aws&quot;&gt;here&lt;/a&gt;. Additionally, the PyTorch distributed team is working closely with teams across AWS to support PyTorch Elastic training within services such as Amazon Sagemaker and Elastic Kubernetes Service (EKS). Look for additional updates in the near future.&lt;/p&gt;

&lt;h3 id=&quot;new-classification-framework&quot;&gt;New Classification Framework&lt;/h3&gt;

&lt;p&gt;Image and video classification are at the core of content understanding. To that end, you can now leverage a new end-to-end framework for large-scale training of state-of-the-art image and video classification models. It allows researchers to quickly prototype and iterate on large distributed training jobs at the scale of billions of images. Advantages include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ease of use - This framework features a modular, flexible design that allows anyone to train machine learning models on top of PyTorch using very simple abstractions. The system also has out-of-the-box integration with AWS on PyTorch Elastic, facilitating research at scale and making it simple to move between research and production.&lt;/li&gt;
  &lt;li&gt;High performance - Researchers can use the framework to train models such as Resnet50 on ImageNet in as little as 15 minutes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can learn more at the &lt;a href=&quot;https://nips.cc/ExpoConferences/2019/schedule?workshop_id=16&quot;&gt;NeurIPS Expo workshop&lt;/a&gt; on Multi-Modal research to production or get started with the PyTorch Elastic Imagenet example &lt;a href=&quot;https://github.com/pytorch/elastic/blob/master/examples/imagenet/main.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;come-see-us-at-neurips&quot;&gt;Come see us at NeurIPS&lt;/h2&gt;

&lt;p&gt;The PyTorch team will be hosting workshops at NeurIPS during the industry expo on 12/8. Join the sessions below to learn more, and visit the team at the PyTorch booth on the show floor and during the Poster Session. At the booth, we’ll be walking through an interactive demo of PyTorch running fast neural style transfer on a Cloud TPU - here’s a &lt;a href=&quot;https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/style_transfer_inference-xrt-1-15.ipynb&quot;&gt;sneak peek&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We’re also publishing a &lt;a href=&quot;https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library&quot;&gt;paper that details the principles that drove the implementation of PyTorch&lt;/a&gt; and how they’re reflected in its architecture.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://nips.cc/ExpoConferences/2019/schedule?workshop_id=16&quot;&gt;Multi-modal Research to Production&lt;/a&gt;&lt;/em&gt; - This workshop will dive into a number of modalities such as computer vision (large scale image classification and instance segmentation) and Translation and Speech (seq-to-seq Transformers) from the lens of taking cutting edge research to production. Lastly, we will also walk through how to use the latest APIs in PyTorch to take eager mode developed models into graph mode via Torchscript and quantize them for scale production deployment on servers or mobile devices. Libraries used include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Classification Framework - a newly open sourced PyTorch framework developed by Facebook AI for research on large-scale image and video classification. It allows researchers to quickly prototype and iterate on large distributed training jobs. Models built on the framework can be seamlessly deployed to production.&lt;/li&gt;
  &lt;li&gt;Detectron2 - the recently released object detection library built by the Facebook AI Research computer vision team. We will articulate the improvements over the previous version including: 1) Support for latest models and new tasks; 2) Increased flexibility, to enable new computer vision research; 3) Maintainable and scalable, to support production use cases.&lt;/li&gt;
  &lt;li&gt;Fairseq - general purpose sequence-to-sequence library, can be used in many applications, including (unsupervised) translation, summarization, dialog and speech recognition.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://nips.cc/ExpoConferences/2019/schedule?workshop_id=14&quot;&gt;Responsible and Reproducible AI&lt;/a&gt;&lt;/em&gt; - This workshop on Responsible and Reproducible AI will dive into important areas that are shaping the future of how we interpret, reproduce research, and build AI with privacy in mind. We will cover major challenges, walk through solutions, and finish each talk with a hands-on tutorial.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reproducibility: As the number of research papers submitted to arXiv and conferences skyrockets, scaling reproducibility becomes difficult. We must address the following challenges: aid extensibility by standardizing code bases, democratize paper implementation by writing hardware agnostic code, facilitate results validation by documenting “tricks” authors use to make their complex systems function. To offer solutions, we will dive into tool like PyTorch Hub and PyTorch Lightning which are used by some of the top researchers in the world to reproduce the state of the art.&lt;/li&gt;
  &lt;li&gt;Interpretability: With the increase in model complexity and the resulting lack of transparency, model interpretability methods have become increasingly important. Model understanding is both an active area of research as well as an area of focus for practical applications across industries using machine learning. To get hands on, we will use the recently released Captum library that provides state-of-the-art algorithms to provide researchers and developers with an easy way to understand the importance of neurons/layers and the predictions made by our models.`&lt;/li&gt;
  &lt;li&gt;Private AI: Practical applications of ML via cloud-based or machine-learning-as-a-service platforms pose a range of security and privacy challenges. There are a number of technical approaches being studied including: homomorphic encryption, secure multi-party computation, trusted execution environments, on-device computation, and differential privacy. To provide an immersive understanding of how some of these technologies are applied, we will use the CrypTen project which provides a community based research platform to take the field of Private AI forward.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">PyTorch continues to be used for the latest state-of-the-art research on display at the NeurIPS conference next week, making up nearly 70% of papers that cite a framework. In addition, we’re excited to welcome Preferred Networks, the maintainers of the Chainer framework, to the PyTorch community. Their teams are moving fully over to PyTorch for developing their ML capabilities and services.</summary></entry><entry><title type="html">OpenMined and PyTorch partner to launch fellowship funding for privacy-preserving ML community</title><link href="https://pytorch.org/blog/openmined-and-pytorch-launch-fellowship-funding-for-privacy-preserving-ml/" rel="alternate" type="text/html" title="OpenMined and PyTorch partner to launch fellowship funding for privacy-preserving ML community" /><published>2019-12-06T00:00:00-08:00</published><updated>2019-12-06T00:00:00-08:00</updated><id>https://pytorch.org/blog/openmined-and-pytorch-launch-fellowship-funding-for-privacy-preserving-ml</id><content type="html" xml:base="https://pytorch.org/blog/openmined-and-pytorch-launch-fellowship-funding-for-privacy-preserving-ml/">&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/openmined-pytorch.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Many applications of machine learning (ML) pose a range of security and privacy challenges. In particular, users may not be willing or allowed to share their data, which prevents them from taking full advantage of ML platforms like PyTorch. To take the field of privacy-preserving ML (PPML) forward, OpenMined and PyTorch are announcing plans to jointly develop a combined platform to accelerate PPML research as well as new funding for fellowships.&lt;/p&gt;

&lt;p&gt;There are many techniques attempting to solve the problem of privacy in ML, each at various levels of maturity. These include (1) homomorphic encryption, (2) secure multi-party computation, (3) trusted execution environments, (4) on-device computation, (5) federated learning with secure aggregation, and (6) differential privacy. Additionally, a number of open source projects implementing these techniques were created with the goal of enabling research at the intersection of privacy, security, and ML. Among them, PySyft and CrypTen have taken an “ML-first” approach by presenting an API that is familiar to the ML community, while masking the complexities of privacy and security protocols. We are excited to announce that these two projects are now collaborating closely to build a mature PPML ecosystem around PyTorch.&lt;/p&gt;

&lt;p&gt;Additionally, to bolster this ecosystem and take the field of privacy preserving ML forward, we are also calling for contributions and supporting research efforts on this combined platform by providing funding to support the OpenMined community and the researchers that contribute, build proofs of concepts and desire to be on the cutting edge of how privacy-preserving technology is applied. We will provide funding through the &lt;a href=&quot;https://www.raais.org/&quot;&gt;RAAIS Foundation&lt;/a&gt;, a non-profit organization with a mission to advance education and research in artificial intelligence for the common good. We encourage interested parties to apply to one or more of the fellowships listed below.&lt;/p&gt;

&lt;h2 id=&quot;tools-powering-the-future-of-privacy-preserving-ml&quot;&gt;Tools Powering the Future of Privacy-Preserving ML&lt;/h2&gt;

&lt;p&gt;The next generation of privacy-preserving open source tools enable ML researchers to easily experiment with ML models using secure computing techniques without needing to be cryptography experts. By integrating with PyTorch, PySyft and CrypTen offer familiar environments for ML developers to research and apply these techniques as part of their work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PySyft&lt;/strong&gt; is a Python library for secure and private ML developed by the OpenMined community. It is a flexible, easy-to-use library that makes secure computation techniques like &lt;a href=&quot;https://en.wikipedia.org/wiki/Secure_multi-party_computation&quot;&gt;multi-party computation (MPC)&lt;/a&gt; and privacy-preserving techniques like &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;&gt;differential privacy&lt;/a&gt; accessible to the ML community. It prioritizes ease of use and focuses on integrating these techniques into end-user use cases like federated learning with mobile phones and other edge devices, encrypted ML as a service, and privacy-preserving data science.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CrypTen&lt;/strong&gt; is a framework built on PyTorch that enables private and secure ML for the PyTorch community. It is the first step along the journey towards a privacy-preserving mode in PyTorch that will make secure computing techniques accessible beyond cryptography researchers. It currently implements &lt;a href=&quot;https://en.wikipedia.org/wiki/Secure_multi-party_computation&quot;&gt;secure multiparty computation&lt;/a&gt; with the goal of offering other secure computing backends in the near future. Other benefits to ML researchers include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is &lt;strong&gt;ML first&lt;/strong&gt; and presents secure computing techniques via a CrypTensor object that looks and feels exactly like a PyTorch Tensor. This allows the user to use automatic differentiation and neural network modules akin to those in PyTorch.&lt;/li&gt;
  &lt;li&gt;The framework focuses on &lt;strong&gt;scalability and performance&lt;/strong&gt; and is built with real-world challenges in mind.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The focus areas for CrypTen and PySyft are naturally aligned and complement each other. The former focuses on building support for various secure and privacy preserving techniques on PyTorch through an encrypted tensor abstraction, while the latter focuses on end user use cases like deployment on edge devices and a user friendly data science platform.&lt;/p&gt;

&lt;p&gt;Working together will enable PySyft to use CrypTen as a backend for encrypted tensors. This can lead to an increase in performance for PySyft and the adoption of CrypTen as a runtime by PySyft’s userbase. In addition to this, PyTorch is also adding cryptography friendly features such as support for cryptographically secure random number generation. Over the long run, this allows each library to focus exclusively on its core competencies while enjoying the benefits of the synergistic relationship.&lt;/p&gt;

&lt;h2 id=&quot;new-funding-for-openmined-contributors&quot;&gt;New Funding for OpenMined Contributors&lt;/h2&gt;

&lt;p&gt;We are especially excited to announce that the PyTorch team has invested $250,000 to support OpenMined in furthering the development and proliferation of privacy-preserving ML. This gift will be facilitated via the &lt;a href=&quot;https://www.raais.org/&quot;&gt;RAAIS Foundation&lt;/a&gt; and will be available immediately to support paid fellowship grants for the OpenMined community.&lt;/p&gt;

&lt;h2 id=&quot;how-to-get-involved&quot;&gt;How to get involved&lt;/h2&gt;

&lt;p&gt;Thanks to the support from the PyTorch team, OpenMined is able to offer three different opportunities for you to participate in the project’s development. Each of these fellowships furthers our shared mission to lower the barrier-to-entry for privacy-preserving ML and to create a more privacy-preserving world.&lt;/p&gt;

&lt;h3 id=&quot;core-pysyft-crypten-integration-fellowships&quot;&gt;Core PySyft CrypTen Integration Fellowships&lt;/h3&gt;

&lt;p&gt;During these fellowships, we will integrate CrypTen as a supported backend for encrypted computation in PySyft. This will allow for the high-performance, secure multi-party computation capabilities of CrypTen to be used alongside other important tools in PySyft such as differential privacy and federated learning. For more information on the roadmap and how to apply for a paid fellowship, check out the project’s &lt;a href=&quot;https://blog.openmined.org/openmined-pytorch-fellowship-crypten-project&quot;&gt;call for contributors&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;federated-learning-on-mobile-web-and-iot-devices&quot;&gt;Federated Learning on Mobile, Web, and IoT Devices&lt;/h3&gt;

&lt;p&gt;During these fellowships, we will be extending PyTorch with the ability to perform federated learning across mobile, web, and IoT devices. To this end, a PyTorch front-end will be able to coordinate across federated learning backends that run in Javascript, Kotlin, Swift, and Python. Furthermore, we will also extend PySyft with the ability to coordinate these backends using peer-to-peer connections, providing low latency and the ability to run secure aggregation as a part of the protocol. For more information on the roadmap and how to apply for a paid fellowship, check out the project’s &lt;a href=&quot;https://blog.openmined.org/announcing-the-pytorch-openmined-federated-learning-fellowships&quot;&gt;call for contributors&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;development-challenges&quot;&gt;Development Challenges&lt;/h3&gt;

&lt;p&gt;Over the coming months, we will issue regular open competitions for increasing the performance and security of the PySyft and PyGrid codebases. For performance-related challenges, contestants will compete (for a cash prize) to make a specific PySyft demo (such as federated learning) as fast as possible. For security-related challenges, contestants will compete to hack into a PyGrid server. The first to demonstrate their ability will win the cash bounty! For more information on the challenges and to sign up to receive emails when each challenge is opened, &lt;a href=&quot;http://blog.openmined.org/announcing-the-openmined-pytorch-development-challenges&quot;&gt;sign up here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To apply, select one of the above projects and identify a role that matches your strengths!&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;Andrew, Laurens, Joe, and Shubho&lt;/p&gt;</content><author><name>Andrew Trask (OpenMined/U.Oxford), Shubho Sengupta, Laurens van der Maaten, Joe Spisak</name></author><summary type="html">Many applications of machine learning (ML) pose a range of security and privacy challenges.</summary></entry><entry><title type="html">PyTorch 1.3 adds mobile, privacy, quantization, and named tensors</title><link href="https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/" rel="alternate" type="text/html" title="PyTorch 1.3 adds mobile, privacy, quantization, and named tensors" /><published>2019-10-10T00:00:00-07:00</published><updated>2019-10-10T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/">&lt;p&gt;PyTorch continues to gain momentum because of its focus on meeting the needs of researchers, its streamlined workflow for production use, and most of all because of the enthusiastic support it has received from the AI community. PyTorch citations in papers on ArXiv &lt;a href=&quot;https://www.oreilly.com/ideas/one-simple-graphic-researchers-love-pytorch-and-tensorflow?fbclid=IwAR3kYmlyD7zky37IYFu0cafQn7yemhl8P-7MNyB30z0q5RDzxcTOrP8kxDk&quot;&gt;grew 194 percent in the first half of 2019 alone, as noted by O’Reilly&lt;/a&gt;, and the number of contributors to the platform has grown more than 50 percent over the last year, to nearly 1,200. Facebook, Microsoft, Uber, and other organizations across industries are increasingly using it as the foundation for their most important machine learning (ML) research and production workloads.&lt;/p&gt;

&lt;p&gt;We are now advancing the platform further with the release of PyTorch 1.3, which includes experimental support for features such as seamless model deployment to mobile devices, model quantization for better performance at inference time, and front-end improvements, like the ability to name tensors and create clearer code with less need for inline comments. We’re also launching a number of additional tools and libraries to support model interpretability and bringing multimodal research to production.&lt;/p&gt;

&lt;p&gt;Additionally, we’ve collaborated with Google and Salesforce to add broad support for Cloud Tensor Processing Units, providing a significantly accelerated option for training large-scale deep neural networks. &lt;a href=&quot;https://data.aliyun.com/bigdata/pai-pytorch?spm=5176.12825654.a9ylfrljh.d112.7b652c4ayuOO4M&amp;amp;scm=20140722.1068.1.1098&amp;amp;aly_as=-PvJ5e4c&quot;&gt;Alibaba Cloud&lt;/a&gt; also joins Amazon Web Services, Microsoft Azure, and Google Cloud as supported cloud platforms for PyTorch users. You can get started now at &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;pytorch.org&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;pytorch-13&quot;&gt;PyTorch 1.3&lt;/h1&gt;

&lt;p&gt;The 1.3 release of PyTorch brings significant new features, including experimental support for mobile device deployment, eager mode quantization at 8-bit integer, and the ability to name tensors. With each of these enhancements, we look forward to additional contributions and improvements from the PyTorch community.&lt;/p&gt;

&lt;h2 id=&quot;named-tensors-experimental&quot;&gt;Named tensors (experimental)&lt;/h2&gt;

&lt;p&gt;Cornell University’s &lt;a href=&quot;http://nlp.seas.harvard.edu/NamedTensor&quot;&gt;Sasha Rush has argued&lt;/a&gt; that, despite its ubiquity in deep learning, the traditional implementation of tensors has significant shortcomings, such as exposing private dimensions, broadcasting based on absolute position, and keeping type information in documentation. He proposed named tensors as an alternative approach.&lt;/p&gt;

&lt;p&gt;Today, we name and access dimensions by comment:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Tensor[N, C, H, W]&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But naming explicitly leads to more readable and maintainable code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;NCHW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NCHW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;quantization-experimental&quot;&gt;Quantization (experimental)&lt;/h2&gt;

&lt;p&gt;It’s important to make efficient use of both server-side and on-device compute resources when developing ML applications. To support more efficient deployment on servers and edge devices, PyTorch 1.3 now supports 8-bit model quantization using the familiar eager mode Python API. Quantization refers to techniques used to perform computation and storage at reduced precision, such as 8-bit integer. This currently experimental feature includes support for post-training quantization, dynamic quantization, and quantization-aware training. It leverages the &lt;a href=&quot;https://github.com/pytorch/FBGEMM&quot;&gt;FBGEMM&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/QNNPACK&quot;&gt;QNNPACK&lt;/a&gt; state-of-the-art quantized kernel back ends, for x86 and ARM CPUs, respectively, which are integrated with PyTorch and now share a common API.&lt;/p&gt;

&lt;p&gt;To learn more about the design and architecture, check out the API docs &lt;a href=&quot;https://pytorch.org/docs/master/quantization.html&quot;&gt;here&lt;/a&gt;, and get started with any of the supported techniques using the tutorials available &lt;a href=&quot;https://pytorch.org/tutorials/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-mobile-experimental&quot;&gt;PyTorch mobile (experimental)&lt;/h2&gt;

&lt;p&gt;Running ML on edge devices is growing in importance as applications continue to demand lower latency. It is also a foundational element for privacy-preserving techniques such as federated learning. To enable more efficient on-device ML, PyTorch 1.3 now supports an end-to-end workflow from Python to deployment on iOS and Android.&lt;/p&gt;

&lt;p&gt;This is an early, experimental release, optimized for end-to-end development. Coming releases will focus on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optimization for size: Build level optimization and selective compilation depending on the operators needed for user applications (i.e., you pay binary size for only the operators you need)&lt;/li&gt;
  &lt;li&gt;Performance: Further improvements to performance and coverage on mobile CPUs and GPUs&lt;/li&gt;
  &lt;li&gt;High level API: Extend mobile native APIs to cover common preprocessing and integration tasks needed for incorporating ML in mobile applications. e.g. Computer vision and NLP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more or get started on Android or iOS &lt;a href=&quot;http://pytorch.org/mobile&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;new-tools-for-model-interpretability-and-privacy&quot;&gt;New tools for model interpretability and privacy&lt;/h1&gt;

&lt;h2 id=&quot;captum&quot;&gt;Captum&lt;/h2&gt;

&lt;p&gt;As models become ever more complex, it is increasingly important to develop new methods for model interpretability. To help address this need, we’re launching Captum, a tool to help developers working in PyTorch understand why their model generates a specific output. Captum provides state-of-the-art tools to understand how the importance of specific neurons and layers and affect predictions made by the models. Captum’s algorithms include integrated gradients, conductance, SmoothGrad and VarGrad, and DeepLift.&lt;/p&gt;

&lt;p&gt;The example below shows how to apply model interpretability algorithms on a pretrained ResNet model and then visualize the attributions for each pixel by overlaying them on the image.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;noise_tunnel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NoiseTunnel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;integrated_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;attributions_ig_nt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;noise_tunnel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attribute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nt_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'smoothgrad_sq'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_label_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;viz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;visualize_image_attr_multiple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;original_image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;heat_map&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                      &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;all&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;positive&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attributions_ig_nt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformed_img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;show_colorbar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/Captum 1.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/Captum 2.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Learn more about Captum at &lt;a href=&quot;https://www.captum.ai/&quot;&gt;captum.ai&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;crypten&quot;&gt;CrypTen&lt;/h2&gt;

&lt;p&gt;Practical applications of ML via cloud-based or machine-learning-as-a-service (MLaaS) platforms pose a range of security and privacy challenges. In particular, users of these platforms may not want or be able to share unencrypted data, which prevents them from taking full advantage of ML tools. To address these challenges, the ML community is exploring a number of technical approaches, at various levels of maturity. These include homomorphic encryption, secure multiparty computation, trusted execution environments, on-device computation, and differential privacy.&lt;/p&gt;

&lt;p&gt;To provide a better understanding of how some of these technologies can be applied, we are releasing CrypTen, a new community-based research platform for taking the field of privacy-preserving ML forward. Learn more about CrypTen &lt;a href=&quot;https://ai.facebook.com/blog/crypten-a-new-research-tool-for-secure-machine-learning-with-pytorch&quot;&gt;here&lt;/a&gt;. It is available on GitHub &lt;a href=&quot;https://github.com/facebookresearch/CrypTen&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;tools-for-multimodal-ai-systems&quot;&gt;Tools for multimodal AI systems&lt;/h1&gt;

&lt;p&gt;Digital content is often made up of several modalities, such as text, images, audio, and video. For example, a single public post might contain an image, body text, a title, a video, and a landing page. Even one particular component may have more than one modality, such as a video that contains both visual and audio signals, or a landing page that is composed of images, text, and HTML sources.&lt;/p&gt;

&lt;p&gt;The ecosystem of tools and libraries that work with PyTorch offer enhanced ways to address the challenges of building multimodal ML systems. Here are some of the latest libraries launching today:&lt;/p&gt;

&lt;h2 id=&quot;detectron2&quot;&gt;Detectron2&lt;/h2&gt;

&lt;p&gt;Object detection and segmentation are used for tasks ranging from autonomous vehicles to content understanding for platform integrity. To advance this work, Facebook AI Research (FAIR) is releasing Detectron2, an object detection library now implemented in PyTorch. Detectron2 provides support for the latest models and tasks, increased flexibility to aid computer vision research, and improvements in maintainability and scalability to support production use cases.&lt;/p&gt;

&lt;p&gt;Detectron2 is available &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;here&lt;/a&gt; and you can learn more &lt;a href=&quot;https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;speech-extensions-to-fairseq&quot;&gt;Speech extensions to fairseq&lt;/h2&gt;

&lt;p&gt;Language translation and audio processing are critical components in systems and applications such as search, translation, speech, and assistants. There has been tremendous progress in these fields recently thanks to the development of new architectures like transformers, as well as large-scale pretraining methods. We’ve extended fairseq, a framework for sequence-to-sequence applications such as language translation, to include support for end-to-end learning for speech and audio recognition tasks.These extensions to fairseq enable faster exploration and prototyping of new speech research ideas while offering a clear path to production.&lt;/p&gt;

&lt;p&gt;Get started with fairseq &lt;a href=&quot;https://github.com/pytorch/fairseq/tree/master/examples/speech_recognition&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;cloud-provider-and-hardware-ecosystem-support&quot;&gt;Cloud provider and hardware ecosystem support&lt;/h1&gt;

&lt;p&gt;Cloud providers such as Amazon Web Services, Microsoft Azure, and Google Cloud provide extensive support for anyone looking to develop ML on PyTorch and deploy in production. We’re excited to share the general availability of Google Cloud TPU support and a newly launched integration with Alibaba Cloud. We’re also expanding hardware ecosystem support.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Google Cloud TPU support now broadly available. To accelerate the largest-scale machine learning (ML) applications deployed today and enable rapid development of the ML applications of tomorrow, Google created custom silicon chips called Tensor Processing Units (&lt;a href=&quot;https://cloud.google.com/tpu/&quot;&gt;TPUs&lt;/a&gt;). When assembled into multi-rack ML supercomputers called &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-pods-break-ai-training-records&quot;&gt;Cloud TPU Pods&lt;/a&gt;, these TPUs can complete ML workloads in minutes or hours that previously took days or weeks on other systems. Engineers from Facebook, Google, and Salesforce worked together to enable and pilot Cloud TPU support in PyTorch, including experimental support for Cloud TPU Pods. PyTorch support for Cloud TPUs is also available in Colab. Learn more about how to get started with PyTorch on Cloud TPUs &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Alibaba adds support for PyTorch in Alibaba Cloud. The initial integration involves a one-click solution for PyTorch 1.x, Data Science Workshop notebook service, distributed training with Gloo/NCCL, as well as seamless integration with Alibaba IaaS such as OSS, ODPS, and NAS. Together with the toolchain provided by Alibaba, we look forward to significantly reducing the overhead necessary for adoption, as well as helping Alibaba Cloud’s global customer base leverage PyTorch to develop new AI applications.&lt;/li&gt;
  &lt;li&gt;ML hardware ecosystem expands. In addition to key GPU and CPU partners, the PyTorch ecosystem has also enabled support for dedicated ML accelerators. Updates from &lt;a href=&quot;https://www.intel.ai/nnpi-glow-pytorch/&quot;&gt;Intel&lt;/a&gt; and &lt;a href=&quot;https://medium.com/@HabanaLabs/unlocking-ai-scaling-through-software-and-hardware-interface-standardization-77561cb7598b&quot;&gt;Habana&lt;/a&gt; showcase how PyTorch, connected to the Glow optimizing compiler, enables developers to utilize these market-specific solutions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;growth-in-the-pytorch-community&quot;&gt;Growth in the PyTorch community&lt;/h1&gt;

&lt;p&gt;As an open source, community-driven project, PyTorch benefits from wide range of contributors bringing new capabilities to the ecosystem. Here are some recent examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mila SpeechBrain aims to provide an open source, all-in-one speech toolkit based on PyTorch. The goal is to develop a single, flexible, user-friendly toolkit that can be used to easily develop state-of-the-art systems for speech recognition (both end to end and HMM-DNN), speaker recognition, speech separation, multi-microphone signal processing (e.g., beamforming), self-supervised learning, and many others. &lt;a href=&quot;https://speechbrain.github.io/&quot;&gt;Learn more&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;SpaCy is a new wrapping library with consistent and easy-to-use interfaces to several models, in order to extract features to power NLP pipelines. Support is provided for via spaCy’s standard training API. The library also calculates an alignment so the transformer features can be related back to actual words instead of just wordpieces. &lt;a href=&quot;https://explosion.ai/blog/spacy-pytorch-transformers&quot;&gt;Learn more&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;HuggingFace PyTorch-Transformers (formerly known as pytorch-pretrained-bert is a library of state-of-the-art pretrained models for Natural Language Processing (NLP). The library currently contains PyTorch implementations, pretrained model weights, usage scripts, and conversion utilities for models such as BERT, GPT-2, RoBERTa, and DistilBERT. It has also grown quickly, with more than 13,000 GitHub stars and a broad set of users. &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;Learn more&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;PyTorch Lightning is a Keras-like ML library for PyTorch. It leaves core training and validation logic to you and automates the rest. Reproducibility is a crucial requirement for many fields of research, including those based on ML techniques. As the number of research papers submitted to arXiv and conferences skyrockets into the tens of thousands, scaling reproducibility becomes difficult. &lt;a href=&quot;https://github.com/williamFalcon/pytorch-lightning&quot;&gt;Learn more&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We recently held the first online Global PyTorch Summer Hackathon, where researchers and developers around the world were invited to build innovative new projects with PyTorch. Nearly 1,500 developers participated, submitting projects ranging from livestock disease detection to AI-powered financial assistants. The winning projects were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Torchmeta, which provides extensions for PyTorch to simplify the development of meta-learning algorithms in PyTorch. It features a unified interface inspired by TorchVision for both few-shot classification and regression problems, to allow easy benchmarking on multiple data sets to aid with reproducibility.&lt;/li&gt;
  &lt;li&gt;Open-Unmix, a system for end-to-end music demixing with PyTorch. Demixing separates the individual instruments or vocal track from any stereo recording.&lt;/li&gt;
  &lt;li&gt;Endless AI-Generated Tees, a store featuring AI-generated T-shirt designs that can be purchased and delivered worldwide. The system uses a state-of-the-art generative model (StyleGAN) that was built with PyTorch and then trained on modern art.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Visit &lt;a href=&quot;https://pytorch.org/&quot;&gt;pytorch.org&lt;/a&gt; to learn more and get started with PyTorch 1.3 and the latest libraries and ecosystem projects. We look forward to the contributions, exciting research advancements, and real-world applications that the community builds with PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">PyTorch continues to gain momentum because of its focus on meeting the needs of researchers, its streamlined workflow for production use, and most of all because of the enthusiastic support it has received from the AI community. PyTorch citations in papers on ArXiv grew 194 percent in the first half of 2019 alone, as noted by O’Reilly, and the number of contributors to the platform has grown more than 50 percent over the last year, to nearly 1,200. Facebook, Microsoft, Uber, and other organizations across industries are increasingly using it as the foundation for their most important machine learning (ML) research and production workloads.</summary></entry><entry><title type="html">New Releases: PyTorch 1.2, torchtext 0.4, torchaudio 0.3, and torchvision 0.4</title><link href="https://pytorch.org/blog/pytorch-1.2-and-domain-api-release/" rel="alternate" type="text/html" title="New Releases: PyTorch 1.2, torchtext 0.4, torchaudio 0.3, and torchvision 0.4" /><published>2019-08-08T00:00:00-07:00</published><updated>2019-08-08T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.2-and-domain-api-release</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.2-and-domain-api-release/">&lt;p&gt;Since the release of PyTorch 1.0, we’ve seen the community expand to add new tools, contribute to a growing set of models available in the PyTorch Hub, and continually increase usage in both research and production.&lt;/p&gt;

&lt;p&gt;From a core perspective, PyTorch has continued to add features to support both research and production usage, including the ability to bridge these two worlds via &lt;a href=&quot;https://pytorch.org/docs/stable/jit.html&quot;&gt;TorchScript&lt;/a&gt;. Today, we are excited to announce that we have four new releases including PyTorch 1.2, torchvision 0.4, torchaudio 0.3, and torchtext 0.4. You can get started now with any of these releases at &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;pytorch.org&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;pytorch-12&quot;&gt;PyTorch 1.2&lt;/h1&gt;

&lt;p&gt;With PyTorch 1.2, the open source ML framework takes a major step forward for production usage with the addition of an improved and more polished TorchScript environment. These improvements make it even easier to ship production models, expand support for exporting ONNX formatted models, and enhance module level support for Transformers. In addition to these new features, &lt;a href=&quot;https://pytorch.org/docs/stable/tensorboard.html&quot;&gt;TensorBoard&lt;/a&gt; is now no longer experimental - you can simply type &lt;code class=&quot;highlighter-rouge&quot;&gt;from torch.utils.tensorboard import SummaryWriter&lt;/code&gt; to get started.&lt;/p&gt;

&lt;h2 id=&quot;torchscript-improvements&quot;&gt;TorchScript Improvements&lt;/h2&gt;

&lt;p&gt;Since its release in PyTorch 1.0, TorchScript has provided a path to production for eager PyTorch models. The TorchScript compiler converts PyTorch models to a statically typed graph representation, opening up opportunities for
optimization and execution in constrained environments where Python is not available. You can incrementally convert your model to TorchScript, mixing compiled code seamlessly with Python.&lt;/p&gt;

&lt;p&gt;PyTorch 1.2 significantly expands TorchScript’s support for the subset of Python used in PyTorch models and delivers a new, easier-to-use API for compiling your models to TorchScript. See the &lt;a href=&quot;https://pytorch.org/docs/master/jit.html#migrating-to-pytorch-1-2-recursive-scripting-api&quot;&gt;migration guide&lt;/a&gt; for details. Below is an example usage of the new API:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Compile the model code to a static representation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_script_module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Save the compiled code and model data so it can be loaded elsewhere&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_script_module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_script_module.pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To learn more, see our &lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript.html&quot;&gt;Introduction to TorchScript&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;Loading a
PyTorch Model in C++&lt;/a&gt; tutorials.&lt;/p&gt;

&lt;h2 id=&quot;expanded-onnx-export&quot;&gt;Expanded ONNX Export&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;http://onnx.ai/&quot;&gt;ONNX&lt;/a&gt; community continues to grow with an open &lt;a href=&quot;https://github.com/onnx/onnx/wiki/Expanded-ONNX-Steering-Committee-Announced!&quot;&gt;governance structure&lt;/a&gt; and additional steering committee members, special interest groups (SIGs), and working groups (WGs). In collaboration with Microsoft, we’ve added full support to export ONNX Opset versions 7(v1.2), 8(v1.3), 9(v1.4) and 10 (v1.5). We’ve have also enhanced the constant folding pass to support Opset 10, the latest available version of ONNX. ScriptModule has also been improved including support for multiple outputs, tensor factories, and tuples as inputs and outputs. Additionally, users are now able to register their own symbolic to export custom ops, and specify the dynamic dimensions of inputs during export. Here is a summary of the all of the major improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support for multiple Opsets including the ability to export dropout, slice, flip, and interpolate in Opset 10.&lt;/li&gt;
  &lt;li&gt;Improvements to ScriptModule including support for multiple outputs, tensor factories, and tuples as inputs and outputs.&lt;/li&gt;
  &lt;li&gt;More than a dozen additional PyTorch operators supported including the ability to export a custom operator.&lt;/li&gt;
  &lt;li&gt;Many big fixes and test infra improvements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can try out the latest tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html&quot;&gt;here&lt;/a&gt;, contributed by @lara-hdr at Microsoft. A big thank you to the entire Microsoft team for all of their hard work to make this release happen!&lt;/p&gt;

&lt;h2 id=&quot;nntransformer&quot;&gt;nn.Transformer&lt;/h2&gt;

&lt;p&gt;In PyTorch 1.2, we now include a standard &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=transformer#torch.nn.Transformer&quot;&gt;nn.Transformer&lt;/a&gt; module, based on the paper “&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention is All You Need&lt;/a&gt;”. The &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt; module relies entirely on an &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=nn%20multiheadattention#torch.nn.MultiheadAttention&quot;&gt;attention mechanism&lt;/a&gt; to draw global dependencies between input and output.  The individual components of the &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt; module are designed so they can be adopted independently. For example, the &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder&quot;&gt;nn.TransformerEncoder&lt;/a&gt; can be used by itself, without the larger &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt;. The new APIs include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerEncoder&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerEncoderLayer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerDecoder&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerDecoderLayer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/transformer.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#transformer-layers&quot;&gt;Transformer Layers&lt;/a&gt; documentation for more information. See &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt; for the full PyTorch 1.2 release notes.&lt;/p&gt;

&lt;h1 id=&quot;domain-api-library-updates&quot;&gt;Domain API Library Updates&lt;/h1&gt;

&lt;p&gt;PyTorch domain libraries like torchvision, torchtext, and torchaudio provide convenient access to common datasets, models, and transforms that can be used to quickly create a state-of-the-art baseline. Moreover, they also provide common abstractions to reduce boilerplate code that users might have to otherwise repeatedly write. Since research domains have distinct requirements, an ecosystem of specialized libraries called domain APIs (DAPI) has emerged around PyTorch to simplify the development of new and existing algorithms in a number of fields. We’re excited to release three updated DAPI libraries for text, audio, and vision that compliment the PyTorch 1.2 core release.&lt;/p&gt;

&lt;h2 id=&quot;torchaudio-03-with-kaldi-compatibility-new-transforms&quot;&gt;Torchaudio 0.3 with Kaldi Compatibility, New Transforms&lt;/h2&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/spectrograms.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Torchaudio specializes in machine understanding of audio waveforms. It is an ML library that provides relevant signal processing functionality (but is not a general signal processing library). It leverages PyTorch’s GPU support to provide many tools and transformations for waveforms to make data loading and standardization easier and more readable. For example, it offers data loaders for waveforms using sox, and transformations such as spectrograms, resampling, and mu-law encoding and decoding.&lt;/p&gt;

&lt;p&gt;We are happy to announce the availability of torchaudio 0.3.0, with a focus on standardization and complex numbers, a transformation (resample) and two new functionals (phase_vocoder, ISTFT), Kaldi compatibility, and a new tutorial. Torchaudio was redesigned to be an extension of PyTorch and a part of the domain APIs (DAPI) ecosystem.&lt;/p&gt;

&lt;h3 id=&quot;standardization&quot;&gt;Standardization&lt;/h3&gt;

&lt;p&gt;Significant effort in solving machine learning problems goes into data preparation. In this new release, we’ve updated torchaudio’s interfaces for its transformations to standardize around the following vocabulary and conventions.&lt;/p&gt;

&lt;p&gt;Tensors are assumed to have channel as the first dimension and time as the last dimension (when applicable). This makes it consistent with PyTorch’s dimensions. For size names, the prefix &lt;code class=&quot;highlighter-rouge&quot;&gt;n_&lt;/code&gt; is used (e.g. “a tensor of size (&lt;code class=&quot;highlighter-rouge&quot;&gt;n_freq&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;n_mel&lt;/code&gt;)”) whereas dimension names do not have this prefix (e.g. “a tensor of dimension (channel, time)”). The input of all transforms and functions now assumes channel first. This is done to be consistent with PyTorch, which has channel followed by the number of samples. The channel parameter of all transforms and functions is now deprecated.&lt;/p&gt;

&lt;p&gt;The output of &lt;code class=&quot;highlighter-rouge&quot;&gt;STFT&lt;/code&gt; is (channel, frequency, time, 2), meaning for each channel, the columns are the Fourier transform of a certain window, so as we travel horizontally we can see each column (the Fourier transformed waveform) change over time. This matches the output of librosa so we no longer need to transpose in our test comparisons with &lt;code class=&quot;highlighter-rouge&quot;&gt;Spectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MelScale&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MelSpectrogram&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;MFCC&lt;/code&gt;. Moreover, because of these new conventions, we deprecated &lt;code class=&quot;highlighter-rouge&quot;&gt;LC2CL&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;BLC2CBL&lt;/code&gt; which were used to transfer from one shape of signal to another.&lt;/p&gt;

&lt;p&gt;As part of this release, we’re also introducing support for complex numbers via tensors of dimension (…, 2), and providing &lt;code class=&quot;highlighter-rouge&quot;&gt;magphase&lt;/code&gt; to convert such a tensor into its magnitude and phase, and similarly &lt;code class=&quot;highlighter-rouge&quot;&gt;complex_norm&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;angle&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The details of the standardization are provided in the &lt;a href=&quot;https://github.com/pytorch/audio/blob/v0.3.0/README.md#Conventions&quot;&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;functionals-transformations-and-kaldi-compatibility&quot;&gt;Functionals, Transformations, and Kaldi Compatibility&lt;/h3&gt;

&lt;p&gt;Prior to the standardization, we separated state and computation into &lt;code class=&quot;highlighter-rouge&quot;&gt;torchaudio.transforms&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torchaudio.functional&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As part of the transforms, we’re adding a new transformation in 0.3.0: &lt;code class=&quot;highlighter-rouge&quot;&gt;Resample&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;Resample&lt;/code&gt; can upsample or downsample a waveform to a different frequency.&lt;/p&gt;

&lt;p&gt;As part of the functionals, we’re introducing: &lt;code class=&quot;highlighter-rouge&quot;&gt;phase_vocoder&lt;/code&gt;, a phase vocoder to change the speed of a waveform without changing its pitch, and &lt;code class=&quot;highlighter-rouge&quot;&gt;ISTFT&lt;/code&gt;, the inverse &lt;code class=&quot;highlighter-rouge&quot;&gt;STFT&lt;/code&gt; implemented to be compatible with STFT provided by PyTorch. This separation allows us to make functionals weak scriptable and to utilize JIT in 0.3.0. We thus have JIT and CUDA support for the following transformations: &lt;code class=&quot;highlighter-rouge&quot;&gt;Spectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;AmplitudeToDB&lt;/code&gt; (previously named &lt;code class=&quot;highlighter-rouge&quot;&gt;SpectrogramToDB&lt;/code&gt;), &lt;code class=&quot;highlighter-rouge&quot;&gt;MelScale&lt;/code&gt;,
&lt;code class=&quot;highlighter-rouge&quot;&gt;MelSpectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MFCC&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MuLawEncoding&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MuLawDecoding&lt;/code&gt; (previously named &lt;code class=&quot;highlighter-rouge&quot;&gt;MuLawExpanding&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;We now also provide a compatibility interface with Kaldi to ease onboarding and reduce a user’s code dependency on Kaldi. We now have an interface for &lt;code class=&quot;highlighter-rouge&quot;&gt;spectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fbank&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;resample_waveform&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;new-tutorial&quot;&gt;New Tutorial&lt;/h3&gt;

&lt;p&gt;To showcase the new conventions and transformations, we have a &lt;a href=&quot;https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html&quot;&gt;new tutorial&lt;/a&gt; demonstrating how to preprocess waveforms using torchaudio. This tutorial walks through an example of loading a waveform and applying some of the available transformations to it.&lt;/p&gt;

&lt;p&gt;We are excited to see an active community around torchaudio and eager to further grow and support it. We encourage you to go ahead and experiment for yourself with this tutorial and the two datasets that are available: VCTK and YESNO! They have an interface to download the datasets and preprocess them in a convenient format. You can find the details in the release notes &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchtext-04-with-supervised-learning-datasets&quot;&gt;Torchtext 0.4 with supervised learning datasets&lt;/h2&gt;

&lt;p&gt;A key focus area of torchtext is to provide the fundamental elements to help accelerate NLP research. This includes easy access to commonly used datasets and basic preprocessing pipelines for working on raw text based data. The torchtext 0.4.0 release includes several popular supervised learning baselines with “one-command” data loading. A &lt;a href=&quot;https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;tutorial&lt;/a&gt; is included to show how to use the new datasets for text classification analysis. We also added and improved on a few functions such as &lt;a href=&quot;https://pytorch.org/text/data.html?highlight=get_tokenizer#torchtext.data.get_tokenizer&quot;&gt;get_tokenizer&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/text/vocab.html#build-vocab-from-iterator&quot;&gt;build_vocab_from_iterator&lt;/a&gt; to make it easier to implement future datasets. Additional examples can be found &lt;a href=&quot;https://github.com/pytorch/text/tree/master/examples/text_classification&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Text classification is an important task in Natural Language Processing with many applications, such as sentiment analysis. The new release includes several popular &lt;a href=&quot;https://pytorch.org/text/datasets.html?highlight=textclassification#torchtext.datasets.TextClassificationDataset&quot;&gt;text classification datasets&lt;/a&gt; for supervised learning including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AG_NEWS&lt;/li&gt;
  &lt;li&gt;SogouNews&lt;/li&gt;
  &lt;li&gt;DBpedia&lt;/li&gt;
  &lt;li&gt;YelpReviewPolarity&lt;/li&gt;
  &lt;li&gt;YelpReviewFull&lt;/li&gt;
  &lt;li&gt;YahooAnswers&lt;/li&gt;
  &lt;li&gt;AmazonReviewPolarity&lt;/li&gt;
  &lt;li&gt;AmazonReviewFull&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each dataset comes with two parts (train vs. test), and can be easily loaded with a single command. The datasets also support an ngrams feature to capture the partial information about the local word order. Take a look at the tutorial &lt;a href=&quot;https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;here&lt;/a&gt; to learn more about how to use the new datasets for supervised problems such as text classification analysis.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchtext.datasets.text_classification&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATASETS&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATASETS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'AG_NEWS'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrams&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In addition to the domain library, PyTorch provides many tools to make data loading easy. Users now can load and preprocess the text classification datasets with some well supported tools, like &lt;a href=&quot;https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html&quot;&gt;torch.utils.data.DataLoader&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/master/data.html#torch.utils.data.IterableDataset&quot;&gt;torch.utils.data.IterableDataset&lt;/a&gt;. Here are a few lines to wrap the data with DataLoader. More examples can be found &lt;a href=&quot;https://github.com/pytorch/text/tree/master/examples/text_classification&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collate_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check out the release notes &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt; to learn more and try out the &lt;a href=&quot;http://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;tutorial here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchvision-04-with-support-for-video&quot;&gt;Torchvision 0.4 with Support for Video&lt;/h2&gt;

&lt;p&gt;Video is now a first-class citizen in torchvision, with support for data loading, datasets, pre-trained models, and transforms. The 0.4 release of torchvision includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Efficient IO primitives for reading/writing video files (including audio), with support for arbitrary encodings and formats.&lt;/li&gt;
  &lt;li&gt;Standard video datasets, compatible with &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataset&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.DataLoader&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Pre-trained models built on the Kinetics-400 dataset for action classification on videos (including the training scripts).&lt;/li&gt;
  &lt;li&gt;Reference training scripts for training your own video models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We wanted working with video data in PyTorch to be as straightforward as possible, without compromising too much on performance.
As such, we avoid the steps that would require re-encoding the videos beforehand, as it would involve:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A preprocessing step which duplicates the dataset in order to re-encode it.&lt;/li&gt;
  &lt;li&gt;An overhead in time and space because this re-encoding is time-consuming.&lt;/li&gt;
  &lt;li&gt;Generally, an external script should be used to perform the re-encoding.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, we provide APIs such as the utility class, &lt;code class=&quot;highlighter-rouge&quot;&gt;VideoClips&lt;/code&gt;, that simplifies the task of enumerating all possible clips of fixed size in a list of video files by creating an index of all clips in a set of videos. It also allows you to specify a fixed frame-rate for the videos. An example of the API is provided below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.datasets.video_utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VideoClips&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyVideoDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_paths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_clips&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VideoClips&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_paths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;clip_length_in_frames&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;frames_between_clips&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;frame_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__getitem__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;video&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;audio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_clips&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;audio&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__len__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_clips&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_clips&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Most of the user-facing API is in Python, similar to PyTorch, which makes it easily extensible. Plus, the underlying implementation is fast — torchvision decodes as little as possible from the video on-the-fly in order to return a clip from the video.&lt;/p&gt;

&lt;p&gt;Check out the torchvision 0.4 &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;release notes here&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;We look forward to continuing our collaboration with the community and hearing your feedback as we further improve and expand the PyTorch deep learning platform.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all of the contributions to this work!&lt;/em&gt;&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Since the release of PyTorch 1.0, we’ve seen the community expand to add new tools, contribute to a growing set of models available in the PyTorch Hub, and continually increase usage in both research and production.</summary></entry><entry><title type="html">Mapillary Research: Seamless Scene Segmentation and In-Place Activated BatchNorm</title><link href="https://pytorch.org/blog/mapillary-research/" rel="alternate" type="text/html" title="Mapillary Research: Seamless Scene Segmentation and In-Place Activated BatchNorm" /><published>2019-07-23T00:00:00-07:00</published><updated>2019-07-23T00:00:00-07:00</updated><id>https://pytorch.org/blog/mapillary-research</id><content type="html" xml:base="https://pytorch.org/blog/mapillary-research/">&lt;p&gt;With roads in developed countries like the US changing up to 15% annually, Mapillary addresses a growing demand for keeping maps updated by combining images from any camera into a 3D visualization of the world. Mapillary’s independent and collaborative approach enables anyone to collect, share, and use street-level images for improving maps, developing cities, and advancing the automotive industry.&lt;/p&gt;

&lt;p&gt;Today, people and organizations all over the world have contributed more than 600 million images toward Mapillary’s mission of helping people understand the world’s places through images and making this data available, with clients and partners including the World Bank, HERE, and Toyota Research Institute.&lt;/p&gt;

&lt;p&gt;Mapillary’s computer vision technology brings intelligence to maps in an unprecedented way, increasing our overall understanding of the world. &lt;a href=&quot;https://www.mapillary.com/&quot;&gt;Mapillary&lt;/a&gt; runs state-of-the-art semantic image analysis and image-based 3d modeling at scale and on all its images. In this post we discuss two recent works from Mapillary Research and their implementations in PyTorch - Seamless Scene Segmentation [1] and In-Place Activated BatchNorm [2] - generating Panoptic segmentation results and saving up to 50% of GPU memory during training, respectively.&lt;/p&gt;

&lt;h2 id=&quot;seamless-scene-segmentation&quot;&gt;Seamless Scene Segmentation&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Github project page: &lt;a href=&quot;https://github.com/mapillary/seamseg/&quot;&gt;https://github.com/mapillary/seamseg/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/seamless.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The objective of Seamless Scene Segmentation is to predict a “panoptic” segmentation [3] from an image, that is a complete labeling where each pixel is assigned with a class id and, where possible, an instance id. Like many modern CNNs dealing with instance detection and segmentation, we adopt the Mask R-CNN framework [4], using ResNet50 + FPN [5] as a backbone. This architecture works in two stages: first, the “Proposal Head” selects a set of candidate bounding boxes on the image (i.e. the proposals) that could contain an object; then, the “Mask Head” focuses on each proposal, predicting its class and segmentation mask. The output of this process is a “sparse” instance segmentation, covering only the parts of the image that contain countable objects (e.g. cars and pedestrians).&lt;/p&gt;

&lt;p&gt;To complete our panoptic approach coined Seamless Scene Segmentation, we add a third stage to Mask R-CNN. Stemming from the same backbone, the “Semantic Head” predicts a dense semantic segmentation over the whole image, also accounting for the uncountable or amorphous classes (e.g. road and sky). The outputs of the Mask and Semantic heads are finally fused using a simple non-maximum suppression algorithm to generate the final panoptic prediction. All details about the actual network architecture, used losses and underlying math can be found at the &lt;a href=&quot;https://research.mapillary.com/publication/cvpr19a&quot;&gt;project website&lt;/a&gt; for our CVPR 2019 paper [1].&lt;/p&gt;

&lt;p&gt;While several versions of Mask R-CNN are publicly available, including an &lt;a href=&quot;https://github.com/facebookresearch/Detectron&quot;&gt;official implementation&lt;/a&gt; written in Caffe2, at Mapillary we decided to build Seamless Scene Segmentation from scratch using PyTorch, in order to have full control and understanding of the whole pipeline. While doing so we encountered a couple of main stumbling blocks, and had to come up with some creative workarounds we are going to describe next.&lt;/p&gt;

&lt;h2 id=&quot;dealing-with-variable-sized-tensors&quot;&gt;Dealing with variable-sized tensors&lt;/h2&gt;

&lt;p&gt;Something that sets aside panoptic segmentation networks from traditional CNNs is the prevalence of variable-sized data. In fact, many of the quantities we are dealing with cannot be easily represented with fixed sized tensors: each image contains a different number of objects, the Proposal head can produce a different number of proposals for each image, and the images themselves can have different sizes. While this is not a problem per-se – one could just process images one at a time – we would still like to exploit batch-level parallelism as much as possible. Furthermore, when performing distributed training with multiple GPUs, &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt; expects its inputs to be batched, uniformly-sized tensors.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/packed_sequence.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Our solution to these issues is to wrap each batch of variable-sized tensors in a &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt; is little more than a glorified list class for tensors, tagging its contents as “related”, ensuring that they all share the same type, and providing useful methods like moving all the tensors to a particular device, etc. When performing light-weight operations that wouldn’t be much faster with batch-level parallelism, we simply iterate over the contents of the &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt; in a for loop. When performance is crucial, e.g. in the body of the network, we simply concatenate the contents of the PackedSequence, adding zero padding as required (like in RNNs with variable-length inputs), and keeping track of the original dimensions of each tensor.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt;s also help us deal with the second problem highlighted above. We slightly modify &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt; to recognize &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt; inputs, splitting them in equally sized chunks and distributing their contents across the GPUs.&lt;/p&gt;

&lt;h2 id=&quot;asymmetric-computational-graphs-with-distributed-data-parallel&quot;&gt;Asymmetric computational graphs with Distributed Data Parallel&lt;/h2&gt;

&lt;p&gt;Another, perhaps more subtle, peculiarity of our network is that it can generate asymmetric computational graphs across GPUs. In fact, some of the modules that compose the network are “optional”, in the sense that they are not always computed for all images. As an example, when the Proposal head doesn’t output any proposal, the Mask head is not traversed at all. If we are training on multiple GPUs with &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt;, this results in one of the replicas not computing gradients for the Mask head parameters.&lt;/p&gt;

&lt;p&gt;Prior to PyTorch 1.1, this resulted in a crash, so we had to develop a workaround. Our simple but effective solution was to compute a “fake forward pass” when no actual forward is required, i.e. something like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fake_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fake_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_correctly_shaped_fake_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fake_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fake_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fake_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fake_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fake_loss&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, we generate a batch of bogus data, pass it through the Mask head, and return a loss that always back-progates zeros to all parameters.&lt;/p&gt;

&lt;p&gt;Starting from PyTorch 1.1 this workaround is no longer required: by setting &lt;code class=&quot;highlighter-rouge&quot;&gt;find_unused_parameters=True&lt;/code&gt; in the constructor, &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt; is told to identify parameters whose gradients have not been computed by all replicas and correctly handle them. This leads to some substantial simplifications in our code base!&lt;/p&gt;

&lt;h2 id=&quot;in-place-activated-batchnorm&quot;&gt;In-place Activated BatchNorm&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Github project page: &lt;a href=&quot;https://github.com/mapillary/inplace_abn/&quot;&gt;https://github.com/mapillary/inplace_abn/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Most researchers would probably agree that there are always constraints in terms of available GPU resources, regardless if their research lab has access to only a few or multiple thousands of GPUs. In a time where at Mapillary we still worked at rather few and mostly 12GB Titan X - style prosumer GPUs, we were searching for a solution that virtually enhances the usable memory during training, so we would be able to obtain and push state-of-the-art results on dense labeling tasks like semantic segmentation. In-place activated BatchNorm is enabling us to use up to 50% more memory (at little computational overhead) and is therefore deeply integrated in all our current projects (including Seamless Scene Segmentation described above).&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/inplace_abn.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;When processing a BN-Activation-Convolution sequence in the forward pass, most deep learning frameworks (including PyTorch) need to store two big buffers, i.e. the input x of BN and the input z of Conv. This is necessary because the standard implementations of the backward passes of BN and Conv depend on their inputs to calculate the gradients. Using InPlace-ABN to replace the BN-Activation sequence, we can safely discard x, thus saving up to 50% GPU memory at training time. To achieve this, we rewrite the backward pass of BN in terms of its output y, which is in turn reconstructed from z by inverting the activation function.&lt;/p&gt;

&lt;p&gt;The only limitation of InPlace-ABN is that it requires using an invertible activation function, such as leaky relu or elu. Except for this, it can be used as a direct, drop-in replacement for BN+activation modules in any network. Our native CUDA implementation offers minimal computational overhead compared to PyTorch’s standard BN, and is available for anyone to use from here: &lt;a href=&quot;https://github.com/mapillary/inplace_abn/&quot;&gt;https://github.com/mapillary/inplace_abn/&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;synchronized-bn-with-asymmetric-graphs-and-unbalanced-batches&quot;&gt;Synchronized BN with asymmetric graphs and unbalanced batches&lt;/h2&gt;

&lt;p&gt;When training networks with synchronized SGD over multiple GPUs and/or multiple nodes, it’s common practice to compute BatchNorm statistics separately on each device. However, in our experience working with semantic and panoptic segmentation networks, we found that accumulating mean and variance across all workers can bring a substantial boost in accuracy. This is particularly true when dealing with small batches, like in Seamless Scene Segmentation where we train with a single, super-high resolution image per GPU.&lt;/p&gt;

&lt;p&gt;InPlace-ABN supports synchronized operation over multiple GPUs and multiple nodes, and, since version 1.1, this can also be achieved in the standard PyTorch library using &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#syncbatchnorm&quot;&gt;SyncBatchNorm&lt;/a&gt;. Compared to SyncBatchNorm, however, we support some additional functionality which is particularly important for Seamless Scene Segmentation: unbalanced batches and asymmetric graphs.&lt;/p&gt;

&lt;p&gt;As mentioned before, Mask R-CNN-like networks naturally give rise to variable-sized tensors. Thus, in InPlace-ABN we calculate synchronized statistics using a variant of the parallel algorithm described &lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm&quot;&gt;here&lt;/a&gt;, which properly takes into account the fact that each GPU can hold a different number of samples. PyTorch’s SyncBatchNorm is currently being revised to support this, and the improved functionality will be available in a future release.&lt;/p&gt;

&lt;p&gt;Asymmetric graphs (in the sense mentioned above) are another complicating factor one has to deal with when creating a synchronized BatchNorm implementation. Luckily, PyTorch’s distributed group functionality allows us to restrict distributed communication to a subset of workers, easily excluding those that are currently inactive. The only missing piece is that, in order to create a distributed group, each process needs to know the ids of all processes that will participate in the group, and even processes that are not part of the group need to call the &lt;code class=&quot;highlighter-rouge&quot;&gt;new_group()&lt;/code&gt; function. In InPlace-ABN we handle it with a function like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributed&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;active_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Initialize a distributed group where each process can independently decide whether to participate or not&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Gather active status from all workers&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unbind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Create group&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;First each process, including inactive ones, communicates its status to all others through an &lt;code class=&quot;highlighter-rouge&quot;&gt;all_gather&lt;/code&gt; call, then it creates the distributed group with the shared information. In the actual implementation we also include a caching mechanism for groups, since &lt;code class=&quot;highlighter-rouge&quot;&gt;new_group()&lt;/code&gt; is usually too expensive to call at each batch.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Seamless Scene Segmentation; Lorenzo Porzi, Samuel Rota Bulò, Aleksander Colovic, Peter Kontschieder; Computer Vision and Pattern Recognition (CVPR), 2019&lt;/p&gt;

&lt;p&gt;[2] In-place Activated BatchNorm for Memory-Optimized Training of DNNs; Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder; Computer Vision and Pattern Recognition (CVPR), 2018&lt;/p&gt;

&lt;p&gt;[3] Panoptic Segmentation; Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollar; Computer Vision and Pattern Recognition (CVPR), 2019&lt;/p&gt;

&lt;p&gt;[4] Mask R-CNN; Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick; International Conference on Computer Vision (ICCV), 2017&lt;/p&gt;

&lt;p&gt;[5] Feature Pyramid Networks for Object Detection; Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie; Computer Vision and Pattern Recognition (CVPR), 2017&lt;/p&gt;</content><author><name>Lorenzo Porzi, Mapillary</name></author><summary type="html">With roads in developed countries like the US changing up to 15% annually, Mapillary addresses a growing demand for keeping maps updated by combining images from any camera into a 3D visualization of the world. Mapillary’s independent and collaborative approach enables anyone to collect, share, and use street-level images for improving maps, developing cities, and advancing the automotive industry.</summary></entry></feed>