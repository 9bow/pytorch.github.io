<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pytorch.org/" rel="alternate" type="text/html" /><updated>2020-09-03T18:56:53-07:00</updated><id>https://pytorch.org/</id><title type="html">PyTorch Website</title><subtitle>Scientific Computing...</subtitle><author><name>Facebook</name></author><entry><title type="html">PyTorch framework for cryptographically secure random number generation, torchcsprng, now available</title><link href="https://pytorch.org/blog/torchcsprng-release-blog/" rel="alternate" type="text/html" title="PyTorch framework for cryptographically secure random number generation, torchcsprng, now available" /><published>2020-08-24T00:00:00-07:00</published><updated>2020-08-24T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchcsprng-release-blog</id><content type="html" xml:base="https://pytorch.org/blog/torchcsprng-release-blog/">&lt;p&gt;One of the key components of modern cryptography is the pseudorandom number generator. Katz and Lindell stated, “The use of badly designed or inappropriate random number generators can often leave a good cryptosystem vulnerable to attack. Particular care must be taken to use a random number generator that is designed for cryptographic use, rather than a ‘general-purpose’ random number generator which may be fine for some applications but not ones that are required to be cryptographically secure.”[1] Additionally, most pseudorandom number generators scale poorly to massively parallel high-performance computation because of their sequential nature. Others don’t satisfy cryptographically secure properties.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/csprng&quot;&gt;torchcsprng&lt;/a&gt; is a PyTorch &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_extension.html&quot;&gt;C++/CUDA extension&lt;/a&gt; that provides &lt;a href=&quot;https://en.wikipedia.org/wiki/Cryptographically_secure_pseudorandom_number_generator&quot;&gt;cryptographically secure pseudorandom number generators&lt;/a&gt; for PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;torchcsprng-overview&quot;&gt;torchcsprng overview&lt;/h2&gt;

&lt;p&gt;Historically, PyTorch had only two pseudorandom number generator implementations: Mersenne Twister for CPU and Nvidia’s cuRAND Philox for CUDA. Despite good performance properties, neither of them are suitable for cryptographic applications. Over the course of the past several months, the PyTorch team developed the torchcsprng extension API. Based on PyTorch dispatch mechanism and operator registration, it allows the users to extend c10::GeneratorImpl and implement their own custom pseudorandom number generator.&lt;/p&gt;

&lt;p&gt;torchcsprng generates a random 128-bit key on the CPU using one of its generators and then runs AES128 in &lt;a href=&quot;https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#Counter_(CTR)&quot;&gt;CTR mode&lt;/a&gt; either on CPU or GPU using CUDA. This then generates a random 128-bit state and applies a transformation function to map it to target tensor values. This approach is based on &lt;a href=&quot;http://www.thesalmons.org/john/random123/papers/random123sc11.pdf&quot;&gt;Parallel Random Numbers: As Easy as 1, 2, 3 (John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, D. E. Shaw Research)&lt;/a&gt;. It makes torchcsprng both crypto-secure and parallel on both CPU and CUDA.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/torchcsprng.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Since torchcsprng is a PyTorch extension, it is available on the platforms where PyTorch is available (support for Windows-CUDA will be available in the coming months).&lt;/p&gt;

&lt;h2 id=&quot;using-torchcsprng&quot;&gt;Using torchcsprng&lt;/h2&gt;

&lt;p&gt;The torchcsprng API is very simple to use and is fully compatible with the PyTorch random infrastructure:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1: Install via binary distribution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Anaconda:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchcsprng&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;pip:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchcsprng&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 2: import packages as usual but add csprng&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchcsprng&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csprng&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 3: Create a cryptographically secure pseudorandom number generator from /dev/urandom:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;urandom_gen&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csprng&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_random_device_generator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/dev/urandom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and simply use it with the existing PyTorch methods:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cpu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urandom_gen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 4: Test with Cuda&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One of the advantages of torchcsprng generators is that they can be used with both CPU and CUDA tensors:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urandom_gen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Another advantage of torchcsprng generators is that they are parallel on CPU unlike the default PyTorch CPU generator.&lt;/p&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;The easiest way to get started with torchcsprng is by visiting the &lt;a href=&quot;https://github.com/pytorch/csprng&quot;&gt;GitHub page&lt;/a&gt; where you can find installation and build instructions, and more how-to examples.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;The PyTorch Team&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://www.amazon.com/Introduction-Modern-Cryptography-Principles-Protocols/dp/1584885513&quot;&gt;Introduction to Modern Cryptography: Principles and Protocols (Chapman &amp;amp; Hall/CRC Cryptography and Network Security Series)&lt;/a&gt; by Jonathan Katz and Yehuda Lindell&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">One of the key components of modern cryptography is the pseudorandom number generator. Katz and Lindell stated, “The use of badly designed or inappropriate random number generators can often leave a good cryptosystem vulnerable to attack. Particular care must be taken to use a random number generator that is designed for cryptographic use, rather than a ‘general-purpose’ random number generator which may be fine for some applications but not ones that are required to be cryptographically secure.”[1] Additionally, most pseudorandom number generators scale poorly to massively parallel high-performance computation because of their sequential nature. Others don’t satisfy cryptographically secure properties.</summary></entry><entry><title type="html">PyTorch 1.6 now includes Stochastic Weight Averaging</title><link href="https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/" rel="alternate" type="text/html" title="PyTorch 1.6 now includes Stochastic Weight Averaging" /><published>2020-08-18T00:00:00-07:00</published><updated>2020-08-18T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/">&lt;p&gt;Do you use stochastic gradient descent (SGD) or Adam? Regardless of the procedure you use to train your neural network, you can likely achieve significantly better generalization at virtually no additional cost with a simple new technique now natively supported in PyTorch 1.6, Stochastic Weight Averaging (SWA) [1]. Even if you have already trained your model, it’s easy to realize the benefits of SWA by running SWA for a small number of epochs starting with a pre-trained model. &lt;a href=&quot;https://twitter.com/MilesCranmer/status/1282140440892932096&quot;&gt;Again&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/leopd/status/1285969855062192129&quot;&gt;again&lt;/a&gt;, researchers are  discovering that SWA improves the performance of well-tuned models in a wide array of practical applications with little cost or effort!&lt;/p&gt;

&lt;p&gt;SWA has a wide range of applications and features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SWA significantly improves performance compared to standard training techniques in computer vision (e.g., VGG, ResNets, Wide ResNets and DenseNets on ImageNet and CIFAR benchmarks [1, 2]).&lt;/li&gt;
  &lt;li&gt;SWA provides state-of-the-art performance on key benchmarks in semi-supervised learning and domain adaptation [2].&lt;/li&gt;
  &lt;li&gt;SWA was shown to improve performance in language modeling (e.g., AWD-LSTM on WikiText-2 [4]) and policy-gradient methods in deep reinforcement learning [3].&lt;/li&gt;
  &lt;li&gt;SWAG, an extension of SWA, can approximate Bayesian model averaging in Bayesian deep learning and achieves state-of-the-art uncertainty calibration results in various settings. Moreover, its recent generalization MultiSWAG provides significant additional performance gains and mitigates double-descent [4, 10]. Another approach, Subspace Inference, approximates the Bayesian posterior in a small subspace of the parameter space around the SWA solution [5].&lt;/li&gt;
  &lt;li&gt;SWA for low precision training, SWALP, can match the performance of full-precision SGD training, even with all numbers quantized down to 8 bits, including gradient accumulators [6].&lt;/li&gt;
  &lt;li&gt;SWA in parallel, SWAP, was shown to greatly speed up the training of neural networks by using large batch sizes and, in particular, set a record by training a neural network to 94% accuracy on CIFAR-10 in 27 seconds [11].&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;. &lt;em&gt;Illustrations of SWA and SGD with a Preactivation ResNet-164 on CIFAR-100 [1]. &lt;strong&gt;Left&lt;/strong&gt;: test error surface for three FGE samples and the corresponding SWA solution (averaging in weight space). &lt;strong&gt;Middle&lt;/strong&gt; and &lt;strong&gt;Right&lt;/strong&gt;: test error and train loss surfaces showing the weights proposed by SGD (at convergence) and SWA, starting from the same initialization of SGD after 125 training epochs. Please see [1] for details on how these figures were constructed&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In short, SWA performs an equal average of the weights traversed by SGD (or any stochastic optimizer) with a modified learning rate schedule (see the left panel of Figure 1.). SWA solutions end up in the center of a wide flat region of loss, while SGD tends to converge to the boundary of the low-loss region, making it susceptible to the shift between train and test error surfaces (see the middle and right panels of Figure 1). We emphasize that SWA &lt;strong&gt;can be used with any optimizer, such as Adam, and is not specific to SGD&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Previously, SWA was in PyTorch contrib. In PyTorch 1.6, we provide a new convenient implementation of SWA in &lt;a href=&quot;https://pytorch.org/docs/stable/optim.html#stochastic-weight-averaging&quot;&gt;torch.optim.swa_utils&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;is-this-just-averaged-sgd&quot;&gt;Is this just Averaged SGD?&lt;/h2&gt;

&lt;p&gt;At a high level, averaging SGD iterates dates back several decades in convex optimization [7, 8], where it is sometimes referred to as Polyak-Ruppert averaging, or averaged SGD. &lt;strong&gt;But the details matter&lt;/strong&gt;. Averaged SGD is often used in conjunction with a decaying learning rate, and an exponential moving average (EMA), typically for convex optimization. In convex optimization, the focus has been on improved rates of convergence. In deep learning, this form of averaged SGD smooths the trajectory of SGD iterates but does not perform very differently.&lt;/p&gt;

&lt;p&gt;By contrast, SWA uses an &lt;strong&gt;equal average&lt;/strong&gt; of SGD iterates with a modified &lt;strong&gt;cyclical or high constant learning rate&lt;/strong&gt; and exploits the flatness of training objectives [8] specific to &lt;strong&gt;deep learning&lt;/strong&gt; for &lt;strong&gt;improved generalization&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;how-does-stochastic-weight-averaging-work&quot;&gt;How does Stochastic Weight Averaging Work?&lt;/h2&gt;

&lt;p&gt;There are two important ingredients that make SWA work. First, SWA uses a &lt;strong&gt;modified learning rate&lt;/strong&gt; schedule so that SGD (or other optimizers such as Adam) continues to bounce around the optimum and explore diverse models instead of simply converging to a single solution. For example, we can use the standard decaying learning rate strategy for the first 75% of training time and then set the learning rate to a reasonably high constant value for the remaining 25% of the time (see Figure 2 below). The second ingredient is to take an average of the weights &lt;strong&gt;(typically an equal average)&lt;/strong&gt; of the networks traversed by SGD. For example, we can maintain a running average of the weights obtained at the end of every epoch within the last 25% of training time (see Figure 2). After training is complete, we then set the weights of the network to the computed SWA averages.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/nswapytorch2.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;. &lt;em&gt;Illustration of the learning rate schedule adopted by SWA. Standard decaying schedule is used for the first 75% of the training and then a high constant value is used for the remaining 25%. The SWA averages are formed during the last 25% of training&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;One important detail is the batch normalization. Batch normalization layers compute running statistics of activations during training. Note that the SWA averages of the weights are never used to make predictions during training. So the batch normalization layers do not have the activation statistics computed at the end of training. We can compute these statistics by doing a single forward pass on the train data with the SWA model.&lt;/p&gt;

&lt;p&gt;While we focus on SGD for simplicity in the description above, SWA can be combined with any optimizer. You can also use cyclical learning rates instead of a high constant value (see e.g., [2]).&lt;/p&gt;

&lt;h2 id=&quot;how-to-use-swa-in-pytorch&quot;&gt;How to use SWA in PyTorch?&lt;/h2&gt;

&lt;p&gt;In &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.optim.swa_utils&lt;/code&gt; we implement all the SWA ingredients to make it convenient to use SWA with any model. In particular, we implement &lt;code class=&quot;highlighter-rouge&quot;&gt;AveragedModel&lt;/code&gt; class for SWA models, &lt;code class=&quot;highlighter-rouge&quot;&gt;SWALR&lt;/code&gt; learning rate scheduler, and &lt;code class=&quot;highlighter-rouge&quot;&gt;update_bn&lt;/code&gt; utility function to update SWA batch normalization statistics at the end of training.&lt;/p&gt;

&lt;p&gt;In the example below, &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_model&lt;/code&gt; is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs, and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.optim.swa_utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AveragedModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SWALR&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.optim.lr_scheduler&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CosineAnnealingLR&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;swa_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AveragedModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CosineAnnealingLR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T_max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;swa_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;swa_scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SWALR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;swa_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;swa_scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Update bn statistics for the swa_model at the end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swa_utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_bn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Use swa_model to make predictions on test data &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we explain each component of &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.optim.swa_utils&lt;/code&gt; in detail.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AveragedModel&lt;/code&gt; class serves to compute the weights of the SWA model. You can create an averaged model by running  &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_model = AveragedModel(model)&lt;/code&gt;. You can then update the parameters of the averaged model by &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_model.update_parameters(model)&lt;/code&gt;. By default, &lt;code class=&quot;highlighter-rouge&quot;&gt;AveragedModel&lt;/code&gt; computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the &lt;code class=&quot;highlighter-rouge&quot;&gt;avg_fn&lt;/code&gt; parameter. In the following example, &lt;code class=&quot;highlighter-rouge&quot;&gt;ema_model&lt;/code&gt; computes an exponential moving average.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ema_avg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averaged_model_parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_averaged&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\
&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averaged_model_parameter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_parameter&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ema_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swa_utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AveragedModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ema_avg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In practice, we find an equal average with the modified learning rate schedule in Figure 2 provides the best performance.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SWALR&lt;/code&gt; is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to &lt;code class=&quot;highlighter-rouge&quot;&gt;0.05&lt;/code&gt; in &lt;code class=&quot;highlighter-rouge&quot;&gt;5&lt;/code&gt; epochs within each parameter group.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;swa_scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swa_utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SWALR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;anneal_strategy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;linear&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;anneal_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We also implement cosine annealing to a fixed value (&lt;code class=&quot;highlighter-rouge&quot;&gt;anneal_strategy=&quot;cos&quot;&lt;/code&gt;). In practice, we typically switch to &lt;code class=&quot;highlighter-rouge&quot;&gt;SWALR&lt;/code&gt; at epoch &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_start&lt;/code&gt;  (e.g. after 75% of the training epochs), and simultaneously start to compute the running averages of the weights:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr_scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CosineAnnealingLR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T_max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;swa_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;75&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;# &amp;lt;train epoch&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;swa_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;swa_scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, &lt;code class=&quot;highlighter-rouge&quot;&gt;update_bn&lt;/code&gt; is a utility function that computes the batchnorm statistics for the SWA model on a given dataloader &lt;code class=&quot;highlighter-rouge&quot;&gt;loader&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.optim.swa_utils.update_bn(loader, swa_model) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;update_bn&lt;/code&gt; applies the &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_model&lt;/code&gt; to every element in the dataloader and computes the activation statistics for each batch normalization layer in the model.&lt;/p&gt;

&lt;p&gt;Once you computed the SWA averages and updated the batch normalization layers, you can apply &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_model&lt;/code&gt; to make predictions on test data.&lt;/p&gt;

&lt;h2 id=&quot;why-does-it-work&quot;&gt;Why does it work?&lt;/h2&gt;

&lt;p&gt;There are large flat regions of the loss surface [9]. In Figure 3 below, we show a visualization of the loss surface in a subspace of the parameter space containing a path connecting two independently trained SGD solutions, such that the loss is similarly low at every point along the path. SGD converges near the boundary of these regions because there isn’t much gradient signal to move inside, as the points in the region all have similarly low values of loss. By increasing the learning rate, SWA spins around this flat region, and then by averaging the iterates, moves towards the center of the flat region.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch3.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: &lt;em&gt;visualization of mode connectivity for ResNet-20 with no skip connections on CIFAR-10 dataset. The visualization is created in collaboration with Javier Ideami &lt;a href=&quot;https://losslandscape.com/&quot;&gt;(https://losslandscape.com/)&lt;/a&gt;. For more details, see this &lt;a href=&quot;https://izmailovpavel.github.io/curves_blogpost/&quot;&gt;blogpost&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We expect solutions that are centered in the flat region of the loss to generalize better than those near the boundary. Indeed, train and test error surfaces are not perfectly aligned in the weight space. Solutions that are centered in the flat region are not as susceptible to the shifts between train and test error surfaces as those near the boundary. In Figure 4 below, we show the train loss and test error surfaces along the direction connecting the SWA and SGD solutions. As you can see, while the SWA solution has a higher train loss compared to the SGD solution, it is centered in a region of low loss and has a substantially better test error.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch4.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;. &lt;em&gt;Train loss and test error along the line connecting the SWA solution (circle) and SGD solution (square). The SWA solution is centered in a wide region of low train loss, while the SGD solution lies near the boundary. Because of the shift between train loss and test error surfaces, the SWA solution leads to much better generalization&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-results-achieved-with-swa&quot;&gt;What are the results achieved with SWA?&lt;/h2&gt;

&lt;p&gt;We release a GitHub &lt;a href=&quot;https://github.com/izmailovpavel/torch_swa_examples&quot;&gt;repo&lt;/a&gt; with examples using the PyTorch implementation of SWA for training DNNs. For example, these examples can be used to achieve the following results on CIFAR-100:&lt;/p&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;VGG-16&lt;/th&gt;
      &lt;th&gt;ResNet-164&lt;/th&gt;
      &lt;th&gt;WideResNet-28x10&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;SGD&lt;/td&gt;
      &lt;td&gt;72.8 ± 0.3&lt;/td&gt;
      &lt;td&gt;78.4 ± 0.3&lt;/td&gt;
      &lt;td&gt;81.0 ± 0.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SWA&lt;/td&gt;
      &lt;td&gt;74.4 ± 0.3&lt;/td&gt;
      &lt;td&gt;79.8 ± 0.4&lt;/td&gt;
      &lt;td&gt;82.5 ± 0.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;semi-supervised-learning&quot;&gt;Semi-Supervised Learning&lt;/h2&gt;

&lt;p&gt;In a follow-up &lt;a href=&quot;https://arxiv.org/abs/1806.05594&quot;&gt;paper&lt;/a&gt; SWA was applied to semi-supervised learning, where it improved the best reported results in multiple settings [2]. For example, with SWA you can get  95% accuracy on CIFAR-10 if you only have the training labels for 4k training data points (the previous best reported result on this problem was 93.7%). This paper also explores averaging multiple times within epochs, which can accelerate convergence and find still flatter solutions in a given time.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch5.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;. Performance of fast-SWA on semi-supervised learning with CIFAR-10. fast-SWA achieves record results in every setting considered.&lt;/p&gt;

&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;In another follow-up &lt;a href=&quot;http://www.gatsby.ucl.ac.uk/~balaji/udl-camera-ready/UDL-24.pdf&quot;&gt;paper&lt;/a&gt; SWA was shown to improve the performance of policy gradient methods A2C and DDPG on several Atari games and MuJoCo environments [3]. This application is also an instance of where SWA is used with Adam. Recall that SWA is not specific to SGD and can benefit essentially any optimizer.&lt;/p&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Environment Name&lt;/th&gt;
      &lt;th&gt;A2C&lt;/th&gt;
      &lt;th&gt;A2C + SWA&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Breakout&lt;/td&gt;
      &lt;td&gt;522 ± 34&lt;/td&gt;
      &lt;td&gt;703 ± 60&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Qbert&lt;/td&gt;
      &lt;td&gt;18777 ± 778&lt;/td&gt;
      &lt;td&gt;21272 ± 655&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SpaceInvaders&lt;/td&gt;
      &lt;td&gt;7727 ± 1121&lt;/td&gt;
      &lt;td&gt;21676 ± 8897&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Seaquest&lt;/td&gt;
      &lt;td&gt;1779 ± 4&lt;/td&gt;
      &lt;td&gt;1795 ± 4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BeamRider&lt;/td&gt;
      &lt;td&gt;9999 ± 402&lt;/td&gt;
      &lt;td&gt;11321 ± 1065&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CrazyClimber&lt;/td&gt;
      &lt;td&gt;147030 ± 10239&lt;/td&gt;
      &lt;td&gt;139752 ± 11618&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;low-precision-training&quot;&gt;Low Precision Training&lt;/h2&gt;

&lt;p&gt;We can filter through quantization noise by combining weights that have been rounded down with weights that have been rounded up. Moreover, by averaging weights to find a flat region of the loss surface, large perturbations of the weights will not affect the quality of the solution (Figures 9 and 10). Recent &lt;a href=&quot;https://arxiv.org/abs/1904.11943&quot;&gt;work&lt;/a&gt; shows that by adapting SWA to the low precision setting, in a method called SWALP, one can match the performance of full-precision SGD even with all training in 8 bits [5]. This is quite a practically important result, given that (1) SGD training in 8 bits performs notably worse than full precision SGD, and (2) low precision training is significantly harder than predictions in low precision after training (the usual setting). For example, a ResNet-164 trained on CIFAR-100 with float (16-bit) SGD achieves 22.2% error, while 8-bit SGD achieves 24.0% error. By contrast, SWALP with 8 bit training achieves 21.8% error.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/nswapytorch6.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 9&lt;/strong&gt;. &lt;em&gt;Quantizing a solution leads to a perturbation of the weights which has a greater effect on the quality of the sharp solution (left) compared to wide solution (right)&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch7.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 10&lt;/strong&gt;. &lt;em&gt;The difference between standard low precision training and SWALP&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Another &lt;a href=&quot;https://arxiv.org/abs/2002.00343&quot;&gt;work&lt;/a&gt;, SQWA, presents an approach for quantization and fine-tuning of neural networks in low precision [12]. In particular, SQWA achieved state-of-the-art results for DNNs quantized to 2 bits on CIFAR-100 and ImageNet.&lt;/p&gt;

&lt;h2 id=&quot;calibration-and-uncertainty-estimates&quot;&gt;Calibration and Uncertainty Estimates&lt;/h2&gt;

&lt;p&gt;By finding a centred solution in the loss, SWA can also improve calibration and uncertainty representation. Indeed, SWA can be viewed as an approximation to an ensemble, resembling a Bayesian model average, but with a single model [1].&lt;/p&gt;

&lt;p&gt;SWA can be viewed as taking the first moment of SGD iterates with a modified learning rate schedule. We can directly generalize SWA by also taking the second moment of iterates to form a Gaussian approximate posterior over the weights, further characterizing the loss geometry with SGD iterates.  This approach,&lt;a href=&quot;https://arxiv.org/abs/1902.02476&quot;&gt;SWA-Gaussian (SWAG)&lt;/a&gt; is a simple, scalable and convenient approach to uncertainty estimation and calibration in Bayesian deep learning [4]. The SWAG distribution approximates the shape of the true posterior: Figure 6 below shows the SWAG distribution and the posterior log-density for ResNet-20 on CIFAR-10.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/nswapytorch8.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;. &lt;em&gt;SWAG posterior approximation and the loss surface for a ResNet-20 without skip-connections trained on CIFAR-10 in the subspace formed by the two largest eigenvalues of the SWAG covariance matrix. The shape of SWAG distribution is aligned with the posterior: the peaks of the two distributions coincide, and both distributions are wider in one direction than in the orthogonal direction. Visualization created in collaboration with&lt;/em&gt; &lt;a href=&quot;https://losslandscape.com/&quot;&gt;Javier Ideami&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Empirically, SWAG performs on par or better than popular alternatives including MC dropout, KFAC Laplace, and temperature scaling on uncertainty quantification, out-of-distribution detection, calibration and transfer learning in computer vision tasks. Code for SWAG is available &lt;a href=&quot;https://github.com/wjmaddox/swa_gaussian&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch9.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 7&lt;/strong&gt;. &lt;em&gt;MultiSWAG generalizes SWAG and deep ensembles, to perform Bayesian model averaging over multiple basins of attraction, leading to significantly improved performance. By contrast, as shown here, deep ensembles select different modes, while standard variational inference (VI) marginalizes (model averages) within a single basin&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;MultiSWAG [9] uses multiple independent SWAG models to form a mixture of Gaussians as an approximate posterior distribution. Different basins of attraction contain highly complementary explanations of the data. Accordingly, marginalizing over these multiple basins provides a significant boost in accuracy and uncertainty representation. MultiSWAG can be viewed as a generalization of deep ensembles, but with performance improvements.&lt;/p&gt;

&lt;p&gt;Indeed, we see in Figure 8 that MultiSWAG entirely mitigates double descent – more flexible models have monotonically improving performance – and provides significantly improved generalization over SGD. For example, when the ResNet-18 has layers of width 20, Multi-SWAG achieves under 30% error whereas SGD achieves over 45%, more than a 15% gap!&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swapytorch10.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 8&lt;/strong&gt;. &lt;em&gt;SGD, SWAG, and Multi-SWAG on CIFAR-100 for a ResNet-18 with varying widths. We see Multi-SWAG in particular mitigates double descent and provides significant accuracy improvements over SGD&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Reference [10] also considers Multi-SWA, which uses multiple independently trained SWA solutions in an ensemble, providing performance improvements over deep ensembles without any additional computational cost. Code for MultiSWA and MultiSWAG is available &lt;a href=&quot;https://github.com/izmailovpavel/understandingbdl&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another &lt;a href=&quot;https://arxiv.org/abs/1907.07504&quot;&gt;method&lt;/a&gt;, Subspace Inference, constructs a low-dimensional subspace around the SWA solution and marginalizes the weights in this subspace to approximate the Bayesian model average [5]. Subspace Inference uses the statistics from the SGD iterates to construct both the SWA solution and the subspace. The method achieves strong performance in terms of prediction accuracy and uncertainty calibration both in classification and regression problems. Code is available &lt;a href=&quot;https://github.com/wjmaddox/drbayes&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;try-it-out&quot;&gt;Try it Out!&lt;/h2&gt;

&lt;p&gt;One of the greatest open questions in deep learning is why SGD manages to find good solutions, given that the training objectives are highly multimodal, and there are many settings of parameters that achieve no training loss but poor generalization. By understanding geometric features such as flatness, which relate to generalization, we can begin to resolve these questions and build optimizers that provide even better generalization, and many other useful features, such as uncertainty representation. We have presented SWA, a simple drop-in replacement for standard optimizers such as SGD and Adam, which can in principle, benefit anyone training a deep neural network. SWA has been demonstrated to have a strong performance in several areas, including computer vision, semi-supervised learning, reinforcement learning, uncertainty representation, calibration, Bayesian model averaging, and low precision training.&lt;/p&gt;

&lt;p&gt;We encourage you to try out SWA! SWA is now as easy as any standard training in PyTorch. And even if you have already trained your model, you can use SWA to significantly improve performance by running it for a small number of epochs from a pre-trained model.&lt;/p&gt;

&lt;p&gt;[1] Averaging Weights Leads to Wider Optima and Better Generalization; Pavel Izmailov, Dmitry Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson; Uncertainty in Artificial Intelligence (UAI), 2018.&lt;/p&gt;

&lt;p&gt;[2] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average; Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson; 
International Conference on Learning Representations (ICLR), 2019.&lt;/p&gt;

&lt;p&gt;[3] Improving Stability in Deep Reinforcement Learning with Weight Averaging; Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, 
Timur Garipov, Pavel Shvechikov, Dmitry Vetrov, Andrew Gordon Wilson; UAI 2018 Workshop: Uncertainty in Deep Learning, 2018.&lt;/p&gt;

&lt;p&gt;[4]  A Simple Baseline for Bayesian Uncertainty in Deep Learning
Wesley Maddox, Timur Garipov, Pavel Izmailov, Andrew Gordon Wilson; Neural Information Processing Systems (NeurIPS), 2019.&lt;/p&gt;

&lt;p&gt;[5] Subspace Inference for Bayesian Deep Learning
Pavel Izmailov, Wesley Maddox, Polina Kirichenko, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson
Uncertainty in Artificial Intelligence (UAI), 2019.&lt;/p&gt;

&lt;p&gt;[6] SWALP : Stochastic Weight Averaging in Low Precision Training
Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, 
Andrew Gordon Wilson, Christopher De Sa; International Conference on Machine Learning  (ICML), 2019.&lt;/p&gt;

&lt;p&gt;[7] David Ruppert. Efficient estimations from a slowly convergent Robbins-Monro process; Technical report, Cornell University Operations Research and Industrial Engineering, 1988.&lt;/p&gt;

&lt;p&gt;[8] Acceleration of stochastic approximation by averaging. Boris T Polyak and Anatoli B Juditsky; SIAM Journal on Control and Optimization, 30(4):838–855, 1992.&lt;/p&gt;

&lt;p&gt;[9] Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, 
Andrew Gordon Wilson. Neural Information Processing Systems (NeurIPS), 2018.&lt;/p&gt;

&lt;p&gt;[10] Bayesian Deep Learning and a Probabilistic Perspective of Generalization
Andrew Gordon Wilson, Pavel Izmailov. ArXiv preprint, 2020.&lt;/p&gt;

&lt;p&gt;[11] Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well
Gupta, Vipul, Santiago Akle Serrano, and Dennis DeCoste; International Conference on Learning Representations (ICLR). 2019.&lt;/p&gt;

&lt;p&gt;[12] SQWA: Stochastic Quantized Weight Averaging for Improving the Generalization Capability of Low-Precision Deep Neural Networks
Shin, Sungho, Yoonho Boo, and Wonyong Sung; arXiv preprint 2020.&lt;/p&gt;</content><author><name>Pavel Izmailov, Andrew Gordon Wilson and Vincent Queneneville-Belair</name></author><summary type="html">Do you use stochastic gradient descent (SGD) or Adam? Regardless of the procedure you use to train your neural network, you can likely achieve significantly better generalization at virtually no additional cost with a simple new technique now natively supported in PyTorch 1.6, Stochastic Weight Averaging (SWA) [1]. Even if you have already trained your model, it’s easy to realize the benefits of SWA by running SWA for a small number of epochs starting with a pre-trained model. Again and again, researchers are discovering that SWA improves the performance of well-tuned models in a wide array of practical applications with little cost or effort!</summary></entry><entry><title type="html">Efficient PyTorch I/O library for Large Datasets, Many Files, Many GPUs</title><link href="https://pytorch.org/blog/efficient-pytorch-io-library-for-large-datasets-many-files-many-gpus/" rel="alternate" type="text/html" title="Efficient PyTorch I/O library for Large Datasets, Many Files, Many GPUs" /><published>2020-08-11T00:00:00-07:00</published><updated>2020-08-11T00:00:00-07:00</updated><id>https://pytorch.org/blog/efficient-pytorch-io-library-for-large-datasets-many-files-many-gpus</id><content type="html" xml:base="https://pytorch.org/blog/efficient-pytorch-io-library-for-large-datasets-many-files-many-gpus/">&lt;p&gt;Data sets are growing bigger every day and GPUs are getting faster. This means there are more data sets for deep learning researchers and engineers to train and validate their models.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Many datasets for research in still image recognition are becoming available with 10 million or more images, including OpenImages and Places.&lt;/li&gt;
  &lt;li&gt;million YouTube videos &lt;a href=&quot;https://research.google.com/youtube8m/&quot;&gt;(YouTube 8M)&lt;/a&gt; consume about 300 TB in 720p, used for research in object recognition, video analytics, and action recognition.&lt;/li&gt;
  &lt;li&gt;The Tobacco Corpus consists of about 20 million scanned HD pages, useful for OCR and text analytics research.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although the most commonly encountered big data sets right now involve images and videos, big datasets occur in many other domains and involve many other kinds of data types: web pages, financial transactions, network traces, brain scans, etc.&lt;/p&gt;

&lt;p&gt;However, working with the large amount of data sets presents a number of challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dataset Size:&lt;/strong&gt; datasets often exceed the capacity of node-local disk storage, requiring distributed storage systems and efficient network access.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of Files:&lt;/strong&gt; datasets often consist of billions of files with uniformly random access patterns, something that often overwhelms both local and network file systems.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Rates:&lt;/strong&gt; training jobs on large datasets often use many GPUs, requiring aggregate I/O bandwidths to the dataset of many GBytes/s; these can only be satisfied by massively parallel I/O systems.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Shuffling and Augmentation:&lt;/strong&gt; training data needs to be shuffled and augmented prior to training.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; users often want to develop and test on small datasets and then rapidly scale up to large datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Traditional local and network file systems, and even object storage servers, are not designed for these kinds of applications. &lt;a href=&quot;https://github.com/tmbdev/webdataset&quot;&gt;The WebDataset I/O library&lt;/a&gt; for PyTorch, together with the optional &lt;a href=&quot;https://github.com/NVIDIA/aistore&quot;&gt;AIStore server&lt;/a&gt; and &lt;a href=&quot;https://github.com/NVlabs/tensorcom&quot;&gt;Tensorcom&lt;/a&gt; RDMA libraries, provide an efficient, simple, and standards-based solution to all these problems. The library is simple enough for day-to-day use, is based on mature open source standards, and is easy to migrate to from existing file-based datasets.&lt;/p&gt;

&lt;p&gt;Using WebDataset is simple and requires little effort, and it will let you scale up the same code from running local experiments to using hundreds of GPUs on clusters or in the cloud with linearly scalable performance. Even on small problems and on your desktop, it can speed up I/O tenfold and simplifies data management and processing of large datasets. The rest of this blog post tells you how to get started with WebDataset and how it works.&lt;/p&gt;

&lt;h2 id=&quot;the-webdataset-library&quot;&gt;The WebDataset Library&lt;/h2&gt;

&lt;p&gt;The WebDataset library provides a simple solution to the challenges listed above. Currently, it is available as a separate library &lt;a href=&quot;https://github.com/tmbdev/webdataset&quot;&gt;(github.com/tmbdev/webdataset)&lt;/a&gt;, but it is on track for being incorporated into PyTorch (see &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/38419&quot;&gt;RFC 38419&lt;/a&gt;).  The WebDataset implementation is small (about 1500 LOC) and has no external dependencies.&lt;/p&gt;

&lt;p&gt;Instead of inventing a new format, WebDataset represents large datasets as collections of POSIX tar archive files consisting of the original data files. The WebDataset library can use such tar archives directly for training, without the need for unpacking or local storage.&lt;/p&gt;

&lt;p&gt;WebDataset scales perfectly from small, local datasets to petascale datasets and training on hundreds of GPUs and allows data to be stored on local disk, on web servers, or dedicated file servers. For container-based training, WebDataset eliminates the need for volume plugins or node-local storage. As an additional benefit, datasets need not be unpacked prior to training, simplifying the distribution and use of research data.&lt;/p&gt;

&lt;p&gt;WebDataset implements PyTorch’s &lt;a href=&quot;https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset&quot;&gt;IterableDataset&lt;/a&gt; interface and can be used like existing DataLoader-based code. Since data is stored as files inside an archive, existing loading and data augmentation code usually requires minimal modification.&lt;/p&gt;

&lt;p&gt;The WebDataset library is a complete solution for working with large datasets and distributed training in PyTorch (and also works with TensorFlow, Keras, and DALI via their Python APIs). Since POSIX tar archives are a standard, widely supported format, it is easy to write other tools for manipulating datasets in this format. E.g., the &lt;a href=&quot;https://github.com/tmbdev/tarp&quot;&gt;tarp&lt;/a&gt; command is written in Go and can shuffle and process training datasets.&lt;/p&gt;

&lt;h2 id=&quot;benefits&quot;&gt;Benefits&lt;/h2&gt;

&lt;p&gt;The use of sharded, sequentially readable formats is essential for very large datasets. In addition, it has benefits in many other environments. WebDataset provides a solution that scales well from small problems on a desktop machine to very large deep learning problems in clusters or in the cloud. The following table summarizes some of the benefits in different environments.&lt;/p&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Environment&lt;/th&gt;
      &lt;th&gt;Benefits of WebDataset&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Local Cluster with AIStore&lt;/td&gt;
      &lt;td&gt;AIStore can be deployed easily as K8s containers and offers linear scalability and near 100% utilization of network and I/O bandwidth. Suitable for petascale deep learning.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cloud Computing&lt;/td&gt;
      &lt;td&gt;WebDataset deep learning jobs can be trained directly against datasets stored in cloud buckets; no volume plugins required. Local and cloud jobs work identically. Suitable for petascale learning.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Local Cluster with existing distributed FS or object store&lt;/td&gt;
      &lt;td&gt;WebDataset’s large sequential reads improve performance with existing distributed stores and eliminate the need for dedicated volume plugins.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Educational Environments&lt;/td&gt;
      &lt;td&gt;WebDatasets can be stored on existing web servers and web caches, and can be accessed directly by students by URL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Training on Workstations from Local Drives&lt;/td&gt;
      &lt;td&gt;Jobs can start training as the data still downloads. Data doesn’t need to be unpacked for training. Ten-fold improvements in I/O performance on hard drives over random access file-based datasets.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;All Environments&lt;/td&gt;
      &lt;td&gt;Datasets are represented in an archival format and contain metadata such as file types. Data is compressed in native formats (JPEG, MP4, etc.). Data management, ETL-style jobs, and data transformations and I/O are simplified and easily parallelized.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We will be adding more examples giving benchmarks and showing how to use WebDataset in these environments over the coming months.&lt;/p&gt;

&lt;h2 id=&quot;high-performance&quot;&gt;High-Performance&lt;/h2&gt;
&lt;p&gt;For high-performance computation on local clusters, the companion open-source &lt;a href=&quot;https://github.com/NVIDIA/AIStore&quot;&gt;AIStore&lt;/a&gt; server provides full disk to GPU I/O bandwidth, subject only to hardware constraints. &lt;a href=&quot;https://arxiv.org/abs/2001.01858&quot;&gt;This Bigdata 2019 Paper&lt;/a&gt; contains detailed benchmarks and performance measurements. In addition to benchmarks, research projects at NVIDIA and Microsoft have used WebDataset for petascale datasets and billions of training samples.&lt;/p&gt;

&lt;p&gt;Below is a benchmark of AIStore with WebDataset clients using 12 server nodes with 10 rotational drives each.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/pytorchwebdataset1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The left axis shows the aggregate bandwidth from the cluster, while the right scale shows the measured per drive I/O bandwidth. WebDataset and AIStore scale linearly to about 300 clients, at which point they are increasingly limited by the maximum I/O bandwidth available from the rotational drives (about 150 MBytes/s per drive). For comparison, HDFS is shown. HDFS uses a similar approach to AIStore/WebDataset and also exhibits linear scaling up to about 192 clients; at that point, it hits a performance limit of about 120 MBytes/s per drive, and it failed when using more than 1024 clients. Unlike HDFS, the WebDataset-based code just uses standard URLs and HTTP to access data and works identically with local files, with files stored on web servers, and with AIStore. For comparison, NFS in similar experiments delivers about 10-20 MBytes/s per drive.&lt;/p&gt;

&lt;h2 id=&quot;storing-datasets-in-tar-archives&quot;&gt;Storing Datasets in Tar Archives&lt;/h2&gt;

&lt;p&gt;The format used for WebDataset is standard POSIX tar archives, the same archives used for backup and data distribution. In order to use the format to store training samples for deep learning, we adopt some simple naming conventions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;datasets are POSIX tar archives&lt;/li&gt;
  &lt;li&gt;each training sample consists of adjacent files with the same basename&lt;/li&gt;
  &lt;li&gt;shards are numbered consecutively&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, ImageNet is stored in 1282 separate 100 Mbyte shards with names &lt;code class=&quot;highlighter-rouge&quot;&gt;pythonimagenet-train-000000.tar to imagenet-train-001281.tar,&lt;/code&gt; the contents of the first shard are:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n03991062_24866&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cls&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;108611&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n03991062_24866&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jpg&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n07749582_9506&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cls&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;129044&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n07749582_9506&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jpg&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n03425413_23604&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cls&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;106255&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n03425413_23604&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jpg&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigdata&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2020&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n02795169_27274&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cls&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;WebDataset datasets can be used directly from local disk, from web servers (hence the name), from cloud storage and object stores, just by changing a URL. WebDataset datasets can be used for training without unpacking, and training can even be carried out on streaming data, with no local storage.&lt;/p&gt;

&lt;p&gt;Shuffling during training is important for many deep learning applications, and WebDataset performs shuffling both at the shard level and at the sample level. Splitting of data across multiple workers is performed at the shard level using a user-provided &lt;code class=&quot;highlighter-rouge&quot;&gt;shard_selection&lt;/code&gt; function that defaults to a function that splits based on &lt;code class=&quot;highlighter-rouge&quot;&gt;get_worker_info.&lt;/code&gt; (WebDataset can be combined with the &lt;a href=&quot;https://github.com/NVLabs/tensorcom&quot;&gt;tensorcom&lt;/a&gt; library to offload decompression/data augmentation and provide RDMA and direct-to-GPU loading; see below.)&lt;/p&gt;

&lt;h2 id=&quot;code-sample&quot;&gt;Code Sample&lt;/h2&gt;
&lt;p&gt;Here are some code snippets illustrating the use of WebDataset in a typical PyTorch deep learning application (you can find a full example at &lt;a href=&quot;http://github.com/tmbdev/pytorch-imagenet-wds&quot;&gt;http://github.com/tmbdev/pytorch-imagenet-wds&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;webdataset&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wds&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sharedurl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/imagenet/imagenet-train-{000000..001281}.tar&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.485&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.456&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.406&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.229&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.225&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;preproc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Compose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomResizedCrop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomHorizontalFlip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;wds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sharedurl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pil&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;jpg;png&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preproc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This code is nearly identical to the file-based I/O pipeline found in the PyTorch Imagenet example: it creates a preprocessing/augmentation pipeline, instantiates a dataset using that pipeline and a data source location, and then constructs a DataLoader instance from the dataset.&lt;/p&gt;

&lt;p&gt;WebDataset uses a fluent API for a configuration that internally builds up a processing pipeline. Without any added processing stages, In this example, WebDataset is used with the PyTorch DataLoader class, which replicates DataSet instances across multiple threads and performs both parallel I/O and parallel data augmentation.&lt;/p&gt;

&lt;p&gt;WebDataset instances themselves just iterate through each training sample as a dictionary:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# load from a web server using a separate client process&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sharedurl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;pipe:curl -s http://server/imagenet/imagenet-train-{000000..001281}.tar&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sharedurl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# sample[&quot;jpg&quot;] contains the raw image data&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# sample[&quot;cls&quot;] contains the class&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For a general introduction to how we handle large scale training with WebDataset, see these &lt;a href=&quot;https://www.youtube.com/playlist?list=PL0dsKxFNMcX4XcB0w1Wm-pvSfQu-eWM26&quot;&gt;YouTube videos&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;related-software&quot;&gt;Related Software&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/NVIDIA/AIStore&quot;&gt;AIStore&lt;/a&gt; is an open-source object store capable of full-bandwidth disk-to-GPU data delivery (meaning that if you have 1000 rotational drives with 200 MB/s read speed, AIStore actually delivers an aggregate bandwidth of 200 GB/s to the GPUs). AIStore is fully compatible with WebDataset as a client, and in addition understands the WebDataset format, permitting it to perform shuffling, sorting, ETL, and some map-reduce operations directly in the storage system. AIStore can be thought of as a remix of a distributed object store, a network file system, a distributed database, and a GPU-accelerated map-reduce implementation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/tmbdev/tarp&quot;&gt;tarp&lt;/a&gt; is a small command-line program for splitting, merging, shuffling, and processing tar archives and WebDataset datasets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/NVLabs/tensorcom&quot;&gt;tensorcom&lt;/a&gt; is a library supporting distributed data augmentation and RDMA to GPU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/tmbdev/pytorch-imagenet-wds&quot;&gt;pytorch-imagenet-wds&lt;/a&gt; contains an example of how to use WebDataset with ImageNet, based on the PyTorch ImageNet example.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.01858&quot;&gt;Bigdata 2019 Paper with Benchmarks&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check out &lt;a href=&quot;https://github.com/tmbdev/webdataset&quot;&gt;the library&lt;/a&gt; and provide your feedback for &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/38419&quot;&gt;RFC 38419&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alex Aizman, Gavin Maltby, Thomas Breuel</name></author><summary type="html">Data sets are growing bigger every day and GPUs are getting faster. This means there are more data sets for deep learning researchers and engineers to train and validate their models.</summary></entry><entry><title type="html">PyTorch 1.6 released w/ Native AMP Support, Microsoft joins as maintainers for Windows</title><link href="https://pytorch.org/blog/pytorch-1.6-released/" rel="alternate" type="text/html" title="PyTorch 1.6 released w/ Native AMP Support, Microsoft joins as maintainers for Windows" /><published>2020-07-28T00:00:00-07:00</published><updated>2020-07-28T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.6-released</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.6-released/">&lt;p&gt;Today, we’re announcing the availability of PyTorch 1.6, along with updated domain libraries. We are also excited to announce the team at &lt;a href=&quot;https://pytorch.org/blog/microsoft-becomes-maintainer-of-the-windows-version-of-pytorch&quot;&gt;Microsoft is now maintaining Windows builds and binaries&lt;/a&gt; and will also be supporting the community on GitHub as well as the PyTorch Windows discussion forums.&lt;/p&gt;

&lt;p&gt;The PyTorch 1.6 release includes a number of new APIs, tools for performance improvement and profiling, as well as major updates to both distributed data parallel (DDP) and remote procedure call (RPC) based distributed training. 
A few of the highlights include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Automatic mixed precision (AMP) training is now natively supported and a stable feature (See &lt;a href=&quot;https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/&quot;&gt;here&lt;/a&gt; for more details) - thanks for NVIDIA’s contributions;&lt;/li&gt;
  &lt;li&gt;Native TensorPipe support now added for tensor-aware, point-to-point communication primitives built specifically for machine learning;&lt;/li&gt;
  &lt;li&gt;Added support for complex tensors to the frontend API surface;&lt;/li&gt;
  &lt;li&gt;New profiling tools providing tensor-level memory consumption information;&lt;/li&gt;
  &lt;li&gt;Numerous improvements and new features for both distributed data parallel (DDP) training and the remote procedural call (RPC) packages.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Additionally, from this release onward, features will be classified as Stable, Beta and Prototype. Prototype features are not included as part of the binary distribution and are instead available through either building from source, using nightlies or via compiler flag. You can learn more about what this change means in the post &lt;a href=&quot;https://pytorch.org/blog/pytorch-feature-classification-changes/&quot;&gt;here&lt;/a&gt;. You can also find the full release notes &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;performance--profiling&quot;&gt;Performance &amp;amp; Profiling&lt;/h1&gt;

&lt;h2 id=&quot;stable-automatic-mixed-precision-amp-training&quot;&gt;[Stable] Automatic Mixed Precision (AMP) Training&lt;/h2&gt;

&lt;p&gt;AMP allows users to easily enable automatic mixed precision training enabling higher performance and memory savings of up to 50% on Tensor Core GPUs. Using the natively supported &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.amp&lt;/code&gt; API, AMP provides convenience methods for mixed precision, where some operations use the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.float32 (float)&lt;/code&gt; datatype and other operations use &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.float16 (half)&lt;/code&gt;. Some ops, like linear layers and convolutions, are much faster in &lt;code class=&quot;highlighter-rouge&quot;&gt;float16&lt;/code&gt;. Other ops, like reductions, often require the dynamic range of &lt;code class=&quot;highlighter-rouge&quot;&gt;float32&lt;/code&gt;. Mixed precision tries to match each op to its appropriate datatype.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Design doc (&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/25081&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/amp.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Usage examples (&lt;a href=&quot;https://pytorch.org/docs/stable/notes/amp_examples.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-forkjoin-parallelism&quot;&gt;[Beta] Fork/Join Parallelism&lt;/h2&gt;

&lt;p&gt;This release adds support for a language-level construct as well as runtime support for coarse-grained parallelism in TorchScript code. This support is useful for situations such as running models in an ensemble in parallel, or running bidirectional components of recurrent nets in parallel, and allows the ability to unlock the computational power of parallel architectures (e.g. many-core CPUs) for task level parallelism.&lt;/p&gt;

&lt;p&gt;Parallel execution of TorchScript programs is enabled through two primitives: &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.jit.fork&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.jit.wait&lt;/code&gt;. In the below example, we parallelize execution of &lt;code class=&quot;highlighter-rouge&quot;&gt;foo&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;neg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@torch.jit.script&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;futures&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;future&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;futures&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/jit.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-memory-profiler&quot;&gt;[Beta] Memory Profiler&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.autograd.profiler&lt;/code&gt; API now includes a memory profiler that lets you inspect the tensor memory cost of different operators inside your CPU and GPU models.&lt;/p&gt;

&lt;p&gt;Here is an example usage of the API:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.autograd.profiler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resnet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;profile_memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record_shapes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# NOTE: some columns were removed for brevity&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prof&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;self_cpu_memory_usage&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_limit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ---------------------------  ---------------  ---------------  ---------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Name                         CPU Mem          Self CPU Mem     Number of Calls&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ---------------------------  ---------------  ---------------  ---------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# empty                        94.79 Mb         94.79 Mb         123&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# resize_                      11.48 Mb         11.48 Mb         2&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# addmm                        19.53 Kb         19.53 Kb         1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# empty_strided                4 b              4 b              1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# conv2d                       47.37 Mb         0 b              20&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ---------------------------  ---------------  ---------------  ---------------&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;PR (&lt;a href=&quot;https://github.com/pytorch/pytorch/pull/37775&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#profiler&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;distributed-training--rpc&quot;&gt;Distributed Training &amp;amp; RPC&lt;/h1&gt;

&lt;h2 id=&quot;beta-tensorpipe-backend-for-rpc&quot;&gt;[Beta] TensorPipe backend for RPC&lt;/h2&gt;

&lt;p&gt;PyTorch 1.6 introduces a new backend for the RPC module which leverages the TensorPipe library, a tensor-aware point-to-point communication primitive targeted at machine learning, intended to complement the current primitives for distributed training in PyTorch (Gloo, MPI, …) which are collective and blocking. The pairwise and asynchronous nature of TensorPipe lends itself to new networking paradigms that go beyond data parallel: client-server approaches (e.g., parameter server for embeddings, actor-learner separation in Impala-style RL, …) and model and pipeline parallel training (think GPipe), gossip SGD, etc.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# One-line change needed to opt in&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_rpc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BackendType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TENSORPIPE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# No changes to the rest of the RPC API&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc_sync&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Design doc (&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/35251&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/rpc/index.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-ddprpc&quot;&gt;[Beta] DDP+RPC&lt;/h2&gt;

&lt;p&gt;PyTorch Distributed supports two powerful paradigms: DDP for full sync data parallel training of models and the RPC framework which allows for distributed model parallelism. Previously, these two features worked independently and users couldn’t mix and match these to try out hybrid parallelism paradigms.&lt;/p&gt;

&lt;p&gt;Starting in PyTorch 1.6, we’ve enabled DDP and RPC to work together seamlessly so that users can combine these two techniques to achieve both data parallelism and model parallelism. An example is where users would like to place large embedding tables on parameter servers and use the RPC framework for embedding lookups, but store smaller dense parameters on trainers and use DDP to synchronize the dense parameters. Below is a simple code snippet.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;On&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;remote_emb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_emb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ps&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ddp_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DDP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dense_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remote_emb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ddp_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;DDP+RPC Tutorial (&lt;a href=&quot;https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/rpc/index.html&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Usage Examples (&lt;a href=&quot;https://github.com/pytorch/examples/pull/800&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;beta-rpc---asynchronous-user-functions&quot;&gt;[Beta] RPC - Asynchronous User Functions&lt;/h2&gt;

&lt;p&gt;RPC Asynchronous User Functions supports the ability to yield and resume on the server side when executing a user-defined function. Prior to this feature, when a callee processes a request, one RPC thread waits until the user function returns. If the user function contains IO (e.g., nested RPC) or signaling (e.g., waiting for another request to unblock), the corresponding RPC thread would sit idle waiting for these events. As a result, some applications have to use a very large number of threads and send additional RPC requests, which can potentially lead to performance degradation. To make a user function yield on such events, applications need to: 1) Decorate the function with the &lt;code class=&quot;highlighter-rouge&quot;&gt;@rpc.functions.async_execution&lt;/code&gt; decorator; and 2) Let the function return a &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.futures.Future&lt;/code&gt; and install the resume logic as callbacks on the &lt;code class=&quot;highlighter-rouge&quot;&gt;Future&lt;/code&gt; object. See below for an example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nd&quot;&gt;@rpc.functions.async_execution&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;async_add_chained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc_async&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fut&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpc_sync&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;worker1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;async_add_chained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;worker2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# prints tensor([3., 3.])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Tutorial for performant batch RPC using Asynchronous User Functions (&lt;a href=&quot;https://github.com/pytorch/tutorials/blob/release/1.6/intermediate_source/rpc_async_execution.rst&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Documentation (&lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.functions.async_execution&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Usage examples (&lt;a href=&quot;https://github.com/pytorch/examples/tree/master/distributed/rpc/batch&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;frontend-api-updates&quot;&gt;Frontend API Updates&lt;/h1&gt;

&lt;h2 id=&quot;beta-complex-numbers&quot;&gt;[Beta] Complex Numbers&lt;/h2&gt;

&lt;p&gt;The PyTorch 1.6 release brings beta level support for complex tensors including torch.complex64 and torch.complex128 dtypes. A complex number is a number that can be expressed in the form a + bj, where a and b are real numbers, and j is a solution of the equation x^2 = −1. Complex numbers frequently occur in mathematics and engineering, especially in signal processing and the area of complex neural networks is an active area of research. The beta release of complex tensors will support common PyTorch and complex tensor functionality, plus functions needed by Torchaudio, ESPnet and others. While this is an early version of this feature, and we expect it to improve over time, the overall goal is provide a NumPy compatible user experience that leverages PyTorch’s ability to run on accelerators and work with autograd to better support the scientific community.&lt;/p&gt;

&lt;h1 id=&quot;updated-domain-libraries&quot;&gt;Updated Domain Libraries&lt;/h1&gt;

&lt;h2 id=&quot;torchvision-07&quot;&gt;torchvision 0.7&lt;/h2&gt;

&lt;p&gt;torchvision 0.7 introduces two new pretrained semantic segmentation models, &lt;a href=&quot;https://arxiv.org/abs/1411.4038&quot;&gt;FCN ResNet50&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1706.05587&quot;&gt;DeepLabV3 ResNet50&lt;/a&gt;, both trained on COCO and using smaller memory footprints than the ResNet101 backbone. We also introduced support for AMP (Automatic Mixed Precision) autocasting for torchvision models and operators, which automatically selects the floating point precision for different GPU operations to improve performance while maintaining accuracy.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Release notes (&lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;torchaudio-06&quot;&gt;torchaudio 0.6&lt;/h2&gt;

&lt;p&gt;torchaudio now officially supports Windows. This release also introduces a new model module (with wav2letter included), new functionals (contrast, cvm, dcshift, overdrive, vad, phaser, flanger, biquad), datasets (GTZAN, CMU), and a new optional sox backend with support for TorchScript.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Release notes (&lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;additional-updates&quot;&gt;Additional updates&lt;/h1&gt;

&lt;h2 id=&quot;hackathon&quot;&gt;HACKATHON&lt;/h2&gt;

&lt;p&gt;The Global PyTorch Summer Hackathon is back! This year, teams can compete in three categories virtually:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;PyTorch Developer Tools:&lt;/strong&gt; Tools or libraries designed to improve productivity and efficiency of PyTorch for researchers and developers&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Web/Mobile Applications powered by PyTorch:&lt;/strong&gt; Applications with web/mobile interfaces and/or embedded devices powered by PyTorch&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PyTorch Responsible AI Development Tools:&lt;/strong&gt; Tools, libraries, or web/mobile apps for responsible AI development&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is a great opportunity to connect with the community and practice your machine learning skills.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch2020.devpost.com/&quot;&gt;Join the hackathon&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/pytorch&quot;&gt;Watch educational videos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lpcv-challenge&quot;&gt;LPCV Challenge&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://lpcv.ai/2020CVPR/video-track&quot;&gt;2020 CVPR Low-Power Vision Challenge (LPCV) - Online Track for UAV video&lt;/a&gt; submission deadline is coming up shortly. You have until July 31, 2020 to build a system that can discover and recognize characters in video captured by an unmanned aerial vehicle (UAV) accurately using PyTorch and Raspberry Pi 3B+.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;Prototype Features&lt;/h2&gt;

&lt;p&gt;To reiterate, Prototype features in PyTorch are early features that we are looking to gather feedback on, gauge the usefulness of and improve ahead of graduating them to Beta or Stable. The following features are not part of the PyTorch 1.6 release and instead are available in nightlies with separate docs/tutorials to help facilitate early usage and feedback.&lt;/p&gt;

&lt;h4 id=&quot;distributed-rpcprofiler&quot;&gt;Distributed RPC/Profiler&lt;/h4&gt;
&lt;p&gt;Allow users to profile training jobs that use &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.distributed.rpc&lt;/code&gt; using the autograd profiler, and remotely invoke the profiler in order to collect profiling information across different nodes. The RFC can be found &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/39675&quot;&gt;here&lt;/a&gt; and a short recipe on how to use this feature can be found &lt;a href=&quot;https://github.com/pytorch/tutorials/tree/master/prototype_source&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;torchscript-module-freezing&quot;&gt;TorchScript Module Freezing&lt;/h4&gt;
&lt;p&gt;Module Freezing is the process of inlining module parameters and attributes values into the TorchScript internal representation. Parameter and attribute values are treated as final value and they cannot be modified in the frozen module. The PR for this feature can be found &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/32178&quot;&gt;here&lt;/a&gt; and a short tutorial on how to use this feature can be found &lt;a href=&quot;https://github.com/pytorch/tutorials/tree/master/prototype_source&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;graph-mode-quantization&quot;&gt;Graph Mode Quantization&lt;/h4&gt;
&lt;p&gt;Eager mode quantization requires users to make changes to their model, including explicitly quantizing activations, module fusion, rewriting use of torch ops with Functional Modules and quantization of functionals are not supported. If we can trace or script the model, then the quantization can be done automatically with graph mode quantization without any of the complexities in eager mode, and it is configurable through a &lt;code class=&quot;highlighter-rouge&quot;&gt;qconfig_dict&lt;/code&gt;. A tutorial on how to use this feature can be found &lt;a href=&quot;https://github.com/pytorch/tutorials/tree/master/prototype_source&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;quantization-numerical-suite&quot;&gt;Quantization Numerical Suite&lt;/h4&gt;
&lt;p&gt;Quantization is good when it works, but it’s difficult to know what’s wrong when it doesn’t satisfy the expected accuracy. A prototype is now available for a Numerical Suite that measures comparison statistics between quantized modules and float modules. This is available to test using eager mode and on CPU only with more support coming. A tutorial on how to use this feature can be found &lt;a href=&quot;https://github.com/pytorch/tutorials/tree/master/prototype_source&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we’re announcing the availability of PyTorch 1.6, along with updated domain libraries. We are also excited to announce the team at Microsoft is now maintaining Windows builds and binaries and will also be supporting the community on GitHub as well as the PyTorch Windows discussion forums.</summary></entry><entry><title type="html">PyTorch feature classification changes</title><link href="https://pytorch.org/blog/pytorch-feature-classification-changes/" rel="alternate" type="text/html" title="PyTorch feature classification changes" /><published>2020-07-28T00:00:00-07:00</published><updated>2020-07-28T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-feature-classification-changes</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-feature-classification-changes/">&lt;p&gt;Traditionally features in PyTorch were classified as either stable or experimental with an implicit third option of testing bleeding edge features by building master or through installing nightly builds (available via prebuilt whls). This has, in a few cases, caused some confusion around the level of readiness, commitment to the feature and backward compatibility that can be expected from a user perspective. Moving forward, we’d like to better classify the 3 types of features as well as define explicitly here what each mean from a user perspective.&lt;/p&gt;

&lt;h1 id=&quot;new-feature-designations&quot;&gt;New Feature Designations&lt;/h1&gt;

&lt;p&gt;We will continue to have three designations for features but, as mentioned, with a few changes: Stable, Beta (previously Experimental) and Prototype (previously Nightlies). Below is a brief description of each and a comment on the backward compatibility expected:&lt;/p&gt;

&lt;h2 id=&quot;stable&quot;&gt;Stable&lt;/h2&gt;
&lt;p&gt;Nothing changes here. A stable feature means that the user value-add is or has been proven, the API isn’t expected to change, the feature is performant and all documentation exists to support end user adoption.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Level of commitment&lt;/em&gt;: We expect to maintain these features long term and generally there should be no major performance limitations, gaps in documentation and we also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).&lt;/p&gt;

&lt;h2 id=&quot;beta&quot;&gt;Beta&lt;/h2&gt;
&lt;p&gt;We previously called these features ‘Experimental’ and we found that this created confusion amongst some of the users. In the case of a Beta level features, the value add, similar to a Stable feature, has been proven (e.g. pruning is a commonly used technique for reducing the number of parameters in NN models, independent of the implementation details of our particular choices) and the feature generally works and is documented. This feature is tagged as Beta because the API may change based on user feedback, because the performance needs to improve or because coverage across operators is not yet complete.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Level of commitment&lt;/em&gt;: We are committing to seeing the feature through to the Stable classification. We are however not committing to Backwards Compatibility. Users can depend on us providing a solution for problems in this area going forward, but the APIs and performance characteristics of this feature may change.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/install-matrix.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;prototype&quot;&gt;Prototype&lt;/h2&gt;
&lt;p&gt;Previously these were features that were known about by developers who paid close attention to RFCs and to features that land in master. In this case the feature is not available as part of binary distributions like PyPI or Conda (except maybe behind run-time flags), but we would like to get high bandwidth partner feedback ahead of a real release in order to gauge utility and any changes we need to make to the UX. To test these kinds of features we would, depending on the feature, recommend building from master or using the nightly whls that are made available on pytorch.org. For each prototype feature, a pointer to draft docs or other instructions will be provided.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Level of commitment&lt;/em&gt;: We are committing to gathering high bandwidth feedback only. Based on this feedback and potential further engagement between community members, we as a community will decide if we want to upgrade the level of commitment or to fail fast. Additionally, while some of these features might be more speculative (e.g. new Frontend APIs), others have obvious utility (e.g. model optimization) but may be in a state where gathering feedback outside of high bandwidth channels is not practical, e.g. the feature may be in an earlier state, may be moving fast (PRs are landing too quickly to catch a major release) and/or generally active development is underway.&lt;/p&gt;

&lt;h1 id=&quot;what-changes-for-current-features&quot;&gt;What changes for current features?&lt;/h1&gt;

&lt;p&gt;First and foremost, you can find these designations on &lt;a href=&quot;http://pytorch.org/docs&quot;&gt;pytorch.org/docs&lt;/a&gt;. We will also be linking any early stage features here for clarity.&lt;/p&gt;

&lt;p&gt;Additionally, the following features will be reclassified under this new rubric:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api&quot;&gt;High Level Autograd APIs&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/quantization.html&quot;&gt;Eager Mode Quantization&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/named_tensor.html&quot;&gt;Named Tensors&lt;/a&gt;: Prototype (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html#rpc&quot;&gt;TorchScript/RPC&lt;/a&gt;: Prototype (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/tensor_attributes.html#torch-memory-format&quot;&gt;Channels Last Memory Layout&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/jit.html?highlight=experimental&quot;&gt;Custom C++ Classes&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/mobile/home/&quot;&gt;PyTorch Mobile&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/packages.html#&quot;&gt;Java Bindings&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html?highlight=experimental#&quot;&gt;Torch.Sparse&lt;/a&gt;: Beta (was Experimental)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;Joe, Greg, Woo &amp;amp; Jessica&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Traditionally features in PyTorch were classified as either stable or experimental with an implicit third option of testing bleeding edge features by building master or through installing nightly builds (available via prebuilt whls). This has, in a few cases, caused some confusion around the level of readiness, commitment to the feature and backward compatibility that can be expected from a user perspective. Moving forward, we’d like to better classify the 3 types of features as well as define explicitly here what each mean from a user perspective.</summary></entry><entry><title type="html">Microsoft becomes maintainer of the Windows version of PyTorch</title><link href="https://pytorch.org/blog/microsoft-becomes-maintainer-of-the-windows-version-of-pytorch/" rel="alternate" type="text/html" title="Microsoft becomes maintainer of the Windows version of PyTorch" /><published>2020-07-28T00:00:00-07:00</published><updated>2020-07-28T00:00:00-07:00</updated><id>https://pytorch.org/blog/microsoft-becomes-maintainer-of-the-windows-version-of-pytorch</id><content type="html" xml:base="https://pytorch.org/blog/microsoft-becomes-maintainer-of-the-windows-version-of-pytorch/">&lt;p&gt;Along with the PyTorch 1.6 release, we are excited to announce that Microsoft has expanded its participation in the PyTorch community and will be responsible for the development and maintenance of the PyTorch build for Windows.&lt;/p&gt;

&lt;p&gt;According to the latest &lt;a href=&quot;https://insights.stackoverflow.com/survey/2020#technology-developers-primary-operating-systems&quot;&gt;Stack Overflow developer survey&lt;/a&gt;, Windows remains the primary operating system for the developer community (46% Windows vs 28% MacOS). &lt;a href=&quot;https://github.com/peterjc123&quot;&gt;Jiachen Pu&lt;/a&gt; initially made a heroic effort to add support for PyTorch on Windows, but due to limited resources, Windows support for PyTorch has lagged behind other platforms. Lack of test coverage resulted in unexpected issues popping up every now and then. Some of the core tutorials, meant for new users to learn and adopt PyTorch, would fail to run. The installation experience was also not as smooth, with the lack of official PyPI support for PyTorch on Windows. Lastly, some of the PyTorch functionality was simply not available on the Windows platform, such as the TorchAudio domain library and distributed training support. To help alleviate this pain, Microsoft is happy to bring its Windows expertise to the table and bring PyTorch on Windows to its best possible self.&lt;/p&gt;

&lt;p&gt;In the PyTorch 1.6 release, we have improved the core quality of the Windows build by bringing test coverage up to par with Linux for core PyTorch and its domain libraries and by automating tutorial testing. Thanks to the broader PyTorch community, which contributed TorchAudio support to Windows, we were able to add test coverage to all three domain libraries: TorchVision, TorchText and TorchAudio. In subsequent releases of PyTorch, we will continue improving the Windows experience based on community feedback and requests. So far, the feedback we received from the community points to distributed training support and a better installation experience using pip as the next areas of improvement.&lt;/p&gt;

&lt;p&gt;In addition to the native Windows experience, Microsoft released a preview adding &lt;a href=&quot;https://blogs.windows.com/windowsdeveloper/2020/06/17/gpu-accelerated-ml-training-inside-the-windows-subsystem-for-linux/&quot;&gt;GPU compute support to Windows Subsystem for Linux (WSL) 2&lt;/a&gt; distros, with a focus on enabling AI and ML developer workflows. WSL is designed for developers that want to run any Linux based tools directly on Windows. This preview enables valuable scenarios for a variety of frameworks and Python packages that utilize &lt;a href=&quot;https://developer.nvidia.com/cuda/wsl&quot;&gt;NVIDIA CUDA&lt;/a&gt; for acceleration and only support Linux. This means WSL customers using the preview can run native Linux based PyTorch applications on Windows unmodified without the need for a traditional virtual machine or a dual boot setup.&lt;/p&gt;

&lt;h2 id=&quot;getting-started-with-pytorch-on-windows&quot;&gt;Getting started with PyTorch on Windows&lt;/h2&gt;
&lt;p&gt;It’s easy to get started with PyTorch on Windows. To install PyTorch using Anaconda with the latest GPU support, run the command below. To install different supported configurations of PyTorch, refer to the installation instructions on &lt;a href=&quot;https://pytorch.org&quot;&gt;pytorch.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;conda install pytorch torchvision cudatoolkit=10.2 -c pytorch&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Once you install PyTorch, learn more by visiting the &lt;a href=&quot;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&quot;&gt;PyTorch Tutorials&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/pytorch1.6.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;getting-started-with-pytorch-on-windows-subsystem-for-linux&quot;&gt;Getting started with PyTorch on Windows Subsystem for Linux&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/win32/direct3d12/gpu-cuda-in-wsl&quot;&gt;preview of NVIDIA CUDA support in WSL&lt;/a&gt; is now available to Windows Insiders running Build 20150 or higher. In WSL, the command to install PyTorch using Anaconda is the same as the above command for native Windows. If you prefer pip, use the command below.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pip install torch torchvision&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You can use the same tutorials and documentation inside your WSL environment as on native Windows. This functionality is still in preview so if you run into issues with WSL please share feedback via the &lt;a href=&quot;https://github.com/microsoft/WSL&quot;&gt;WSL GitHub repo&lt;/a&gt; or with NVIDIA CUDA support share via NVIDIA’s &lt;a href=&quot;https://forums.developer.nvidia.com/c/accelerated-computing/cuda/cuda-on-windows-subsystem-for-linux/303&quot;&gt;Community Forum for CUDA on WSL&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;feedback&quot;&gt;Feedback&lt;/h2&gt;
&lt;p&gt;If you find gaps in the PyTorch experience on Windows, please let us know on the &lt;a href=&quot;https://discuss.pytorch.org/c/windows/26&quot;&gt;PyTorch discussion forum&lt;/a&gt; or file an issue on &lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;GitHub&lt;/a&gt; using the #module: windows label.&lt;/p&gt;</content><author><name>Maxim Lukiyanov - Principal PM at Microsoft, Emad Barsoum - Group EM at Microsoft, Guoliang Hua - Principal EM at Microsoft, Nikita Shulga - Tech Lead at Facebook, Geeta Chauhan - PE Lead at Facebook, Chris Gottbrath - Technical PM at Facebook, Jiachen Pu - Engineer at Facebook</name></author><summary type="html">Along with the PyTorch 1.6 release, we are excited to announce that Microsoft has expanded its participation in the PyTorch community and will be responsible for the development and maintenance of the PyTorch build for Windows.</summary></entry><entry><title type="html">Introducing native PyTorch automatic mixed precision for faster training on NVIDIA GPUs</title><link href="https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/" rel="alternate" type="text/html" title="Introducing native PyTorch automatic mixed precision for faster training on NVIDIA GPUs" /><published>2020-07-28T00:00:00-07:00</published><updated>2020-07-28T00:00:00-07:00</updated><id>https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision</id><content type="html" xml:base="https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/">&lt;p&gt;Most deep learning frameworks, including PyTorch, train with 32-bit floating point (FP32) arithmetic by default. However this is not essential to achieve full accuracy for many deep learning models. In 2017, NVIDIA researchers developed a methodology for &lt;a href=&quot;https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/&quot;&gt;mixed-precision training&lt;/a&gt;, which combined &lt;a href=&quot;https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/&quot;&gt;single-precision&lt;/a&gt; (FP32) with half-precision (e.g. FP16) format when training a network, and achieved the same accuracy as FP32 training using the same hyperparameters, with additional performance benefits on NVIDIA GPUs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shorter training time;&lt;/li&gt;
  &lt;li&gt;Lower memory requirements, enabling larger batch sizes, larger models, or larger inputs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to streamline the user experience of training in mixed precision for researchers and practitioners, NVIDIA developed &lt;a href=&quot;https://developer.nvidia.com/blog/apex-pytorch-easy-mixed-precision-training/&quot;&gt;Apex&lt;/a&gt; in 2018, which is a lightweight PyTorch extension with &lt;a href=&quot;https://developer.nvidia.com/automatic-mixed-precision&quot;&gt;Automatic Mixed Precision&lt;/a&gt; (AMP) feature. This feature enables automatic conversion of certain GPU operations from FP32 precision to mixed precision, thus improving performance while maintaining accuracy.&lt;/p&gt;

&lt;p&gt;For the PyTorch 1.6 release, developers at NVIDIA and Facebook moved mixed precision functionality into PyTorch core as the AMP package, &lt;a href=&quot;https://pytorch.org/docs/stable/amp.html&quot;&gt;torch.cuda.amp&lt;/a&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.amp&lt;/code&gt; is more flexible and intuitive compared to &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp&lt;/code&gt;. Some of &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp&lt;/code&gt;’s known pain points that &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.amp&lt;/code&gt; has been able to fix:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Guaranteed PyTorch version compatibility, because it’s part of PyTorch&lt;/li&gt;
  &lt;li&gt;No need to build extensions&lt;/li&gt;
  &lt;li&gt;Windows support&lt;/li&gt;
  &lt;li&gt;Bitwise accurate &lt;a href=&quot;https://pytorch.org/docs/master/amp.html#torch.cuda.amp.GradScaler.load_state_dict&quot;&gt;saving/restoring&lt;/a&gt; of checkpoints&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/master/notes/amp_examples.html#dataparallel-in-a-single-process&quot;&gt;DataParallel&lt;/a&gt; and intra-process model parallelism (although we still recommend &lt;a href=&quot;https://pytorch.org/docs/master/notes/amp_examples.html#distributeddataparallel-one-gpu-per-process&quot;&gt;torch.nn.DistributedDataParallel&lt;/a&gt; with one GPU per process as the most performant approach)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/master/notes/amp_examples.html#gradient-penalty&quot;&gt;Gradient penalty&lt;/a&gt; (double backward)&lt;/li&gt;
  &lt;li&gt;torch.cuda.amp.autocast() has no effect outside regions where it’s enabled, so it should serve cases that formerly struggled with multiple calls to &lt;a href=&quot;https://github.com/NVIDIA/apex/issues/439&quot;&gt;apex.amp.initialize()&lt;/a&gt; (including &lt;a href=&quot;https://github.com/NVIDIA/apex/issues/392#issuecomment-610038073&quot;&gt;cross-validation)&lt;/a&gt; without difficulty. Multiple convergence runs in the same script should each use a fresh &lt;a href=&quot;https://github.com/NVIDIA/apex/issues/439#issuecomment-610028282&quot;&gt;GradScaler instance&lt;/a&gt;, but GradScalers are lightweight and self-contained so that’s not a problem.&lt;/li&gt;
  &lt;li&gt;Sparse gradient support&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With AMP being added to PyTorch core, we have started the process of deprecating &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp.&lt;/code&gt; We have moved &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp&lt;/code&gt; to maintenance mode and will support customers using &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp.&lt;/code&gt; However, we highly encourage &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp&lt;/code&gt; customers to transition to using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.amp&lt;/code&gt; from PyTorch Core.&lt;/p&gt;

&lt;h1 id=&quot;example-walkthrough&quot;&gt;Example Walkthrough&lt;/h1&gt;
&lt;p&gt;Please see official docs for usage:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/amp.html&quot;&gt;https://pytorch.org/docs/stable/amp.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/amp_examples.html&quot;&gt;https://pytorch.org/docs/stable/notes/amp_examples.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt; 
&lt;span class=&quot;c&quot;&gt;# Creates once at the beginning of training &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
 
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
   &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
   &lt;span class=&quot;c&quot;&gt;# Casts operations to mixed precision &lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autocast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; 
      &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
 
   &lt;span class=&quot;c&quot;&gt;# Scales the loss, and calls backward() &lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# to create scaled gradients &lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
 
   &lt;span class=&quot;c&quot;&gt;# Unscales gradients and calls &lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# or skips optimizer.step() &lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
 
   &lt;span class=&quot;c&quot;&gt;# Updates the scale for next iteration &lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;performance-benchmarks&quot;&gt;Performance Benchmarks&lt;/h1&gt;
&lt;p&gt;In this section, we discuss the accuracy and performance of mixed precision training with AMP on the latest NVIDIA GPU A100 and also previous generation V100 GPU. The mixed precision performance is compared to FP32 performance, when running Deep Learning workloads in the &lt;a href=&quot;https://ngc.nvidia.com/catalog/containers/nvidia:pytorch?ncid=partn-52193#cid=ngc01_partn_en-us&quot;&gt;NVIDIA pytorch:20.06-py3 container&lt;/a&gt; from NGC.&lt;/p&gt;

&lt;h2 id=&quot;accuracy-amp-fp16-fp32&quot;&gt;Accuracy: AMP (FP16), FP32&lt;/h2&gt;
&lt;p&gt;The advantage of using AMP for Deep Learning training is that the models converge to the similar final accuracy while providing improved training performance. To illustrate this point, for &lt;a href=&quot;https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/resnet50v1.5#training-accuracy-nvidia-dgx-a100-8x-a100-40gb&quot;&gt;Resnet 50 v1.5 training&lt;/a&gt;, we see the following accuracy results where higher is better. Please note that the below accuracy numbers are sample numbers that are subject to run to run variance of up to 0.4%. Accuracy numbers for other models including BERT, Transformer, ResNeXt-101, Mask-RCNN, DLRM can be found at  &lt;a href=&quot;https://github.com/NVIDIA/DeepLearningExamples&quot;&gt;NVIDIA Deep Learning Examples Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Training accuracy: NVIDIA DGX A100 (8x A100 40GB)&lt;/p&gt;

&lt;table width=&quot;460&quot; border=&quot;0&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;&amp;nbsp;epochs&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;&amp;nbsp;Mixed Precision Top 1(%)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;strong&gt;TF32 Top1(%)&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&amp;nbsp;90&lt;/td&gt;
      &lt;td&gt;&amp;nbsp;76.93&lt;/td&gt;
      &lt;td&gt;&amp;nbsp;76.85&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Training accuracy: NVIDIA DGX-1 (8x V100 16GB)&lt;/p&gt;

&lt;table width=&quot;460&quot; border=&quot;0&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
     &lt;td&gt;&lt;strong&gt;&amp;nbsp;epochs&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;&amp;nbsp;Mixed Precision Top 1(%)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;strong&gt;FP32 Top1(%)&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;76.25&lt;/td&gt;
      &lt;td&gt;76.26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;77.09&lt;/td&gt;
      &lt;td&gt;77.01&lt;/td&gt;
    &lt;/tr&gt;
	  &lt;tr&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt;78.42&lt;/td&gt;
      &lt;td&gt;78.30&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;speedup-performance&quot;&gt;Speedup Performance:&lt;/h2&gt;

&lt;h3 id=&quot;fp16-on-nvidia-v100-vs-fp32-on-v100&quot;&gt;FP16 on NVIDIA V100 vs. FP32 on V100&lt;/h3&gt;
&lt;p&gt;AMP with FP16 is the most performant option for DL training on the V100. In Table 1, we can observe that for various models, AMP on V100 provides a speedup of 1.5x to 5.5x over FP32 on V100 while converging to the same final accuracy.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/nvidiafp32onv100.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Figure 2. Performance of mixed precision training on NVIDIA 8xV100 vs. FP32 training on 8xV100 GPU. Bars represent the speedup factor of V100 AMP over V100 FP32. The higher the better.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;fp16-on-nvidia-a100-vs-fp16-on-v100&quot;&gt;FP16 on NVIDIA A100 vs. FP16 on V100&lt;/h2&gt;

&lt;p&gt;AMP with FP16 remains the most performant option for DL training on the A100. In Figure 3, we can observe that for various models, AMP on A100 provides a speedup of 1.3x to 2.5x over AMP on V100 while converging to the same final accuracy.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/nvidiafp16onv100.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Figure 3. Performance of mixed precision training on NVIDIA 8xA100 vs. 8xV100 GPU. Bars represent the speedup factor of A100 over V100. The higher the better.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;call-to-action&quot;&gt;Call to action&lt;/h1&gt;
&lt;p&gt;AMP provides a healthy speedup for Deep Learning training workloads on Nvidia Tensor Core GPUs, especially on the latest Ampere generation A100 GPUs.  You can start experimenting with AMP enabled models and model scripts for A100, V100, T4 and other GPUs available at NVIDIA deep learning &lt;a href=&quot;https://github.com/NVIDIA/DeepLearningExamples&quot;&gt;examples&lt;/a&gt;. NVIDIA PyTorch with native AMP support is available from the &lt;a href=&quot;https://ngc.nvidia.com/catalog/containers/nvidia:pytorch?ncid=partn-52193#cid=ngc01_partn_en-us&quot;&gt;PyTorch NGC container&lt;/a&gt; version 20.06. We highly encourage existing &lt;code class=&quot;highlighter-rouge&quot;&gt;apex.amp&lt;/code&gt; customers to transition to using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.amp&lt;/code&gt; from PyTorch Core available in the latest &lt;a href=&quot;https://pytorch.org/blog/pytorch-1.6-released/&quot;&gt;PyTorch 1.6 release&lt;/a&gt;.&lt;/p&gt;</content><author><name>Mengdi Huang, Chetan Tekur, Michael Carilli</name></author><summary type="html">Most deep learning frameworks, including PyTorch, train with 32-bit floating point (FP32) arithmetic by default. However this is not essential to achieve full accuracy for many deep learning models. In 2017, NVIDIA researchers developed a methodology for mixed-precision training, which combined single-precision (FP32) with half-precision (e.g. FP16) format when training a network, and achieved the same accuracy as FP32 training using the same hyperparameters, with additional performance benefits on NVIDIA GPUs:</summary></entry><entry><title type="html">Updates &amp;amp; Improvements to PyTorch Tutorials</title><link href="https://pytorch.org/blog/updates-improvements-to-pytorch-tutorials/" rel="alternate" type="text/html" title="Updates &amp; Improvements to PyTorch Tutorials" /><published>2020-05-05T00:00:00-07:00</published><updated>2020-05-05T00:00:00-07:00</updated><id>https://pytorch.org/blog/updates-improvements-to-pytorch-tutorials</id><content type="html" xml:base="https://pytorch.org/blog/updates-improvements-to-pytorch-tutorials/">&lt;p&gt;PyTorch.org provides researchers and developers with documentation, installation instructions, latest news, community projects, tutorials, and more. Today, we are introducing usability and content improvements including tutorials in additional categories, a new recipe format for quickly referencing common topics, sorting using tags, and an updated homepage.&lt;/p&gt;

&lt;p&gt;Let’s take a look at them in detail.&lt;/p&gt;

&lt;h2 id=&quot;tutorials-home-page-update&quot;&gt;TUTORIALS HOME PAGE UPDATE&lt;/h2&gt;
&lt;p&gt;The tutorials home page now provides clear actions that developers can take. For new PyTorch users, there is an easy-to-discover button to take them directly to “A 60 Minute Blitz”. Right next to it, there is a button to view all recipes which are designed to teach specific features quickly with examples.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/tutorialhomepage.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;In addition to the existing left navigation bar, tutorials can now be quickly filtered by multi-select tags. Let’s say you want to view all tutorials related to “Production” and “Quantization”. You can select the “Production” and “Quantization” filters as shown in the image shown below:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/blockfiltering.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The following additional resources can also be found at the bottom of the Tutorials homepage:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/ptcheat.html&quot;&gt;PyTorch Cheat Sheet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/examples&quot;&gt;PyTorch Examples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/tutorials&quot;&gt;Tutorial on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pytorch-recipes&quot;&gt;PYTORCH RECIPES&lt;/h2&gt;
&lt;p&gt;Recipes are new bite-sized, actionable examples designed to teach researchers and developers how to use specific PyTorch features. Some notable new recipes include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html&quot;&gt;Loading Data in PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/Captum_Recipe.html&quot;&gt;Model Interpretability Using Captum&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html&quot;&gt;How to Use TensorBoard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;View the full recipes &lt;a href=&quot;http://pytorch.org/tutorials/recipes/recipes_index.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;learning-pytorch&quot;&gt;LEARNING PYTORCH&lt;/h2&gt;
&lt;p&gt;This section includes tutorials designed for users new to PyTorch. Based on community feedback, we have made updates to the current &lt;a href=&quot;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&quot;&gt;Deep Learning with PyTorch: A 60 Minute Blitz&lt;/a&gt; tutorial, one of our most popular tutorials for beginners. Upon completion, one can understand what PyTorch and neural networks are, and be able to build and train a simple image classification network. Updates include adding explanations to clarify output meanings and linking back to where users can read more in the docs, cleaning up confusing syntax errors, and reconstructing and explaining new concepts for easier readability.&lt;/p&gt;

&lt;h2 id=&quot;deploying-models-in-production&quot;&gt;DEPLOYING MODELS IN PRODUCTION&lt;/h2&gt;
&lt;p&gt;This section includes tutorials for developers looking to take their PyTorch models to production. The tutorials include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html&quot;&gt;Deploying PyTorch in Python via a REST API with Flask&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&quot;&gt;Introduction to TorchScript&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;Loading a TorchScript Model in C++&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html&quot;&gt;Exploring a Model from PyTorch to ONNX and Running it using ONNX Runtime&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;frontend-apis&quot;&gt;FRONTEND APIS&lt;/h2&gt;
&lt;p&gt;PyTorch provides a number of frontend API features that can help developers to code, debug, and validate their models more efficiently. This section includes tutorials that teach what these features are and how to use them. Some tutorials to highlight:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html&quot;&gt;Introduction to Named Tensors in PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_frontend.html&quot;&gt;Using the PyTorch C++ Frontend&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html&quot;&gt;Extending TorchScript with Custom C++ Operators&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html&quot;&gt;Extending TorchScript with Custom C++ Classes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_autograd.html&quot;&gt;Autograd in C++ Frontend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-optimization&quot;&gt;MODEL OPTIMIZATION&lt;/h2&gt;
&lt;p&gt;Deep learning models often consume large amounts of memory, power, and compute due to their complexity. This section provides tutorials for model optimization:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/pruning_tutorial.html&quot;&gt;Pruning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;Dynamic Quantization on BERT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;Static Quantization with Eager Mode in PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parallel-and-distributed-training&quot;&gt;PARALLEL AND DISTRIBUTED TRAINING&lt;/h2&gt;
&lt;p&gt;PyTorch provides features that can accelerate performance in research and production such as native support for asynchronous execution of collective operations and peer-to-peer communication that is accessible from Python and C++. This section includes tutorials on parallel and distributed training:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html&quot;&gt;Single-Machine Model Parallel Best Practices&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/ddp_tutorial.html&quot;&gt;Getting started with Distributed Data Parallel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/rpc_tutorial.html&quot;&gt;Getting started with Distributed RPC Framework&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html&quot;&gt;Implementing a Parameter Server Using Distributed RPC Framework&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Making these improvements are just the first step of improving PyTorch.org for the community. Please submit your suggestions &lt;a href=&quot;https://github.com/pytorch/tutorials/pulls&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">PyTorch.org provides researchers and developers with documentation, installation instructions, latest news, community projects, tutorials, and more. Today, we are introducing usability and content improvements including tutorials in additional categories, a new recipe format for quickly referencing common topics, sorting using tags, and an updated homepage.</summary></entry><entry><title type="html">PyTorch library updates including new model serving library</title><link href="https://pytorch.org/blog/pytorch-library-updates-new-model-serving-library/" rel="alternate" type="text/html" title="PyTorch library updates including new model serving library " /><published>2020-04-21T00:00:00-07:00</published><updated>2020-04-21T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-library-updates-new-model-serving-library</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-library-updates-new-model-serving-library/">&lt;p&gt;Along with the PyTorch 1.5 release, we are announcing new libraries for high-performance PyTorch model serving and tight integration with TorchElastic and Kubernetes. Additionally, we are releasing updated packages for torch_xla (Google Cloud TPUs), torchaudio, torchvision, and torchtext. All of these new libraries and enhanced capabilities are available today and accompany all of the core features &lt;a href=&quot;https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis&quot;&gt;released in PyTorch 1.5&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchserve-experimental&quot;&gt;TorchServe (Experimental)&lt;/h2&gt;

&lt;p&gt;TorchServe is a flexible and easy to use library for serving PyTorch models in production performantly at scale. It is cloud and environment agnostic and supports features such as multi-model serving, logging, metrics, and the creation of RESTful endpoints for application integration. TorchServe was jointly developed by engineers from Facebook and AWS with feedback and engagement from the broader PyTorch community. The experimental release of TorchServe is available today. Some of the highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support for both Python-based and TorchScript-based models&lt;/li&gt;
  &lt;li&gt;Default handlers for common use cases (e.g., image segmentation, text classification) as well as the ability to write custom handlers for other use cases&lt;/li&gt;
  &lt;li&gt;Model versioning, the ability to run multiple versions of a model at the same time, and the ability to roll back to an earlier version&lt;/li&gt;
  &lt;li&gt;The ability to package a model, learning weights, and supporting files (e.g., class mappings, vocabularies) into a single, persistent artifact (a.k.a. the “model archive”)&lt;/li&gt;
  &lt;li&gt;Robust management capability, allowing full configuration of models, versions, and individual worker threads via command line, config file, or run-time API&lt;/li&gt;
  &lt;li&gt;Automatic batching of individual inferences across HTTP requests&lt;/li&gt;
  &lt;li&gt;Logging including common metrics, and the ability to incorporate custom metrics&lt;/li&gt;
  &lt;li&gt;Ready-made Dockerfile for easy deployment&lt;/li&gt;
  &lt;li&gt;HTTPS support for secure deployment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To learn more about the APIs and the design of this feature, see the links below:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;See &lt;here&gt; for a full multi-node deployment reference architecture.&lt;/here&gt;&lt;/li&gt;
  &lt;li&gt;The full documentation can be found &lt;a href=&quot;https://pytorch.org/serve&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;torchelastic-integration-with-kubernetes-experimental&quot;&gt;TorchElastic integration with Kubernetes (Experimental)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/elastic&quot;&gt;TorchElastic&lt;/a&gt; is a proven library for training large scale deep neural networks at scale within companies like Facebook, where having the ability to dynamically adapt to server availability and scale as new compute resources come online is critical. Kubernetes enables customers using machine learning frameworks like PyTorch to run training jobs distributed across fleets of powerful GPU instances like the Amazon EC2 P3. Distributed training jobs, however, are not fault-tolerant, and a job cannot continue if a node failure or reclamation interrupts training. Further, jobs cannot start without acquiring all required resources, or scale up and down without being restarted. This lack of resiliency and flexibility results in increased training time and costs from idle resources. TorchElastic addresses these limitations by enabling distributed training jobs to be executed in a fault-tolerant and elastic manner. Until today, Kubernetes users needed to manage Pods and Services required for TorchElastic training jobs manually.&lt;/p&gt;

&lt;p&gt;Through the joint collaboration of engineers at Facebook and AWS, TorchElastic, adding elasticity and fault tolerance, is now supported using vanilla Kubernetes and through the managed EKS service from AWS.&lt;/p&gt;

&lt;p&gt;To learn more see the &lt;a href=&quot;http://pytorch.org/elastic/0.2.0rc0/kubernetes.html&quot;&gt;TorchElastic repo&lt;/a&gt; for the controller implementation and docs on how to use it.&lt;/p&gt;

&lt;h2 id=&quot;torch_xla-15-now-available&quot;&gt;torch_xla 1.5 now available&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://pytorch.org/xla/&quot;&gt;torch_xla&lt;/a&gt; is a Python package that uses the &lt;a href=&quot;https://www.tensorflow.org/xla&quot;&gt;XLA linear algebra compiler&lt;/a&gt; to accelerate the &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch deep learning framework&lt;/a&gt; on &lt;a href=&quot;https://cloud.google.com/tpu/&quot;&gt;Cloud TPUs&lt;/a&gt; and &lt;a href=&quot;https://cloud.google.com/tpu/docs/tutorials/pytorch-pod&quot;&gt;Cloud TPU Pods&lt;/a&gt;. torch_xla aims to give PyTorch users the ability to do everything they can do on GPUs on Cloud TPUs as well while minimizing changes to the user experience. The project began with a conversation at NeurIPS 2017 and gathered momentum in 2018 when teams from Facebook and Google came together to create a proof of concept. We announced this collaboration at PTDC 2018 and made the PyTorch/XLA integration broadly available at PTDC 2019. The project already has 28 contributors, nearly 2k commits, and a repo that has been forked more than 100 times.&lt;/p&gt;

&lt;p&gt;This release of &lt;a href=&quot;http://pytorch.org/xla/&quot;&gt;torch_xla&lt;/a&gt; is aligned and tested with PyTorch 1.5 to reduce friction for developers and to provide a stable and mature PyTorch/XLA stack for training models using Cloud TPU hardware. You can &lt;a href=&quot;https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc&quot;&gt;try it for free&lt;/a&gt; in your browser on an 8-core Cloud TPU device with &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Google Colab&lt;/a&gt;, and you can use it at a much larger scaleon &lt;a href=&quot;https://cloud.google.com/gcp&quot;&gt;Google Cloud&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See the full torch_xla release notes &lt;a href=&quot;https://github.com/pytorch/xla/releases&quot;&gt;here&lt;/a&gt;. Full docs and tutorials can be found &lt;a href=&quot;https://pytorch.org/xla/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://cloud.google.com/tpu/docs/tutorials&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-domain-libraries&quot;&gt;PyTorch Domain Libraries&lt;/h2&gt;

&lt;p&gt;torchaudio, torchvision, and torchtext complement PyTorch with common datasets, models, and transforms in each domain area. We’re excited to share new releases for all three domain libraries alongside PyTorch 1.5 and the rest of the library updates. For this release, all three domain libraries are removing support for Python2 and will support Python3 only.&lt;/p&gt;

&lt;h3 id=&quot;torchaudio-05&quot;&gt;torchaudio 0.5&lt;/h3&gt;
&lt;p&gt;The torchaudio 0.5 release includes new transforms, functionals, and datasets. Highlights for the release include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Added the Griffin-Lim functional and transform, &lt;code class=&quot;highlighter-rouge&quot;&gt;InverseMelScale&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Vol&lt;/code&gt; transforms, and &lt;code class=&quot;highlighter-rouge&quot;&gt;DB_to_amplitude&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Added support for &lt;code class=&quot;highlighter-rouge&quot;&gt;allpass&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fade&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;bandpass&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;bandreject&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;band&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;treble&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;deemph&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;riaa&lt;/code&gt; filters and transformations.&lt;/li&gt;
  &lt;li&gt;New datasets added including &lt;code class=&quot;highlighter-rouge&quot;&gt;LJSpeech&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;SpeechCommands&lt;/code&gt; datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the release full notes &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt; and full docs can be found &lt;a href=&quot;https://pytorch.org/audio/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchvision-06&quot;&gt;torchvision 0.6&lt;/h3&gt;
&lt;p&gt;The torchvision 0.6 release includes updates to datasets, models and a significant number of bug fixes. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Faster R-CNN now supports negative samples which allows the feeding of images without annotations at training time.&lt;/li&gt;
  &lt;li&gt;Added &lt;code class=&quot;highlighter-rouge&quot;&gt;aligned&lt;/code&gt; flag to &lt;code class=&quot;highlighter-rouge&quot;&gt;RoIAlign&lt;/code&gt; to match Detectron2.&lt;/li&gt;
  &lt;li&gt;Refactored abstractions for C++ video decoder&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the release full notes &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt; and full docs can be found &lt;a href=&quot;https://pytorch.org/docs/stable/torchvision/index.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchtext-06&quot;&gt;torchtext 0.6&lt;/h3&gt;
&lt;p&gt;The torchtext 0.6 release includes a number of bug fixes and improvements to documentation. Based on user’s feedback, dataset abstractions are currently being redesigned also. Highlights for the release include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fixed an issue related to the SentencePiece dependency in conda package.&lt;/li&gt;
  &lt;li&gt;Added support for the experimental IMDB dataset to allow a custom vocab.&lt;/li&gt;
  &lt;li&gt;A number of documentation updates including adding a code of conduct and a deduplication of the docs on the torchtext site.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Your feedback and discussions on the experimental datasets API are welcomed. You can send them to &lt;a href=&quot;https://github.com/pytorch/text/issues/664&quot;&gt;issue #664&lt;/a&gt;. We would also like to highlight the pull request &lt;a href=&quot;https://github.com/pytorch/text/pull/701&quot;&gt;here&lt;/a&gt; where the latest dataset abstraction is applied to the text classification datasets. The feedback can be beneficial to finalizing this abstraction.&lt;/p&gt;

&lt;p&gt;See the release full notes &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt; and full docs can be found &lt;a href=&quot;https://pytorch.org/text/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team, the Amazon team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Along with the PyTorch 1.5 release, we are announcing new libraries for high-performance PyTorch model serving and tight integration with TorchElastic and Kubernetes. Additionally, we are releasing updated packages for torch_xla (Google Cloud TPUs), torchaudio, torchvision, and torchtext. All of these new libraries and enhanced capabilities are available today and accompany all of the core features released in PyTorch 1.5.</summary></entry><entry><title type="html">PyTorch 1.5 released, new and updated APIs including C++ frontend API parity with Python</title><link href="https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis/" rel="alternate" type="text/html" title="PyTorch 1.5 released, new and updated APIs including C++ frontend API parity with Python" /><published>2020-04-21T00:00:00-07:00</published><updated>2020-04-21T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis/">&lt;p&gt;Today, we’re announcing the availability of PyTorch 1.5, along with new and updated libraries. This release includes several major new API additions and improvements. PyTorch now includes a significant update to the C++ frontend, ‘channels last’ memory format for computer vision models, and a stable release of the distributed RPC framework used for model-parallel training. The release also has new APIs for autograd for hessians and jacobians, and an API that allows the creation of Custom C++ Classes that was inspired by pybind.&lt;/p&gt;

&lt;p&gt;You can find the detailed release notes &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;c-frontend-api-stable&quot;&gt;C++ Frontend API (Stable)&lt;/h2&gt;

&lt;p&gt;The C++ frontend API is now at parity with Python, and the features overall have been moved to ‘stable’ (previously tagged as experimental). Some of the major highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Now with ~100% coverage and docs for C++ torch::nn module/functional, users can easily translate their model from Python API to C++ API, making the model authoring experience much smoother.&lt;/li&gt;
  &lt;li&gt;Optimizers in C++ had deviated from the Python equivalent: C++ optimizers can’t take parameter groups as input while the Python ones can. Additionally, step function implementations were not exactly the same. With the 1.5 release, C++ optimizers will always behave the same as the Python equivalent.&lt;/li&gt;
  &lt;li&gt;The lack of tensor multi-dim indexing API in C++ is a well-known issue and had resulted in many posts in PyTorch Github issue tracker and forum. The previous workaround was to use a combination of &lt;code class=&quot;highlighter-rouge&quot;&gt;narrow&lt;/code&gt; / &lt;code class=&quot;highlighter-rouge&quot;&gt;select&lt;/code&gt; / &lt;code class=&quot;highlighter-rouge&quot;&gt;index_select&lt;/code&gt; / &lt;code class=&quot;highlighter-rouge&quot;&gt;masked_select&lt;/code&gt;, which was clunky and error-prone compared to the Python API’s elegant &lt;code class=&quot;highlighter-rouge&quot;&gt;tensor[:, 0, ..., mask]&lt;/code&gt; syntax. With the 1.5 release, users can use &lt;code class=&quot;highlighter-rouge&quot;&gt;tensor.index({Slice(), 0, &quot;...&quot;, mask})&lt;/code&gt; to achieve the same purpose.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;channels-last-memory-format-for-computer-vision-models-experimental&quot;&gt;‘Channels last’ memory format for Computer Vision models (Experimental)&lt;/h2&gt;

&lt;p&gt;‘Channels last’ memory layout unlocks ability to use performance efficient convolution algorithms and hardware (NVIDIA’s Tensor Cores, FBGEMM, QNNPACK). Additionally, it is designed to automatically propagate through the operators, which allows easy switching between memory layouts.&lt;/p&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators&quot;&gt;here&lt;/a&gt; on how to write memory format aware operators.&lt;/p&gt;

&lt;h2 id=&quot;custom-c-classes-experimental&quot;&gt;Custom C++ Classes (Experimental)&lt;/h2&gt;

&lt;p&gt;This release adds a new API, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch::class_&lt;/code&gt;, for binding custom C++ classes into TorchScript and Python simultaneously. This API is almost identical in syntax to &lt;a href=&quot;https://pybind11.readthedocs.io/en/stable/&quot;&gt;pybind11&lt;/a&gt;. It allows users to expose their C++ class and its methods to the TorchScript type system and runtime system such that they can instantiate and manipulate arbitrary C++ objects from TorchScript and Python. An example C++ binding:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CustomClassHolder&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testStack&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;myclasses&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;MyStackClass&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;push&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pop&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intrusive_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which exposes a class you can use in Python and TorchScript like so:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nd&quot;&gt;@torch.jit.script&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;do_stacks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myclasses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myclasses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyStackClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hi&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;mom&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# &quot;mom&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;foobar&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [&quot;hi&quot;, &quot;foobar&quot;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can try it out in the tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;distributed-rpc-framework-apis-now-stable&quot;&gt;Distributed RPC framework APIs (Now Stable)&lt;/h2&gt;

&lt;p&gt;The Distributed &lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html&quot;&gt;RPC framework&lt;/a&gt; was launched as experimental in the 1.4 release and the proposal is to mark Distributed RPC framework as stable and no longer experimental. This work involves a lot of enhancements and bug fixes to make the distributed RPC framework more reliable and robust overall, as well as adding a couple of new features, including profiling support, using TorchScript functions in RPC, and several enhancements for ease of use. Below is an overview of the various APIs within the framework:&lt;/p&gt;

&lt;h3 id=&quot;rpc-api&quot;&gt;RPC API&lt;/h3&gt;
&lt;p&gt;The RPC API allows users to specify functions to run and objects to be instantiated on remote nodes. These functions are transparently recorded so that gradients can backpropagate through remote nodes using Distributed Autograd.&lt;/p&gt;

&lt;h3 id=&quot;distributed-autograd&quot;&gt;Distributed Autograd&lt;/h3&gt;
&lt;p&gt;Distributed Autograd connects the autograd graph across several nodes and allows gradients to flow through during the backwards pass. Gradients are accumulated into a context (as opposed to the .grad field as with Autograd) and users must specify their model’s forward pass under a with &lt;code class=&quot;highlighter-rouge&quot;&gt;dist_autograd.context()&lt;/code&gt; manager in order to ensure that all RPC communication is recorded properly. Currently, only FAST mode is implemented (see &lt;a href=&quot;https://pytorch.org/docs/stable/rpc/distributed_autograd.html#distributed-autograd-design&quot;&gt;here&lt;/a&gt; for the difference between FAST and SMART modes).&lt;/p&gt;

&lt;h3 id=&quot;distributed-optimizer&quot;&gt;Distributed Optimizer&lt;/h3&gt;
&lt;p&gt;The distributed optimizer creates RRefs to optimizers on each worker with parameters that require gradients, and then uses the RPC API to run the optimizer remotely. The user must collect all remote parameters and wrap them in an &lt;code class=&quot;highlighter-rouge&quot;&gt;RRef&lt;/code&gt;, as this is required input to the distributed optimizer. The user must also specify the distributed autograd &lt;code class=&quot;highlighter-rouge&quot;&gt;context_id&lt;/code&gt; so that the optimizer knows in which context to look for gradients.&lt;/p&gt;

&lt;p&gt;Learn more about distributed RPC framework APIs &lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;new-high-level-autograd-api-experimental&quot;&gt;New High level autograd API (Experimental)&lt;/h2&gt;

&lt;p&gt;PyTorch 1.5 brings new functions including jacobian, hessian, jvp, vjp, hvp and vhp to the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.autograd.functional&lt;/code&gt; submodule. This feature builds on the current API and allows the user to easily perform these functions.&lt;/p&gt;

&lt;p&gt;Detailed design discussion on GitHub can be found &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/30632&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;python-2-no-longer-supported&quot;&gt;Python 2 no longer supported&lt;/h2&gt;

&lt;p&gt;Starting PyTorch 1.5.0, we will no longer support Python 2, specifically version 2.7. Going forward support for Python will be limited to Python 3, specifically Python 3.5, 3.6, 3.7 and 3.8 (first enabled in PyTorch 1.4.0).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we’re announcing the availability of PyTorch 1.5, along with new and updated libraries. This release includes several major new API additions and improvements. PyTorch now includes a significant update to the C++ frontend, ‘channels last’ memory format for computer vision models, and a stable release of the distributed RPC framework used for model-parallel training. The release also has new APIs for autograd for hessians and jacobians, and an API that allows the creation of Custom C++ Classes that was inspired by pybind.</summary></entry></feed>