<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pytorch.org/" rel="alternate" type="text/html" /><updated>2021-08-18T15:51:39-07:00</updated><id>https://pytorch.org/feed.xml</id><title type="html">PyTorch Website</title><subtitle>Scientific Computing...</subtitle><author><name>Facebook</name></author><entry><title type="html">PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models</title><link href="https://pytorch.org/blog/pipetransformer-automated-elastic-pipelining/" rel="alternate" type="text/html" title="PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models" /><published>2021-08-18T00:00:00-07:00</published><updated>2021-08-18T00:00:00-07:00</updated><id>https://pytorch.org/blog/pipetransformer-automated-elastic-pipelining</id><content type="html" xml:base="https://pytorch.org/blog/pipetransformer-automated-elastic-pipelining/">&lt;p&gt;In this blog post, we describe the first peer-reviewed research paper that explores accelerating the hybrid of PyTorch DDP (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;) [1] and Pipeline (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.distributed.pipeline&lt;/code&gt;) - &lt;a href=&quot;http://proceedings.mlr.press/v139/he21a.html&quot;&gt;PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models&lt;/a&gt; (Transformers such as BERT [2]  and ViT [3]), published at ICML 2021.&lt;/p&gt;

&lt;p&gt;PipeTransformer leverages automated elastic pipelining for efficient distributed training of Transformer models. In PipeTransformer, we designed an adaptive on-the-fly freeze algorithm that can identify and freeze some layers gradually during training and an elastic pipelining system that can dynamically allocate resources to train the remaining active layers. More specifically, PipeTransformer automatically excludes frozen layers from the pipeline, packs active layers into fewer GPUs, and forks more replicas to increase data-parallel width. We evaluate PipeTransformer using Vision Transformer (ViT) on ImageNet and BERT on SQuAD and GLUE datasets. Our results show that compared to the state-of-the-art baseline, PipeTransformer attains up to 2.83-fold speedup without losing accuracy. We also provide various performance analyses for a more comprehensive understanding of our algorithmic and system-wise design.&lt;/p&gt;

&lt;p&gt;Next, we will introduce the background, motivation, our idea, design, and how we implement the algorithm and system with PyTorch Distributed APIs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Paper: &lt;a href=&quot;http://proceedings.mlr.press/v139/he21a.html&quot;&gt;http://proceedings.mlr.press/v139/he21a.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Source Code: &lt;a href=&quot;https://distml.ai&quot;&gt;https://DistML.ai&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Slides: &lt;a href=&quot;https://docs.google.com/presentation/d/1t6HWL33KIQo2as0nSHeBpXYtTBcy0nXCoLiKd0EashY/edit?usp=sharing&quot;&gt;https://docs.google.com/presentation/d/1t6HWL33KIQo2as0nSHeBpXYtTBcy0nXCoLiKd0EashY/edit?usp=sharing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/model_size.png&quot; alt=&quot;Model Size&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;
Figure 1: the Parameter Number of Transformer Models Increases Dramatically.
&lt;/p&gt;

&lt;p&gt;Large Transformer models [4][5] have powered accuracy breakthroughs in both natural language processing and computer vision. GPT-3 [4] hit a new record high accuracy for nearly all NLP tasks. Vision Transformer (ViT) [3] also achieved 89\% top-1 accuracy in ImageNet, outperforming state-of-the-art convolutional networks ResNet-152 and EfficientNet. To tackle the growth in model sizes, researchers have proposed various distributed training techniques, including parameter servers [6][7][8], pipeline parallelism [9][10][11][12], intra-layer parallelism [13][14][15], and zero redundancy data-parallel [16].&lt;/p&gt;

&lt;p&gt;Existing distributed training solutions, however, only study scenarios where all model weights are required to be optimized throughout the training (i.e., computation and communication overhead remains relatively static over different iterations). Recent works on &lt;em&gt;progressive training&lt;/em&gt; suggest that parameters in neural networks can be trained dynamically:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Freeze Training: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. NeurIPS 2017&lt;/li&gt;
  &lt;li&gt;Efficient Training of BERT by Progressively Stacking. ICML 2019&lt;/li&gt;
  &lt;li&gt;Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. NeurIPS 2020.&lt;/li&gt;
  &lt;li&gt;On the Transformer Growth for Progressive BERT Training. NACCL 2021&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/freeze_training.png&quot; alt=&quot;Freeze Training&quot; width=&quot;560&quot; /&gt;
&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;Figure 2. Interpretable Freeze Training: DNNs converge bottom-up (Results on CIFAR10 using ResNet). Each pane shows layer-by-layer similarity using SVCCA [17][18]&lt;/p&gt;

&lt;p&gt;For example, in freeze training [17][18], neural networks usually converge from the bottom-up (i.e., not all layers need to be trained all the way through training). Figure 2 shows an example of how weights gradually stabilize during training in this approach. This observation motivates us to utilize freeze training for distributed training of Transformer models to accelerate training by dynamically allocating resources to focus on a shrinking set of active layers. Such a layer freezing strategy is especially pertinent to pipeline parallelism, as excluding consecutive bottom layers from the pipeline can reduce computation, memory, and communication overhead.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/PipeTransformer.png&quot; width=&quot;100%&quot; /&gt;
&lt;br /&gt;
Figure 3. The process of PipeTransformer’s automated and elastic pipelining to accelerate distributed training of Transformer models
&lt;/p&gt;

&lt;p&gt;We propose PipeTransformer, an elastic pipelining training acceleration framework that automatically reacts to frozen layers by dynamically transforming the scope of the pipelined model and the number of pipeline replicas. To the best of our knowledge, this is the first paper that studies layer freezing in the context of both pipeline and data-parallel training. Figure 3 demonstrates the benefits of such a combination. First, by excluding frozen layers from the pipeline, the same model can be packed into fewer GPUs, leading to both fewer cross-GPU communications and smaller pipeline bubbles. Second, after packing the model into fewer GPUs, the same cluster can accommodate more pipeline replicas, increasing the width of data parallelism. More importantly, the speedups acquired from these two benefits are multiplicative rather than additive, further accelerating the training.&lt;/p&gt;

&lt;p&gt;The design of PipeTransformer faces four major challenges. First, the freeze algorithm must make on-the-fly and adaptive freezing decisions; however, existing work [17][18] only provides a posterior analysis tool. Second, the efficiency of pipeline re-partitioning results is influenced by multiple factors, including partition granularity, cross-partition activation size, and the chunking (the number of micro-batches) in mini-batches, which require reasoning and searching in a large solution space. Third, to dynamically introduce additional pipeline replicas, PipeTransformer must overcome the static nature of collective communications and avoid potentially complex cross-process messaging protocols when onboarding new processes (one pipeline is handled by one process). Finally, caching can save time for repeated forward propagation of frozen layers, but it must be shared between existing pipelines and newly added ones, as the system cannot afford to create and warm up a dedicated cache for each replica.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/PipeTransformer-Animation.gif&quot; alt=&quot;Freeze Training&quot; /&gt;
&lt;br /&gt;
Figure 4: An Animation to Show the Dynamics of PipeTransformer
&lt;/p&gt;

&lt;p&gt;As shown in the animation (Figure 4), PipeTransformer is designed with four core building blocks to address the aforementioned challenges. First, we design a tunable and adaptive algorithm to generate signals that guide the selection of layers to freeze over different iterations (Freeze Algorithm). Once triggered by these signals, our elastic pipelining module (AutoPipe), then packs the remaining active layers into fewer GPUs by taking both activation sizes and variances of workloads across heterogeneous partitions (frozen layers and active layers) into account. It then splits a mini-batch into an optimal number of micro-batches based on prior profiling results for different pipeline lengths. Our next module, AutoDP, spawns additional pipeline replicas to occupy freed-up GPUs and maintains hierarchical communication process groups to attain dynamic membership for collective communications. Our final module, AutoCache, efficiently shares activations across existing and new data-parallel processes and automatically replaces stale caches during transitions.&lt;/p&gt;

&lt;p&gt;Overall, PipeTransformer combines the Freeze Algorithm, AutoPipe, AutoDP, and AutoCache modules to provide a significant training speedup.
We evaluate PipeTransformer using Vision Transformer (ViT) on ImageNet and BERT on GLUE and SQuAD datasets. Our results show that PipeTransformer attains up to 2.83-fold speedup without losing accuracy. We also provide various performance analyses for a more comprehensive understanding of our algorithmic and system-wise design.
Finally, we have also developed open-source flexible APIs for PipeTransformer, which offer a clean separation among the freeze algorithm, model definitions, and training accelerations, allowing for transferability to other algorithms that require similar freezing strategies.&lt;/p&gt;

&lt;h1 id=&quot;overall-design&quot;&gt;Overall Design&lt;/h1&gt;

&lt;p&gt;Suppose we aim to train a massive model in a distributed training system where the hybrid of pipelined model parallelism and data parallelism is used to target scenarios where either the memory of a single GPU device cannot hold the model, or if loaded, the batch size is small enough to avoid running out of memory. More specifically, we define our settings as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training task and model definition.&lt;/strong&gt; We train Transformer models (e.g., Vision Transformer, BERT on large-scale image or text datasets. The Transformer model &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=mathcal{F}&quot; /&gt; has &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=L&quot; /&gt; layers, in which the &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=i&quot; /&gt; th layer is composed of a forward computation function &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=f_i&quot; /&gt; and a corresponding set of parameters.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training infrastructure.&lt;/strong&gt; Assume the training infrastructure contains a GPU cluster that has &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=N&quot; /&gt; GPU servers (i.e. nodes). Each node has &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=I&quot; /&gt; GPUs. Our cluster is homogeneous, meaning that each GPU and server have the same hardware configuration. Each GPU’s memory capacity is &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=M_\text{GPU}&quot; /&gt;. Servers are connected by a high bandwidth network interface such as InfiniBand interconnect.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pipeline parallelism.&lt;/strong&gt; In each machine, we load a model &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathcal{F}&quot; /&gt; into a pipeline &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathcal{P}&quot; /&gt; which has &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K&quot; /&gt;partitions (&lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K&quot; /&gt; also represents the pipeline length). The &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=k&quot; /&gt;th partition &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=p_k&quot; /&gt; consists of consecutive layers. We assume each partition is handled by a single GPU device. &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=1 \leq K \leq I&quot; /&gt;, meaning that we can build multiple pipelines for multiple model replicas in a single machine. We assume all GPU devices in a pipeline belonging to the same machine. Our pipeline is a synchronous pipeline, which does not involve stale gradients, and the number of micro-batches is &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=M&quot; /&gt;. In the Linux OS, each pipeline is handled by a single process. We refer the reader to GPipe [10] for more details.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data parallelism.&lt;/strong&gt; DDP is a cross-machine distributed data-parallel process group within &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=R&quot; /&gt; parallel workers. Each worker is a pipeline replica (a single process). The &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=r&quot; /&gt;th worker’s index (ID) is rank &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=r&quot; /&gt;. For any two pipelines in DDP, they can belong to either the same GPU server or different GPU servers, and they can exchange gradients with the AllReduce algorithm.&lt;/p&gt;

&lt;p&gt;Under these settings, our goal is to accelerate training by leveraging freeze training, which does not require all layers to be trained throughout the duration of the training. Additionally, it may help save computation, communication, memory cost, and potentially prevent overfitting by consecutively freezing layers. However, these benefits can only be achieved by overcoming the four challenges of designing an adaptive freezing algorithm, dynamical pipeline re-partitioning, efficient resource reallocation, and cross-process caching, as discussed in the introduction.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/pipetransformer_overview.png&quot; alt=&quot;Overview&quot; width=&quot;560&quot; /&gt;
&lt;br /&gt;
Figure 5. Overview of PipeTransformer Training System
&lt;/p&gt;

&lt;p&gt;PipeTransformer co-designs an on-the-fly freeze algorithm and an automated elastic pipelining training system that can dynamically transform the scope of the pipelined model and the number of pipeline replicas. The overall system architecture is illustrated in Figure 5.  To support PipeTransformer’s elastic pipelining, we maintain a customized version of PyTorch Pipeline. For data parallelism, we use PyTorch DDP as a baseline. Other libraries are standard mechanisms of an operating system (e.g.,multi-processing) and thus avoid specialized software or hardware customization requirements. To ensure the generality of our framework, we have decoupled the training system into four core components: &lt;strong&gt;freeze algorithm&lt;/strong&gt;, &lt;strong&gt;AutoPipe&lt;/strong&gt;, &lt;strong&gt;AutoDP&lt;/strong&gt;, and &lt;strong&gt;AutoCache&lt;/strong&gt;. The &lt;strong&gt;freeze algorithm&lt;/strong&gt; (grey) samples indicators from the training loop and makes layer-wise freezing decisions, which will be shared with &lt;strong&gt;AutoPipe&lt;/strong&gt; (green). AutoPipe is an elastic pipeline module that speeds up training by excluding frozen layers from the pipeline and packing the active layers into fewer GPUs (pink), leading to both fewer cross-GPU communications and smaller pipeline bubbles. Subsequently, &lt;strong&gt;AutoPipe&lt;/strong&gt; passes pipeline length information to &lt;strong&gt;AutoDP&lt;/strong&gt; (purple), which then spawns more pipeline replicas to increase data-parallel width, if possible. The illustration also includes an example in which AutoDP introduces a new replica (purple). &lt;strong&gt;AutoCache&lt;/strong&gt; (orange edges) is a cross-pipeline caching module, as illustrated by connections between pipelines. The source code architecture is aligned with Figure 5 for readability and generality.&lt;/p&gt;

&lt;h1 id=&quot;implementation-using-pytorch-apis&quot;&gt;Implementation Using PyTorch APIs&lt;/h1&gt;

&lt;p&gt;As can be seen from Figure 5, PipeTransformers contain four components: Freeze Algorithm, AutoPipe, AutoDP, and AutoCache. Among them, AutoPipe and AutoDP relies on PyTorch DDP (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;) [1] and Pipeline (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.distributed.pipeline&lt;/code&gt;), respectively. In this blog, we only highlight the key implementation details of AutoPipe and AutoDP. For details of Freeze Algorithm and AutoCache, please refer to our paper.&lt;/p&gt;

&lt;h2 id=&quot;autopipe-elastic-pipelining&quot;&gt;AutoPipe: Elastic Pipelining&lt;/h2&gt;

&lt;p&gt;AutoPipe can accelerate training by excluding frozen layers from the pipeline and packing the active layers into fewer GPUs. This section elaborates on the key components of AutoPipe that dynamically 1) partition pipelines, 2) minimize the number of pipeline devices, and 3) optimize mini-batch chunk size accordingly.&lt;/p&gt;

&lt;h3 id=&quot;basic-usage-of-pytorch-pipeline&quot;&gt;Basic Usage of PyTorch Pipeline&lt;/h3&gt;

&lt;p&gt;Before diving into details of AutoPipe, let us warm up the basic usage of PyTorch Pipeline (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.distributed.pipeline.sync.Pipe&lt;/code&gt;, see &lt;a href=&quot;https://pytorch.org/docs/stable/pipeline.html&quot;&gt;this tutorial&lt;/a&gt;). More specially, we present a simple example to understand the design of Pipeline in practice:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Step 1: build a model including two linear layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Step 2: wrap the two layers with nn.Sequential
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Step 3: build Pipe (torch.distributed.pipeline.sync.Pipe)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chunks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# do training/inference
&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output_rref&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this basic example, we can see that before initializing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pipe&lt;/code&gt;, we need to partition the model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Sequential&lt;/code&gt; into multiple GPU devices and set optimal chunk number (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chunks&lt;/code&gt;). Balancing computation time across partitions is critical to pipeline training speed, as skewed workload distributions across stages can lead to stragglers and forcing devices with lighter workloads to wait. The chunk number may also have a non-trivial influence on the throughput of the pipeline.&lt;/p&gt;

&lt;h3 id=&quot;balanced-pipeline-partitioning&quot;&gt;Balanced Pipeline Partitioning&lt;/h3&gt;

&lt;p&gt;In dynamic training system such as PipeTransformer, maintaining optimally balanced partitions in terms of parameter numbers does not guarantee the fastest training speed because other factors also play a crucial role:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/balancing_partition.png&quot; width=&quot;560&quot; /&gt;
&lt;br /&gt;
Figure 6. The partition boundary is in the middle of a skip connection
&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cross-partition communication overhead.&lt;/strong&gt; Placing a partition boundary in the middle of a skip connection leads to additional communications since tensors in the skip connection must now be copied to a different GPU. For example, with BERT partitions in Figure 6, partition &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=k&quot; /&gt; must take intermediate outputs from both partition &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=k-2&quot; /&gt; and partition &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=k-1&quot; /&gt;. In contrast, if the boundary is placed after the addition layer, the communication overhead between partition &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=k-1&quot; /&gt; and &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=k&quot; /&gt; is visibly smaller. Our measurements show that having cross-device communication is more expensive than having slightly imbalanced partitions (see the Appendix in our paper). Therefore, we do not consider breaking skip connections (highlighted separately as an entire attention layer and MLP layer in green color at line 7 in Algorithm 1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Frozen layer memory footprint.&lt;/strong&gt; During training, AutoPipe must recompute partition boundaries several times to balance two distinct types of layers: frozen layers and active layers. The frozen layer’s memory cost is a fraction of that inactive layer, given that the frozen layer does not need backward activation maps, optimizer states, and gradients. Instead of launching intrusive profilers to obtain thorough metrics on memory and computational cost, we define a tunable cost factor &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=lambda_{\text{frozen}}&quot; /&gt; to estimate the memory footprint ratio of a frozen layer over the same active layer. Based on empirical measurements in our experimental hardware, we set it to &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\frac{1}{6}&quot; /&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/AutoPipe_algorithm.png&quot; width=&quot;100%&quot; /&gt;
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;Based on the above two considerations, AutoPipe balances pipeline partitions based on parameter sizes. More specifically, AutoPipe uses a greedy algorithm to allocate all frozen and active layers to evenly distribute partitioned sublayers into &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K&quot; /&gt; GPU devices. Pseudocode is described as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load\_balance()&lt;/code&gt; function in Algorithm 1. The frozen layers are extracted from the original model and kept in a separate model instance &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathcal{F}_{\text{frozen}}&quot; /&gt; in the first device of a pipeline.&lt;/p&gt;

&lt;p&gt;Note that the partition algorithm employed in this paper is not the only option; PipeTransformer is modularized to work with any alternatives.&lt;/p&gt;

&lt;h3 id=&quot;pipeline-compression&quot;&gt;Pipeline Compression&lt;/h3&gt;

&lt;p&gt;Pipeline compression helps to free up GPUs to accommodate more pipeline replicas and reduce the number of cross-device communications between partitions. To determine the timing of compression, we can estimate the memory cost of the largest partition after compression, and then compare it with that of the largest partition of a pipeline at timestep &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=T=0&quot; /&gt;. To avoid extensive memory profiling, the compression algorithm uses the parameter size as a proxy for the training memory footprint. Based on this simplification, the criterion of pipeline compression is as follows:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/memory_reduction.png&quot; width=&quot;320&quot; /&gt;
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;Once the freeze notification is received, AutoPipe will always attempt to divide the pipeline length &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K&quot; /&gt; by 2 (e.g., from 8 to 4, then 2). By using &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\frac{K}{2}&quot; /&gt; as the input, the compression algorithm can verify if the result satisfies the criterion in Equation (1). Pseudocode is shown in lines 25-33 in Algorithm 1. Note that this compression makes the acceleration ratio exponentially increase during training, meaning that if a GPU server has a larger number of GPUs (e.g., more than 8), the acceleration ratio will be further amplified.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/pipe_buble.png&quot; width=&quot;560&quot; /&gt;
&lt;br /&gt;
Figure 7. Pipeline Bubble: &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=F_{d,b}&quot; /&gt;, and &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=U_d&quot; /&gt; denote forward, backward, and the optimizer update of micro-batch &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=b&quot; /&gt; on device &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=d&quot; /&gt;, respectively. The total bubble size in each iteration is &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K-1&quot; /&gt; times per micro-batch forward and backward cost.
&lt;/p&gt;

&lt;p&gt;Additionally, such a technique can also speed up training by shrinking the size of pipeline bubbles. To explain bubble sizes in a pipeline, Figure 7 depicts how 4 micro-batches run through a 4-device pipeline &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K = 4&quot; /&gt;. In general, the total bubble size is &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=(K-1)&quot; /&gt; times per micro-batch forward and backward cost. Therefore, it is clear that shorter pipelines have smaller bubble sizes.&lt;/p&gt;

&lt;h3 id=&quot;dynamic-number-of-micro-batches&quot;&gt;Dynamic Number of Micro-Batches&lt;/h3&gt;

&lt;p&gt;Prior pipeline parallel systems use a fixed number of micro-batches per mini-batch (&lt;img src=&quot;https://render.githubusercontent.com/render/math?math=M&quot; /&gt; ). GPipe suggests &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=M \geq 4 \times K&quot; /&gt;, where &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K&quot; /&gt; is the number of partitions (pipeline length). However, given that PipeTransformer dynamically configures &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K&quot; /&gt;, we find it to be sub-optimal to maintain a static &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=M&quot; /&gt; during training. Moreover, when integrated with DDP, the value of &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=M&quot; /&gt; also has an impact on the efficiency of DDP gradient synchronizations. Since DDP must wait for the last micro-batch to finish its backward computation on a parameter before launching its gradient synchronization, finer micro-batches lead to a smaller overlap between computation and communication. Hence, instead of using a static value, PipeTransformer searches for optimal &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=M&quot; /&gt; on the fly in the hybrid of DDP environment by enumerating &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=M&quot; /&gt; values ranging from &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K&quot; /&gt; to &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=6K&quot; /&gt;. For a specific training environment, the profiling needs only to be done once (see Algorithm 1 line 35).&lt;/p&gt;

&lt;p&gt;For the complete source code, please refer to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/Distributed-AI/PipeTransformer/blob/master/pipe_transformer/pipe/auto_pipe.py&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;autodp-spawning-more-pipeline-replicas&quot;&gt;AutoDP: Spawning More Pipeline Replicas&lt;/h2&gt;
&lt;p&gt;As AutoPipe compresses the same pipeline into fewer GPUs, AutoDP can automatically spawn new pipeline replicas to increase data-parallel width.&lt;/p&gt;

&lt;p&gt;Despite the conceptual simplicity, subtle dependencies on communications and states require careful design. The challenges are threefold:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;DDP Communication&lt;/strong&gt;: Collective communications in PyTorch DDP requires static membership, which prevents new pipelines from connecting with existing ones;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;State Synchronization&lt;/strong&gt;: newly activated processes must be consistent with existing pipelines in the training progress (e.g., epoch number and learning rate), weights and optimizer states, the boundary of frozen layers, and pipeline GPU range;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dataset Redistribution&lt;/strong&gt;: the dataset should be re-balanced to match a dynamic number of pipelines. This not only avoids stragglers but also ensures that gradients from all DDP processes are equally weighted.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/AutoDP.png&quot; width=&quot;560&quot; /&gt;
&lt;br /&gt;
Figure 8. AutoDP: handling dynamical data-parallel with messaging between double process groups (Process 0-7 belong to machine 0, while process 8-15 belong to machine 1)
&lt;/p&gt;

&lt;p&gt;To tackle these challenges, we create double communication process groups for DDP. As in the example shown in Figure 8, the message process group (purple) is responsible for light-weight control messages and covers all processes, while the active training process group (yellow) only contains active processes and serves as a vehicle for heavy-weight tensor communications during training. The message group remains static, whereas the training group is dismantled and reconstructed to match active processes.
In T0, only processes 0 and 8 are active. During the transition to T1, process 0 activates processes 1 and 9 (newly added pipeline replicas) and synchronizes necessary information mentioned above using the message group. The four active processes then form a new training group, allowing static collective communications adaptive to dynamic memberships.
To redistribute the dataset, we implement a variant of DistributedSampler that can seamlessly adjust data samples to match the number of active pipeline replicas.&lt;/p&gt;

&lt;p&gt;The above design also naturally helps to reduce DDP communication overhead. More specifically, when transitioning from T0 to T1, processes 0 and 1 destroy the existing DDP instances, and active processes construct a new DDP training group using a cached pipelined model (AutoPipe stores frozen model and cached model separately).&lt;/p&gt;

&lt;p&gt;We use the following APIs to implement the design above.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributed&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.parallel&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DistributedDataParallel&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DDP&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# initialize the process group (this must be called in the initialization of PyTorch DDP)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_process_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tcp://'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;master_addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;':'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;master_port&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Backend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GLOO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# create active process group (yellow color)
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active_process_group&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ranks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active_ranks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Backend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NCCL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;days&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;365&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# create message process group (yellow color)
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comm_broadcast_group&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ranks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Backend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GLOO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;days&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;365&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# create DDP-enabled model when the number of data-parallel workers is changed. Note:
# 1. The process group to be used for distributed data all-reduction.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;If&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;which&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;created&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_process_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;will&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;be&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;our&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;case&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;we&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active_process_group&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 2. device_ids should be set when the pipeline length = 1 (the model resides on a single CUDA device).
&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipe_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gpu_num_per_process&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gpu_num_per_process&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DDP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process_group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active_process_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;find_unused_parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DDP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process_group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active_process_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;find_unused_parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# to broadcast message among processes, we use dist.broadcast_object_list
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dist_broadcast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;object_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Broadcasts a given object to all parties.&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;broadcast_object_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;object_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;object_list&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For the complete source code, please refer to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/Distributed-AI/PipeTransformer/blob/master/pipe_transformer/dp/auto_dp.py&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;This section first summarizes experiment setups and then evaluates PipeTransformer using computer vision and natural language processing tasks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hardware.&lt;/strong&gt; Experiments were conducted on 2 identical machines connected by InfiniBand CX353A (&lt;img src=&quot;https://render.githubusercontent.com/render/math?math=5&quot; /&gt;GB/s), where each machine is equipped with 8 NVIDIA Quadro RTX 5000 (16GB GPU memory). GPU-to-GPU bandwidth within a machine (PCI 3.0, 16 lanes) is &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=15.754&quot; /&gt;GB/s.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Implementation.&lt;/strong&gt; We used PyTorch Pipe as a building block. The BERT model definition, configuration, and related tokenizer are from HuggingFace 3.5.0. We implemented Vision Transformer using PyTorch by following its TensorFlow implementation. More details can be found in our source code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Models and Datasets.&lt;/strong&gt; Experiments employ two representative Transformers in CV and NLP: Vision Transformer (ViT) and BERT. ViT was run on an image classification task, initialized with pre-trained weights on ImageNet21K and fine-tuned on ImageNet and CIFAR-100. BERT was run on two tasks, text classification on the SST-2 dataset from the General Language Understanding Evaluation (GLUE) benchmark, and question answering on the SQuAD v1.1 Dataset (Stanford Question Answering), which is a collection of 100k crowdsourced question/answer pairs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training Schemes.&lt;/strong&gt; Given that large models normally would require thousands of GPU-days {\emph{e.g.}, GPT-3) if trained from scratch, fine-tuning downstream tasks using pre-trained models has become a trend in CV and NLP communities. Moreover, PipeTransformer is a complex training system that involves multiple core components. Thus, for the first version of PipeTransformer system development and algorithmic research, it is not cost-efficient to develop and evaluate from scratch using large-scale pre-training. Therefore, the experiments presented in this section focuses on pre-trained models. Note that since the model architectures in pre-training and fine-tuning are the same, PipeTransformer can serve both. We discussed pre-training results in the Appendix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Baseline.&lt;/strong&gt; Experiments in this section compare PipeTransformer to the state-of-the-art framework, a hybrid scheme of PyTorch Pipeline (PyTorch’s implementation of GPipe) and PyTorch DDP. Since this is the first paper that studies accelerating distributed training by freezing layers, there are no perfectly aligned counterpart solutions yet.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hyper-parameters.&lt;/strong&gt; Experiments use ViT-B/16 (12 transformer layers, &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=16 \times 16&quot; /&gt; input patch size) for ImageNet and CIFAR-100, BERT-large-uncased (24 layers) for SQuAD 1.1, and BERT-base-uncased (12 layers) for SST-2. With PipeTransformer, ViT and BERT training can set the per-pipeline batch size to around 400 and 64, respectively. Other hyperparameters (e.g., epoch, learning rate) for all experiments are presented in Appendix.&lt;/p&gt;

&lt;h2 id=&quot;overall-training-acceleration&quot;&gt;Overall Training Acceleration&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/experiments_table1.png&quot; width=&quot;560&quot; /&gt;
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;We summarize the overall experimental results in the table above. Note that the speedup we report is based on a conservative &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\alpha&quot; /&gt; &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\frac{1}{3}&quot; /&gt; value that can obtain comparable or even higher accuracy. A more aggressive &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\alpha&quot; /&gt; (&lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\frac{2}{5}&quot; /&gt;, &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\frac{1}{2}&quot; /&gt;) can obtain a higher speedup but may lead to a slight loss in accuracy. Note that the model size of BERT (24 layers) is larger than ViT-B/16 (12 layers), thus it takes more time for communication.&lt;/p&gt;

&lt;h2 id=&quot;performance-analysis&quot;&gt;Performance Analysis&lt;/h2&gt;

&lt;h3 id=&quot;speedup-breakdown&quot;&gt;Speedup Breakdown&lt;/h3&gt;

&lt;p&gt;This section presents evaluation results and analyzes the performance of different components in \autopipe. More experimental results can be found in the Appendix.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/experiments_throughput.png&quot; width=&quot;560&quot; /&gt;
&lt;br /&gt;
Figure 9. Speedup Breakdown (ViT on ImageNet)
&lt;/p&gt;

&lt;p&gt;To understand the efficacy of all four components and their impacts on training speed, we experimented with different combinations and used their training sample throughput (samples/second) and speedup ratio as metrics. Results are illustrated in Figure 9. Key takeaways from these experimental results are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the main speedup is the result of elastic pipelining which is achieved through the joint use of AutoPipe and AutoDP;&lt;/li&gt;
  &lt;li&gt;AutoCache’s contribution is amplified by AutoDP;&lt;/li&gt;
  &lt;li&gt;freeze training alone without system-wise adjustment even downgrades the training speed.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;tuning--in-freezing-algorithm&quot;&gt;Tuning &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\alpha&quot; /&gt; in Freezing Algorithm&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/experiments_tuning_alpha.png&quot; width=&quot;460&quot; /&gt;
&lt;br /&gt;
Figure 10. Tuning &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\alpha&quot; /&gt; in Freezing Algorithm
&lt;/p&gt;

&lt;p&gt;We ran experiments to show how the &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\alpha&quot; /&gt;  in the freeze algorithms influences training speed. The result clearly demonstrates that a larger &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\alpha&quot; /&gt; (excessive freeze) leads to a greater speedup but suffers from a slight performance degradation. In the case shown in Figure 10, where &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\alpha=1/5&quot; /&gt;, freeze training outperforms normal training and obtains a &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=2.04&quot; /&gt;-fold speedup. We provide more results in the Appendix.&lt;/p&gt;

&lt;h3 id=&quot;optimal-chunks-in-the-elastic-pipeline&quot;&gt;Optimal Chunks in the elastic pipeline&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/experiments_optimal_k.png&quot; width=&quot;450&quot; /&gt;
&lt;br /&gt;
Figure 11. Optimal chunk number in the elastic pipeline
&lt;/p&gt;

&lt;p&gt;We profiled the optimal number of micro-batches &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=M&quot; /&gt; for different pipeline lengths &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K&quot; /&gt;. Results are summarized in Figure 11. As we can see, different &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K&quot; /&gt; values lead to different optimal &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=M&quot; /&gt;, and the throughput gaps across different M values are large (as shown when &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=K=8&quot; /&gt;), which confirms the necessity of an anterior profiler in elastic pipelining.&lt;/p&gt;

&lt;h3 id=&quot;understanding-the-timing-of-caching&quot;&gt;Understanding the Timing of Caching&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/experiment_autocache.png&quot; width=&quot;320&quot; /&gt;
&lt;br /&gt;
Figure 12. the timing of caching
&lt;/p&gt;

&lt;p&gt;To evaluate AutoCache, we compared the sample throughput of training that activates AutoCache from epoch &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=0&quot; /&gt; (blue) with the training job without AutoCache (red). Figure 12 shows that enabling caching too early can slow down training, as caching can be more expensive than the forward propagation on a small number of frozen layers. After more layers are frozen, caching activations clearly outperform the corresponding forward propagation. As a result, AutoCache uses a profiler to determine the proper timing to enable caching. In our system, for ViT (12 layers), caching starts from 3 frozen layers, while for BERT (24 layers), caching starts from 5 frozen layers.&lt;/p&gt;

&lt;p&gt;For more detailed experimental analysis, please refer to our paper.&lt;/p&gt;

&lt;h1 id=&quot;summarization&quot;&gt;Summarization&lt;/h1&gt;
&lt;p&gt;This blog introduces PipeTransformer, a holistic solution that combines elastic pipeline-parallel and data-parallel for distributed training using PyTorch Distributed APIs. More specifically, PipeTransformer incrementally freezes layers in the pipeline, packs remaining active layers into fewer GPUs, and forks more pipeline replicas to increase the data-parallel width. Evaluations on ViT and BERT models show that compared to the state-of-the-art baseline, PipeTransformer attains up to 2.83× speedups without accuracy loss.&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;p&gt;[1] Li, S., Zhao, Y., Varma, R., Salpekar, O., Noordhuis, P., Li,T., Paszke, A., Smith, J., Vaughan, B., Damania, P., et al. Pytorch Distributed:  Experiences on Accelerating Dataparallel Training. Proceedings of the VLDB Endowment,13(12), 2020&lt;/p&gt;

&lt;p&gt;[2] Devlin, J., Chang, M. W., Lee, K., and Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT, 2019&lt;/p&gt;

&lt;p&gt;[3] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is Worth 16x16 words: Transformers for Image Recognition at Scale.&lt;/p&gt;

&lt;p&gt;[4] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language Models are Few-shot Learners.&lt;/p&gt;

&lt;p&gt;[5] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling Giant Models with Conditional Computation and Automatic Sharding.&lt;/p&gt;

&lt;p&gt;[6] Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., Long, J., Shekita, E. J., and Su, B. Y. Scaling Distributed Machine Learning with the Parameter Server. In 11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14), pp. 583–598, 2014.&lt;/p&gt;

&lt;p&gt;[7] Jiang, Y., Zhu, Y., Lan, C., Yi, B., Cui, Y., and Guo, C. A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pp. 463–479. USENIX Association, November 2020. ISBN 978-1-939133-19- 9.&lt;/p&gt;

&lt;p&gt;[8] Kim, S., Yu, G. I., Park, H., Cho, S., Jeong, E., Ha, H., Lee, S., Jeong, J. S., and Chun, B. G. Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks. In Proceedings of the Fourteenth EuroSys Conference 2019, pp. 1–15, 2019.&lt;/p&gt;

&lt;p&gt;[9] Kim, C., Lee, H., Jeong, M., Baek, W., Yoon, B., Kim, I., Lim, S., and Kim, S. TorchGPipe: On-the-fly Pipeline Parallelism for Training Giant Models.&lt;/p&gt;

&lt;p&gt;[10] Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, M. X., Chen, D., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism.&lt;/p&gt;

&lt;p&gt;[11] Park, J. H., Yun, G., Yi, C. M., Nguyen, N. T., Lee, S., Choi, J., Noh, S. H., and ri Choi, Y. Hetpipe: Enabling Large DNN Training on (whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism. In 2020 USENIX Annual Technical Conference (USENIX ATC 20), pp. 307–321. USENIX Association, July 2020. ISBN 978-1-939133- 14-4.&lt;/p&gt;

&lt;p&gt;[12] Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N. R., Ganger, G. R., Gibbons, P. B., and Zaharia, M. Pipedream: Generalized Pipeline Parallelism for DNN Training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP ’19, pp. 1–15, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450368735. doi: 10.1145/3341301.3359646.&lt;/p&gt;

&lt;p&gt;[13] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling Giant Models with Conditional Computation and Automatic Sharding.&lt;/p&gt;

&lt;p&gt;[14] Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H., Hong, M., Young, C., Sepassi, R., and Hechtman, B. Mesh-Tensorflow: Deep Learning for Supercomputers. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31, pp. 10414–10423. Curran Associates, Inc., 2018.&lt;/p&gt;

&lt;p&gt;[15] Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-LM: Training Multi-billion Parameter Language Models using Model Parallelism.&lt;/p&gt;

&lt;p&gt;[16] Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. ZERO: Memory Optimization towards Training a Trillion Parameter Models.&lt;/p&gt;

&lt;p&gt;[17] Raghu, M., Gilmer, J., Yosinski, J., and Sohl Dickstein, J. Svcca: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. In NIPS, 2017.&lt;/p&gt;

&lt;p&gt;[18] Morcos, A., Raghu, M., and Bengio, S. Insights on Representational Similarity in Neural Networks with Canonical Correlation. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31, pp. 5732–5741. Curran Associates, Inc., 2018.&lt;/p&gt;</content><author><name>Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr</name></author><summary type="html">In this blog post, we describe the first peer-reviewed research paper that explores accelerating the hybrid of PyTorch DDP (torch.nn.parallel.DistributedDataParallel) [1] and Pipeline (torch.distributed.pipeline) - PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models (Transformers such as BERT [2] and ViT [3]), published at ICML 2021.</summary></entry><entry><title type="html">What’s New in PyTorch Profiler 1.9?</title><link href="https://pytorch.org/blog/pytorch-profiler-1.9-released/" rel="alternate" type="text/html" title="What’s New in PyTorch Profiler 1.9?" /><published>2021-08-03T00:00:00-07:00</published><updated>2021-08-03T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-profiler-1.9-released</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-profiler-1.9-released/">&lt;p&gt;PyTorch Profiler v1.9 has been released! The goal of this new release (previous &lt;a href=&quot;https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/&quot;&gt;PyTorch Profiler release&lt;/a&gt;) is to provide you with new state-of-the-art tools to help diagnose and fix machine learning performance issues regardless of whether you are working on one or numerous machines. The objective is to target the execution steps that are the most costly in time and/or memory, and visualize the work load distribution between GPUs and CPUs.&lt;/p&gt;

&lt;p&gt;Here is a summary of the five major features being released:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Distributed Training View&lt;/strong&gt;: This helps you understand how much time and memory is consumed in your distributed training job. Many issues occur when you take a training model and split the load into worker nodes to be run in parallel as it can be a black box. The overall model goal is to speed up model training. This distributed training view will help you diagnose and debug issues within individual nodes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Memory View&lt;/strong&gt;: This view allows you to understand your memory usage better. This tool will help you avoid the famously pesky Out of Memory error by showing active memory allocations at various points of your program run.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GPU Utilization Visualization&lt;/strong&gt;: This tool helps you make sure that your GPU is being fully utilized.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cloud Storage Support&lt;/strong&gt;: Tensorboard plugin can now read profiling data from Azure Blob Storage, Amazon S3, and Google Cloud Platform.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Jump to Source Code&lt;/strong&gt;: This feature allows you to visualize stack tracing information and jump directly into the source code. This helps you quickly optimize and iterate on your code based on your profiling results.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;getting-started-with-pytorch-profiling-tool&quot;&gt;Getting Started with PyTorch Profiling Tool&lt;/h2&gt;
&lt;p&gt;PyTorch includes a profiling functionality called « PyTorch Profiler ». The PyTorch Profiler tutorial can be found &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To instrument your PyTorch code for profiling, you must:&lt;/p&gt;

&lt;p&gt;$ pip install torch-tb-profiler&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.profiler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;With&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XXXX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Comments&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;• For CUDA and CPU profiling, see &lt;a href=&quot;https://github.com/pytorch/kineto/blob/master/tb_plugin/examples/resnet50_profiler_api.py&quot;&gt;below&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.profiler.profile( 
activities=[ 
torch.profiler.ProfilerActivity.CPU, 
torch.profiler.ProfilerActivity.CUDA], 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;•	With profiler.record_function(“$NAME”): allows putting a decorator (a tag associated to a name) for a block of function&lt;/p&gt;

&lt;p&gt;•	Profile_memory=True parameter under profiler.profile allows you to profile CPU and GPU memory footprint&lt;/p&gt;

&lt;h2 id=&quot;visualizing-pytorch-model-performance-using-pytorch-profiler&quot;&gt;Visualizing PyTorch Model Performance using PyTorch Profiler&lt;/h2&gt;

&lt;h3 id=&quot;distributed-training&quot;&gt;Distributed Training&lt;/h3&gt;

&lt;p&gt;Recent advances in deep learning argue for the value of large datasets and large models, which requires you to scale out model training to more computational resources. Distributed Data Parallel (DDP) and NVIDIA Collective Communications Library (NCCL) are the widely adopted paradigms in PyTorch for accelerating your deep learning training.&lt;/p&gt;

&lt;p&gt;In this release of PyTorch Profiler, DDP with NCCL backend is now supported.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;computationcommunication-overview&quot;&gt;Computation/Communication Overview&lt;/h3&gt;

&lt;p&gt;In the Computation/Communication overview under the Distributed training view, you can observe the computation-to-communication ratio of each worker and [load balancer](https://en.wikipedia.org/wiki/Load_balancing_(computing) nodes between worker as measured by granularity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 1&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;If the computation and overlapping time of one worker is much larger than the others, this may suggest an issue in the workload balance or worker being a straggler. Computation is the sum of kernel time on GPU minus the overlapping time. The overlapping time is the time saved by interleaving communications during computation. The more overlapping time represents better parallelism between computation and communication. Ideally the computation and communication completely overlap with each other. Communication is the total communication time minus the overlapping time. The example image below displays how this scenario appears on Tensorboard.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image2.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure: A straggler example&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Scenario 2&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;If there is a small batch size (i.e. less computation on each worker) or the data to be transferred is large, the computation-to-communication may also be small and be seen in the profiler with low GPU utilization and long waiting times. This computation/communication view will allow you to diagnose your code to reduce communication by adopting gradient accumulation, or to decrease the communication proportion by increasing batch size. DDP communication time depends on model size. Batch size has no relationship with model size. So increasing batch size could make computation time longer and make computation-to-communication ratio bigger.&lt;/p&gt;

&lt;h3 id=&quot;synchronizingcommunication-overview&quot;&gt;Synchronizing/Communication Overview&lt;/h3&gt;

&lt;p&gt;In the Synchronizing/Communication view, you can observe the efficiency of communication. This is done by taking the step time minus computation and communication time. Synchronizing time is part of the total communication time for waiting and synchronizing with other workers. The Synchronizing/Communication view includes initialization, data loader, CPU computation, and so on Insights like what is the ratio of total communication is really used for exchanging data and what is the idle time of waiting for data from other workers can be drawn from this view.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image4.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;For example, if there is an inefficient workload balance or straggler issue, you’ll be able to identify it in this Synchronizing/Communication view. This view will show several workers’ waiting time being longer than others.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image5.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;This table view above allows you to see the detailed statistics of all communication ops in each node. This allows you to see what operation types are being called, how many times each op is called, what is the size of the data being transferred by each op, etc.&lt;/p&gt;

&lt;h3 id=&quot;memory-view&quot;&gt;Memory View:&lt;/h3&gt;

&lt;p&gt;This memory view tool helps you understand the hardware resource consumption of the operators in your model. Understanding the time and memory consumption on the operator-level allows you to resolve performance bottlenecks and in turn, allow your model to execute faster. Given limited GPU memory size, optimizing the memory usage can:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Allow bigger model which can potentially generalize better on end level tasks.&lt;/li&gt;
  &lt;li&gt;Allow bigger batch size. Bigger batch sizes increase the training speed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The profiler records all the memory allocation during the profiler interval. Selecting the “Device” will allow you to see each operator’s memory usage on the GPU side or host side. You must enable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;profile_memory=True&lt;/code&gt; to generate the below memory data as shown &lt;a href=&quot;https://github.com/pytorch/kineto/blob/master/tb_plugin/examples/resnet50_profiler_api.py#L39&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;With torch.profiler.profile(
Profiler_memory=True # this will take 1 – 2 minutes to complete. 
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Important Definitions&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;•	“Size Increase” displays the sum of all allocation bytes and minus all the memory release bytes.&lt;/p&gt;

&lt;p&gt;•	“Allocation Size” shows the sum of all allocation bytes without considering the memory release.&lt;/p&gt;

&lt;p&gt;•	“Self” means the allocated memory is not from any child operators, instead by the operator itself.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image6.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;gpu-metric-on-timeline&quot;&gt;GPU Metric on Timeline:&lt;/h3&gt;

&lt;p&gt;This feature will help you debug performance issues when one or more GPU are underutilized. Ideally, your program should have high GPU utilization (aiming for 100% GPU utilization), minimal CPU to GPU communication, and no overhead.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;:
The overview page highlights the results of three important GPU usage metrics at different levels (i.e. GPU Utilization, Est. SM Efficiency, and Est. Achieved Occupancy). Essentially, each GPU has a bunch of SM each with a bunch of warps that can execute a bunch of threads concurrently. Warps execute a bunch because the amount depends on the GPU. But at a high level, this GPU Metric on Timeline tool allows you can see the whole stack, which is useful.&lt;/p&gt;

&lt;p&gt;If the GPU utilization result is low, this suggests a potential bottleneck is present in your model. Common reasons:&lt;/p&gt;

&lt;p&gt;•Insufficient parallelism in kernels (i.e., low batch size)&lt;/p&gt;

&lt;p&gt;•Small kernels called in a loop. This is to say the launch overheads are not amortized&lt;/p&gt;

&lt;p&gt;•CPU or I/O bottlenecks lead to the GPU not receiving enough work to keep busy&lt;/p&gt;

&lt;p&gt;Looking of the overview page where the performance recommendation section is where you’ll find potential suggestions on how to increase that GPU utilization. In this example, GPU utilization is low so the performance recommendation was to increase batch size. Increasing batch size 4 to 32, as per the performance recommendation, increased the GPU Utilization by 60.68%.&lt;/p&gt;

&lt;p&gt;GPU Utilization: the step interval time in the profiler when a GPU engine was executing a workload. The high the utilization %, the better. The drawback of using GPU utilization solely to diagnose performance bottlenecks is it is too high-level and coarse. It won’t be able to tell you how many Streaming Multiprocessors are in use. Note that while this metric is useful for detecting periods of idleness, a high value does not indicate efficient use of the GPU, only that it is doing anything at all. For instance, a kernel with a single thread running continuously will get a GPU Utilization of 100%&lt;/p&gt;

&lt;p&gt;Estimated Stream Multiprocessor Efficiency (Est. SM Efficiency) is a finer grained metric, it indicates what percentage of SMs are in use at any point in the trace This metric reports the percentage of time where there is at least one active warp on a SM and those that are stalled (NVIDIA &lt;a href=&quot;https://forums.developer.nvidia.com/t/nvprof-question-about-the-sm-efficiency-metric/72640#:~:text=My%20understanding%20from%20the%20profiler%20documentation%20is%20that,that%20%E2%80%9Cactive%20warps%E2%80%9D%20include%20warps%20that%20are%20stalled.&quot;&gt;doc&lt;/a&gt;). Est. SM Efficiency also has it’s limitation. For instance, a kernel with only one thread per block can’t fully use each SM. SM Efficiency does not tell us how busy each SM is, only that they are doing anything at all, which can include stalling while waiting on the result of a memory load. To keep an SM busy, it is necessary to have a sufficient number of ready warps that can be run whenever a stall occurs&lt;/p&gt;

&lt;p&gt;Estimated Achieved Occupancy (Est. Achieved Occupancy) is a layer deeper than Est. SM Efficiency and GPU Utilization for diagnosing performance issues. Estimated Achieved Occupancy indicates how many warps can be active at once per SMs. Having a sufficient number of active warps is usually key to achieving good throughput. Unlike GPU Utilization and SM Efficiency, it is not a goal to make this value as high as possible. As a rule of thumb, good throughput gains can be had by improving this metric to 15% and above. But at some point you will hit diminishing returns. If the value is already at 30% for example, further gains will be uncertain. This metric reports the average values of all warp schedulers for the kernel execution period (NVIDIA &lt;a href=&quot;https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/achievedoccupancy.htm&quot;&gt;doc&lt;/a&gt;). The larger the Est. Achieve Occupancy value is the better.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image7.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Overview details: Resnet50_batchsize4&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image8.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Overview details: Resnet50_batchsize32&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Kernel View&lt;/em&gt;
The kernel has “Blocks per SM” and “Est. Achieved Occupancy” which is a great tool to compare model runs.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image9.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Mean Blocks per SM:&lt;br /&gt;
Blocks per SM = Blocks of this kernel / SM number of this GPU. If this number is less than 1, it indicates the GPU multiprocessors are not fully utilized. “Mean Blocks per SM” is weighted average of all runs of this kernel name, using each run’s duration as weight.&lt;/p&gt;

&lt;p&gt;Mean Est. Achieved Occupancy:&lt;br /&gt;
Est. Achieved Occupancy is defined as above in overview. “Mean Est. Achieved Occupancy” is weighted average of all runs of this kernel name, using each run’s duration as weight.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Trace View&lt;/em&gt;
This trace view displays a timeline that shows the duration of operators in your model and which system executed the operation. This view can help you identify whether the high consumption and long execution is because of input or model training. Currently, this trace view shows GPU Utilization and Est. SM Efficiency on a timeline.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image10.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;GPU utilization is calculated independently and divided into multiple 10 millisecond buckets. The buckets’ GPU utilization values are drawn alongside the timeline between 0 – 100%. In the above example, the “ProfilerStep5” GPU utilization during thread 28022’s busy time is higher than the following the one during “Optimizer.step”. This is where you can zoom-in to investigate why that is.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image11.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;From above, we can see the former’s kernels are longer than the later’s kernels. The later’s kernels are too short in execution, which results in lower GPU utilization.&lt;/p&gt;

&lt;p&gt;Est. SM Efficiency: Each kernel has a calculated est. SM efficiency between 0 – 100%. For example, the below kernel has only 64 blocks, while the SMs in this GPU is 80. Then its “Est. SM Efficiency” is 64/80, which is 0.8.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image12.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cloud-storage-support&quot;&gt;Cloud Storage Support&lt;/h3&gt;

&lt;p&gt;After running pip install tensorboard, to have data be read through these cloud providers, you can now run:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch-tb-profiler[blob] 
torch-tb-profiler[gs] 
torch-tb-profiler[s3] 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install torch-tb-profiler[blob]&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install torch-tb-profiler[gs]&lt;/code&gt;, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install torch-tb-profiler[S3]&lt;/code&gt; to have data be read through these cloud providers. For more information, please refer to this &lt;a href=&quot;https://github.com/pytorch/kineto/tree/main/tb_plugin&quot;&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;jump-to-source-code&quot;&gt;Jump to Source Code:&lt;/h3&gt;

&lt;p&gt;One of the great benefits of having both TensorBoard and the PyTorch Profiler being integrated directly in Visual Studio Code (VS Code) is the ability to directly jump to the source code (file and line) from the profiler stack traces. VS Code Python Extension now &lt;a href=&quot;https://devblogs.microsoft.com/python/python-in-visual-studio-code-february-2021-release/&quot;&gt;supports TensorBoard Integration&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jump to source is ONLY available when Tensorboard is launched within VS Code. Stack tracing will appear on the plugin UI if the profiling with_stack=True. When you click on a stack trace from the PyTorch Profiler, VS Code will automatically open the corresponding file side by side and jump directly to the line of code of interest for you to debug. This allows you to quickly make actionable optimizations and changes to your code based on the profiling results and suggestions.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/profiler_1.9_image13.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Gify: Jump to Source using Visual Studio Code Plug In UI &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;For how to optimize batch size performance, check out the step-by-step tutorial &lt;a href=&quot;https://opendatascience.com/optimizing-pytorch-performance-batch-size-with-pytorch-profiler/&quot;&gt;here&lt;/a&gt;. PyTorch Profiler is also integrated with &lt;a href=&quot;https://pytorch-lightning.readthedocs.io/en/stable/advanced/profiler.html#pytorch-profiling&quot;&gt;PyTorch Lightning&lt;/a&gt; and you can simply launch your lightning training jobs with –&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trainer.profiler=pytorch&lt;/code&gt; flag to generate the traces. Check out an example &lt;a href=&quot;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/basic_examples/profiler_example.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;whats-next-for-the-pytorch-profiler&quot;&gt;What’s Next for the PyTorch Profiler?&lt;/h2&gt;
&lt;p&gt;You just saw how PyTorch Profiler can help optimize a model. You can now try the Profiler by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install torch-tb-profiler&lt;/code&gt; to optimize your PyTorch model.&lt;/p&gt;

&lt;p&gt;Look out for an advanced version of this tutorial in the future. If you want tailored enterprise-grade support for this, check out &lt;a href=&quot;https://azure.microsoft.com/en-us/develop/pytorch/&quot;&gt;PyTorch Enterprise on Azure&lt;/a&gt;. We are also thrilled to continue to bring state-of-the-art tool to PyTorch users to improve ML performance. We’d love to hear from you. Feel free to open an issue &lt;a href=&quot;https://github.com/pytorch/kineto/issues&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For new and exciting features coming up with PyTorch Profiler, follow @PyTorch on Twitter and check us out on pytorch.org.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;The author would like to thank the contributions of the following individuals to this piece. From the Facebook side: Geeta Chauhan, Gisle Dankel, Woo Kim, Sam Farahzad, and Mark Saroufim. On the Microsoft side: AI Framework engineers (Teng Gao, Mike Guo, and Yang Gu), Guoliang Hua, and Thuy Nguyen.&lt;/p&gt;</content><author><name>Sabrina Smai, Program Manager on the AI Framework team at Microsoft</name></author><summary type="html">PyTorch Profiler v1.9 has been released! The goal of this new release (previous PyTorch Profiler release) is to provide you with new state-of-the-art tools to help diagnose and fix machine learning performance issues regardless of whether you are working on one or numerous machines. The objective is to target the execution steps that are the most costly in time and/or memory, and visualize the work load distribution between GPUs and CPUs.</summary></entry><entry><title type="html">Everything You Need To Know About Torchvision’s SSDlite Implementation</title><link href="https://pytorch.org/blog/torchvision-ssdlite-implementation/" rel="alternate" type="text/html" title="Everything You Need To Know About Torchvision’s SSDlite Implementation" /><published>2021-06-27T00:00:00-07:00</published><updated>2021-06-27T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchvision-ssdlite-implementation</id><content type="html" xml:base="https://pytorch.org/blog/torchvision-ssdlite-implementation/">&lt;p&gt;In the &lt;a href=&quot;https://pytorch.org/blog/torchvision-ssd-implementation/&quot;&gt;previous article&lt;/a&gt;, we’ve discussed how the SSD algorithm works, covered its implementation details and presented its training process. If you have not read the previous blog post, I encourage you to check it out before continuing.&lt;/p&gt;

&lt;p&gt;In this part 2 of the series, we will focus on the mobile-friendly variant of SSD called SSDlite. Our plan is to first go through the main components of the algorithm highlighting the parts that differ from the original SSD, then discuss how the released model was trained and finally provide detailed benchmarks for all the new Object Detection models that we explored.&lt;/p&gt;

&lt;h1 id=&quot;the-ssdlite-network-architecture&quot;&gt;The SSDlite Network Architecture&lt;/h1&gt;

&lt;p&gt;The SSDlite is an adaptation of SSD which was first briefly introduced on the &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;MobileNetV2 paper&lt;/a&gt; and later reused on the &lt;a href=&quot;https://arxiv.org/abs/1905.02244&quot;&gt;MobileNetV3 paper&lt;/a&gt;. Because the main focus of the two papers was to introduce novel CNN architectures, most of the implementation details of SSDlite were not clarified. Our code follows all the details presented on the two papers and where necessary fills the gaps from the &lt;a href=&quot;https://github.com/tensorflow/models/tree/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection&quot;&gt;official implementation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As noted before, the SSD is a family of models because one can configure it with different backbones (such as VGG, MobileNetV3 etc) and different Heads (such as using regular convolutions, separable convolutions etc). Thus many of the SSD components remain the same in SSDlite. Below we discuss only those that are different&lt;/p&gt;

&lt;h2 id=&quot;classification-and-regression-heads&quot;&gt;Classification and Regression Heads&lt;/h2&gt;

&lt;p&gt;Following the Section 6.2 of the MobileNetV2 paper, SSDlite replaces the regular convolutions used on the original Heads with separable convolutions. Consequently, our implementation introduces &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L65-L95&quot;&gt;new heads&lt;/a&gt; that use &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L26-L36&quot;&gt;3x3 Depthwise convolutions and 1x1 projections&lt;/a&gt;. Since all other components of the SSD method remain the same, to create an SSDlite model our implementation &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L222-L223&quot;&gt;initializes the SSDlite head&lt;/a&gt; and passes it directly to the SSD constructor.&lt;/p&gt;

&lt;h2 id=&quot;backbone-feature-extractor&quot;&gt;Backbone Feature Extractor&lt;/h2&gt;

&lt;p&gt;Our implementation introduces a new class for building MobileNet &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L98&quot;&gt;feature extractors&lt;/a&gt;. Following the Section 6.3 of the MobileNetV3 paper, the backbone returns the &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L106&quot;&gt;output of the expansion layer&lt;/a&gt; of the Inverted Bottleneck block which has an output stride of 16 and the &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L107&quot;&gt;output of the layer just before the pooling&lt;/a&gt; which has an output stride of 32. Moreover, all &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L111-L116&quot;&gt;extra blocks&lt;/a&gt; of the backbone are replaced with &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L39-L54&quot;&gt;lightweight equivalents&lt;/a&gt; which use a 1x1 compression, a separable 3x3 convolution with stride 2 and a 1x1 expansion. Finally to ensure that the heads have enough prediction power even when small &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L99&quot;&gt;width multipliers&lt;/a&gt; are used, the &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L110&quot;&gt;minimum depth&lt;/a&gt; size of all convolutions is controlled by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min_depth&lt;/code&gt; hyperparameter.&lt;/p&gt;

&lt;h1 id=&quot;the-ssdlite320-mobilenetv3-large-model&quot;&gt;The SSDlite320 MobileNetV3-Large model&lt;/h1&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/ssdlite-pre-trained.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;This section discusses the configuration of the provided &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L159-L162&quot;&gt;SSDlite pre-trained&lt;/a&gt; model along with the training processes followed to replicate the paper results as closely as possible.&lt;/p&gt;

&lt;h2 id=&quot;training-process&quot;&gt;Training process&lt;/h2&gt;

&lt;p&gt;All of the hyperparameters and scripts used to train the model on the COCO dataset can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/blob/e35793a1a4000db1f9f99673437c514e24e65451/references/detection/README.md#ssdlite320-mobilenetv3-large&quot;&gt;references&lt;/a&gt; folder. Here we discuss the most notable details of the training process.&lt;/p&gt;

&lt;h3 id=&quot;tuned-hyperparameters&quot;&gt;Tuned Hyperparameters&lt;/h3&gt;

&lt;p&gt;Though the papers don’t provide any information on the hyperparameters used for training the models (such as regularization, learning rate and the batch size), the parameters listed in the &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config&quot;&gt;configuration files&lt;/a&gt; on the official repo were good starting points and using cross validation we adjusted them to their optimal values. All the above gave us a significant boost over the baseline SSD configuration.&lt;/p&gt;

&lt;h3 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h3&gt;

&lt;p&gt;Key important difference of SSDlite comparing to SSD is that the backbone of the first has only a fraction of the weights of the latter. This is why in SSDlite, the Data Augmentation focuses more on making the model robust to objects of variable sizes than trying to avoid overfitting. Consequently, SSDlite &lt;a href=&quot;https://github.com/pytorch/vision/blob/43d772067fe77965ec8fc49c799de5cea44b8aa2/references/detection/presets.py#L19-L24&quot;&gt;uses only a subset&lt;/a&gt; of the SSD transformations and this way it avoids the over-regularization of the model.&lt;/p&gt;

&lt;h3 id=&quot;lr-scheme&quot;&gt;LR Scheme&lt;/h3&gt;

&lt;p&gt;Due to the reliance on Data Augmentation to make the model robust to small and medium sized objects, we found that it is particularly beneficial for the training recipe to use large number of epochs. More specifically by using roughly 3x more epochs than SSD we are able to increase our precision by 4.2mAP points and by using a 6x multiplier we improve by 4.9mAP. Increasing further the epochs seems to yield diminishing returns and makes the training too slow and impractical, nevertheless based on the &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config#L154&quot;&gt;model configuration&lt;/a&gt; it seems that the authors of the paper used an equivalent &lt;em&gt;16x multiplier&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;weight-initialization--input-scaling--relu6&quot;&gt;Weight Initialization &amp;amp; Input Scaling &amp;amp; ReLU6&lt;/h3&gt;

&lt;p&gt;A set of final optimizations that brought our implementation very close to the official one and helped us bridge the accuracy gap was training the backbone &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L139-L141&quot;&gt;from scratch&lt;/a&gt; instead of initializing from ImageNet, adapting our &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L57-L62&quot;&gt;weight initialization scheme&lt;/a&gt;, changing our &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L216-L219&quot;&gt;Input Scaling&lt;/a&gt; and replacing all standard ReLUs added on the SSDlite heads with ReLU6. Note that since we trained the model from random weights, we additionally applied the speed optimization described on the paper of using a &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L196-L197&quot;&gt;reduced tail&lt;/a&gt; on the backbone.&lt;/p&gt;

&lt;h3 id=&quot;implementation-differences&quot;&gt;Implementation Differences&lt;/h3&gt;

&lt;p&gt;Comparing the above implementation with the one on the official repo, we’ve identified a few differences. Most of them are minor and they are related to how we initialize the weights (for example &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L57-L62&quot;&gt;Normal initialization&lt;/a&gt; vs &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config#L104-L107&quot;&gt;Truncated Normal&lt;/a&gt;), how we parameterize the LR Scheduling (for example &lt;a href=&quot;https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/references/detection/engine.py#L21-L22&quot;&gt;smaller&lt;/a&gt; vs &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config#L169-L170&quot;&gt;larger&lt;/a&gt; warmup rate, &lt;a href=&quot;https://github.com/pytorch/vision/tree/master/references/detection#ssdlite320-mobilenetv3-large&quot;&gt;shorter&lt;/a&gt; vs &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config#L154&quot;&gt;longer&lt;/a&gt; training) etc. The biggest known difference lies in the way we compute the Classification loss. More specifically the implementation of SSDlite with MobileNetV3 backbone on the official repo &lt;a href=&quot;https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config#L121-L124&quot;&gt;doesn’t use the SSD’s Multibox loss&lt;/a&gt; but instead uses RetinaNet’s &lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;&gt;focal loss&lt;/a&gt;. This is a rather significant deviation from the paper and since TorchVision already offers a full implementation of RetinaNet, we decided to implement SSDlite using the normal Multi-box SSD loss.&lt;/p&gt;

&lt;h2 id=&quot;break-down-of-key-accuracy-improvements&quot;&gt;Break down of key accuracy improvements&lt;/h2&gt;

&lt;p&gt;As discussed in previous articles, reproducing research papers and porting them to code is not a journey of monotonically increasing accuracies, especially in cases where the full training and implementation details are not known. Typically the process involves lots of backtracking as one needs to identify those implementation details and parameters that have significant impact on the accuracy from those that don’t. Below we try to visualize the most important iterations that improved our accuracy from the baseline:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/mAP-of-SSD320-MobileNetV3-Large.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Iteration&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;mAP&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Baseline with “SSD-style” Hyperparams&lt;/td&gt;
      &lt;td&gt;10.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Tuned Hyperparams&lt;/td&gt;
      &lt;td&gt;14.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ SSDlite Data Augmentation&lt;/td&gt;
      &lt;td&gt;15.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ 3x LR Scheme&lt;/td&gt;
      &lt;td&gt;19.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ 6x LR Scheme&lt;/td&gt;
      &lt;td&gt;20.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Weight Initialization &amp;amp; Input Scaling &amp;amp; ReLU6&lt;/td&gt;
      &lt;td&gt;21.3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The order of optimizations presented above is accurate, though a bit idealized in some cases. For example, though different schedulers were tested during the Hyperparameter tuning phase, none of them provided significant improvements and thus we maintained the MultiStepLR which was used in the baseline. Nevertheless while later experimenting with different LR Schemes, we found it beneficial to switch to CosineAnnealingLR, as it required less configuration. Consequently, we believe that the main takeaway from the above summary should be that even by starting with a correct implementation and a set of optimal hyperparams from a model of the same family, there is always accuracy points to be found by optimizing the training recipe and tuning the implementation. Admittedly the above is a rather extreme case where the accuracy doubled, but still in many cases there is a large number of optimizations that can help us push the accuracy significantly.&lt;/p&gt;

&lt;h1 id=&quot;benchmarks&quot;&gt;Benchmarks&lt;/h1&gt;

&lt;p&gt;Here is how to initialize the two pre-trained models:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ssdlite&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssdlite320_mobilenet_v3_large&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssd300_vgg16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are the benchmarks between the new and selected previous detection models:&lt;/p&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;mAP&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Inference on CPU (sec)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;# Params (M)&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;SSDlite320 MobileNetV3-Large&lt;/td&gt;
      &lt;td&gt;21.3&lt;/td&gt;
      &lt;td&gt;0.0911&lt;/td&gt;
      &lt;td&gt;3.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SSD300 VGG16&lt;/td&gt;
      &lt;td&gt;25.1&lt;/td&gt;
      &lt;td&gt;0.8303&lt;/td&gt;
      &lt;td&gt;35.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SSD512 VGG16 (not released)&lt;/td&gt;
      &lt;td&gt;28.8&lt;/td&gt;
      &lt;td&gt;2.2494&lt;/td&gt;
      &lt;td&gt;37.08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SSD512 ResNet50 (not released)&lt;/td&gt;
      &lt;td&gt;30.2&lt;/td&gt;
      &lt;td&gt;1.1137&lt;/td&gt;
      &lt;td&gt;42.70&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN MobileNetV3-Large 320 FPN (Low-Res)&lt;/td&gt;
      &lt;td&gt;22.8&lt;/td&gt;
      &lt;td&gt;0.1679&lt;/td&gt;
      &lt;td&gt;19.39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN MobileNetV3-Large FPN (High-Res)&lt;/td&gt;
      &lt;td&gt;32.8&lt;/td&gt;
      &lt;td&gt;0.8409&lt;/td&gt;
      &lt;td&gt;19.39&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As we can see, the SSDlite320 MobileNetV3-Large model is by far the fastest and smallest model and thus it’s an excellent candidate for real-world mobile applications. Though its accuracy is lower than the pre-trained low-resolution Faster R-CNN equivalent, the SSDlite framework is adaptable and one can boost its accuracy by introducing heavier heads with more convolutions.&lt;/p&gt;

&lt;p&gt;On the other hand, the SSD300 VGG16 model is rather slow and less accurate. This is mainly because of its VGG16 backbone. Though extremely important and influential, the VGG architecture is nowadays quite outdated. Thus though the specific model has historical and research value and hence it’s included in TorchVision, we recommend to users who want high-resolution detectors for real world applications to either combine SSD with alternative backbones (see this &lt;a href=&quot;https://github.com/pytorch/vision/pull/3760&quot;&gt;example&lt;/a&gt; on how to create one) or use one of the Faster R-CNN pre-trained models.&lt;/p&gt;

&lt;p&gt;We hope you enjoyed the 2nd and final part of the SSD series. We are looking forward to your feedback.&lt;/p&gt;</content><author><name>Vasilis Vryniotis</name></author><summary type="html">In the previous article, we’ve discussed how the SSD algorithm works, covered its implementation details and presented its training process. If you have not read the previous blog post, I encourage you to check it out before continuing.</summary></entry><entry><title type="html">The torch.linalg module: Accelerated Linear Algebra with Autograd in PyTorch</title><link href="https://pytorch.org/blog/torch-linalg-autograd/" rel="alternate" type="text/html" title="The torch.linalg module: Accelerated Linear Algebra with Autograd in PyTorch" /><published>2021-06-23T00:00:00-07:00</published><updated>2021-06-23T00:00:00-07:00</updated><id>https://pytorch.org/blog/torch-linalg-autograd</id><content type="html" xml:base="https://pytorch.org/blog/torch-linalg-autograd/">&lt;p&gt;Linear algebra is essential to deep learning and scientific computing, and it’s always been a core part of PyTorch. PyTorch 1.9 extends PyTorch’s support for linear algebra operations with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; module. This module, documented &lt;a href=&quot;https://pytorch.org/docs/master/linalg.html?highlight=linalg#module-torch.linalg&quot;&gt;here&lt;/a&gt;, has 26 operators, including faster and easier to use versions of older PyTorch operators, every function from &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.linalg.html&quot;&gt;NumPy’s linear algebra module&lt;/a&gt; extended with accelerator and autograd support, and a few operators that are completely new. This makes the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; immediately familiar to NumPy users and an exciting update to PyTorch’s linear algebra support.&lt;/p&gt;

&lt;h1 id=&quot;numpy-like-linear-algebra-in-pytorch&quot;&gt;NumPy-like linear algebra in PyTorch&lt;/h1&gt;

&lt;p&gt;If you’re familiar with NumPy’s linear algebra module then it’ll be easy to start using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt;. In most cases it’s a drop-in replacement. Let’s looking at drawing samples from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_normal_distribution&quot;&gt;multivariate normal distribution&lt;/a&gt; using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cholesky_decomposition&quot;&gt;Cholesky decomposition&lt;/a&gt; as a motivating example to demonstrate this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Creates inputs
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mu_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Covariance matrix sigma is positive-definite
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;normal_noise_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;standard_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;multivariate_normal_sample_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cholesky&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Random sample: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
      &lt;span class=&quot;n&quot;&gt;multivariate_normal_sample_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Random&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.9502426&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.78518077&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.83168697&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.90798228&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now let’s see the same sampler implemented in PyTorch:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;multivariate_normal_sample_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cholesky&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The two functions are identical, and we can validate their behavior by calling the function with the same arguments wrapped as PyTorch tensors:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# NumPy arrays are wrapped as tensors and share their memory
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_torch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;normal_noise_torch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal_noise_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;multivariate_normal_sample_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal_noise_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.9502&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.7852&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.8317&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9080&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The only difference is in how PyTorch prints tensors by default.&lt;/p&gt;

&lt;p&gt;The Cholesky decomposition can also help us quickly compute the probability density function of the non-degenerate multivariate normal distribution. One of the expensive terms in that computation is the square root of the determinant of the covariance matrix. Using &lt;a href=&quot;https://en.wikipedia.org/wiki/Determinant#Properties_of_the_determinant&quot;&gt;properties of the determinant&lt;/a&gt; and the Cholesky decomposition we can calculate the same result faster than the naive computation, however. Here’s the NumPy program that demonstrates this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sqrt_sigma_det_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;det&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sqrt_L_det_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cholesky&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;|sigma|^0.5 = &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt_sigma_det_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|^&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.237127491242027&lt;/span&gt;
 
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;|L| = &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt_L_det_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.237127491242028&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And here’s the same validation in PyTorch:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sqrt_sigma_det_torch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;det&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sqrt_L_det_torch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cholesky&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;|sigma|^0.5 = &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt_sigma_det_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|^&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.2371&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;|L| = &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt_L_det_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.2371&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can measure the difference in run time using PyTorch’s built-in benchmark utility:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.benchmark&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stmt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'torch.sqrt(torch.linalg.det(sigma))'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sigma'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stmt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'torch.prod(torch.diag(torch.linalg.cholesky(sigma)))'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sigma'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;det&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;mf&quot;&gt;80.80&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cholesky&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
  &lt;span class=&quot;mf&quot;&gt;11.56&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Demonstrating that the approach using the Cholesky decomposition can be significantly faster. Behind the scenes, PyTorch’s linear algebra module uses OpenBLAS or MKL implementations of the LAPACK standard to maximize its CPU performance.&lt;/p&gt;

&lt;h1 id=&quot;autograd-support&quot;&gt;Autograd Support&lt;/h1&gt;

&lt;p&gt;PyTorch’s linear algebra module doesn’t just implement the same functions as NumPy’s linear algebra module (and a few more), it also extends them with autograd and CUDA support.&lt;/p&gt;

&lt;p&gt;Let’s look at a very simple program that just computes an inverse and the gradient of that operation to show how autograd works:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can mimic the same computation in NumPy by defining the autograd formula ourselves:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;inv_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;inv_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inv_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Of course, as programs become more complicated it’s convenient to have builtin autograd support, and PyTorch’s linear algebra module supports both real and complex autograd.&lt;/p&gt;

&lt;h1 id=&quot;cuda-support&quot;&gt;CUDA Support&lt;/h1&gt;

&lt;p&gt;Support for autograd and accelerators, like CUDA devices, is a core part of PyTorch. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; module was developed with NVIDIA’s PyTorch and cuSOLVER teams, who helped optimize its performance on CUDA devices with the cuSOLVER, cuBLAS, and MAGMA libraries. These improvements make PyTorch’s CUDA linear algebra operations faster than ever. For example, let’s look at the performance of PyTorch 1.9’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg.cholesky&lt;/code&gt; vs. PyTorch 1.8’s (now deprecated) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.cholesky&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/cholesky-decomposition.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;(The above charts were created using an Ampere A100 GPU with CUDA 11.3, cuSOLVER 11.1.1.58, and MAGMA 2.5.2. Matrices are in double precision.)&lt;/p&gt;

&lt;p&gt;These charts show that performance has increased significantly on larger matrices, and that batched performance is better across the board. Other linear algebra operations, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg.qr&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg.lstsq&lt;/code&gt;, have also had their CUDA performance improved.&lt;/p&gt;

&lt;h1 id=&quot;beyond-numpy&quot;&gt;Beyond NumPy&lt;/h1&gt;

&lt;p&gt;In addition to offering all the functions in NumPy’s linear algebra module with support for autograd and accelerators, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; has a few new functions of its own. NumPy’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linalg.norm&lt;/code&gt; does not allow users to compute vector norms over arbitrary subsets of dimensions, so to enable this functionality we added &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg.vector_norm&lt;/code&gt;. We’ve also started modernizing other linear algebra functionality in PyTorch, so we created &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg.householder_product&lt;/code&gt; to replace the older &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.orgqr&lt;/code&gt;, and we plan to continue adding more linear algebra functionality in the future, too.&lt;/p&gt;

&lt;h1 id=&quot;the-future-of-linear-algebra-in-pytorch&quot;&gt;The Future of Linear Algebra in PyTorch&lt;/h1&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.linalg&lt;/code&gt; module is fast and familiar with great support for autograd and accelerators. It’s already being used in libraries like &lt;a href=&quot;https://github.com/pytorch/botorch&quot;&gt;botorch&lt;/a&gt;, too. But we’re not stopping here. We plan to continue updating more of PyTorch’s existing linear algebra functionality (like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.lobpcg&lt;/code&gt;) and offering more support for low rank and sparse linear algebra. We also want to hear your feedback on how we can improve, so start a conversation on the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;forum&lt;/a&gt; or file an issue on our &lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;Github&lt;/a&gt; and share your thoughts.&lt;/p&gt;

&lt;p&gt;We look forward to hearing from you and seeing what the community does with PyTorch’s new linear algebra functionality!&lt;/p&gt;</content><author><name>Mike Ruberry, Ivan Yashchuk, Xiao Wang, Mario Lezcano and Natalia Gimelshein</name></author><summary type="html">Linear algebra is essential to deep learning and scientific computing, and it’s always been a core part of PyTorch. PyTorch 1.9 extends PyTorch’s support for linear algebra operations with the torch.linalg module. This module, documented here, has 26 operators, including faster and easier to use versions of older PyTorch operators, every function from NumPy’s linear algebra module extended with accelerator and autograd support, and a few operators that are completely new. This makes the torch.linalg immediately familiar to NumPy users and an exciting update to PyTorch’s linear algebra support.</summary></entry><entry><title type="html">An Overview of the PyTorch Mobile Demo Apps</title><link href="https://pytorch.org/blog/mobile-demo-apps-overview/" rel="alternate" type="text/html" title="An Overview of the PyTorch Mobile Demo Apps" /><published>2021-06-18T10:00:00-07:00</published><updated>2021-06-18T10:00:00-07:00</updated><id>https://pytorch.org/blog/mobile-demo-apps-overview</id><content type="html" xml:base="https://pytorch.org/blog/mobile-demo-apps-overview/">&lt;p&gt;PyTorch Mobile provides a runtime environment to execute state-of-the-art machine learning models on mobile devices. Latency is reduced, privacy preserved, and models can run on mobile devices anytime, anywhere.&lt;/p&gt;

&lt;p&gt;In this blog post, we provide a quick overview of 10 currently available PyTorch Mobile powered demo apps running various state-of-the-art PyTorch 1.9 machine learning models spanning images, video, audio and text.&lt;/p&gt;

&lt;p&gt;It’s never been easier to deploy a state-of-the-art ML model to a phone. You don’t need any domain knowledge in Machine Learning and we hope one of the below examples resonates enough with you to be the starting point for your next project.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/mobile_app_code.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;computer-vision&quot;&gt;Computer Vision&lt;/h2&gt;
&lt;h3 id=&quot;image-classification&quot;&gt;Image Classification&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use PyTorch C++ libraries on iOS and Android to classify a static image with the MobileNetv2/3 model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld&quot;&gt;iOS #1&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/workshops/tree/master/PTMobileWalkthruIOS&quot;&gt;iOS #2&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp&quot;&gt;Android #1&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/workshops/tree/master/PTMobileWalkthruAndroid&quot;&gt;Android #2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://youtu.be/amTepUIR93k&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://youtu.be/5Lxuu16_28o&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_helloworld.png&quot; width=&quot;50%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;live-image-classification&quot;&gt;Live Image Classification&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to run a quantized MobileNetV2 and Resnet18 models to classify images in real time with an iOS and Android device camera.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/PyTorchDemo&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/PyTorchDemoApp&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
&lt;img src=&quot;/assets/images/screenshot_live_image_classification1.png&quot; width=&quot;40%&quot; /&gt;
&lt;img src=&quot;/assets/images/screenshot_live_image_classification2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;image-segmentation&quot;&gt;Image Segmentation&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use the PyTorch DeepLabV3 model to segment images. The updated app for PyTorch 1.9 also demonstrates how to create the model using the Mobile Interpreter and load the model with the LiteModuleLoader API.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/ImageSegmentation&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/ImageSegmentation&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tutorial.png&quot; /&gt; &lt;a href=&quot;https://pytorch.org/tutorials/beginner/deeplabv3_on_ios.html&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/ImageSegmentation&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/deeplab1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/deeplab2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;vision-transformer-for-handwritten-digit-recognition&quot;&gt;Vision Transformer for Handwritten Digit Recognition&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use Facebook’s latest optimized Vision Transformer DeiT model to do image classification and handwritten digit recognition.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/ViT4MNIST&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/ViT4MNIST&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/11L5mIjrLn7B7VdwjQl5vJv3ZVK4hcYut/view?usp=sharing&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_digit_recognition1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_digit_recognition2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to convert the popular YOLOv5 model and use it on an iOS app that detects objects from pictures in your photos, taken with camera, or with live camera.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/ObjectDetection&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/ObjectDetection&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/1pIDrUDnCD5uF-mIz8nbSlZcXxPlRBKhl/view&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://drive.google.com/file/d/1-5AoRONUqZPZByM-fy0m7r8Ct11OnlIT/view&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_object_detection1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_object_detection2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;d2go&quot;&gt;D2Go&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to create and use a much lighter and faster Facebook D2Go model to detect objects from pictures in your photos, taken with camera, or with live camera.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/D2Go&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/D2Go&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/1GO2Ykfv5ut2Mfoc06Y3QUTFkS7407YA4/view&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://drive.google.com/file/d/18-2hLc-7JAKtd1q00X-5pHQCAdyJg7dZ/view?usp=sharing&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_d2go1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_d2go2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;
&lt;h3 id=&quot;video-classification&quot;&gt;Video Classification&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use a pre-trained PyTorchVideo model to perform video classification on tested videos, videos from the Photos library, or even real-time videos.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/TorchVideo&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/TorchVideo&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/1ijb4UIuF2VQiab4xfAsBwrQXCInvb9wd/view&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://drive.google.com/file/d/193tkZgt5Rlk7u-EQPcvkoFtmOQ14-zCC/view&quot;&gt;Android&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=Qb4vDm-ruwI&quot;&gt;Deep Dive&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_video1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_video2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;natural-language-processing&quot;&gt;Natural Language Processing&lt;/h2&gt;
&lt;h3 id=&quot;text-classification&quot;&gt;Text Classification&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use a pre-trained Reddit model to perform text classification.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/PyTorchDemo&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/PyTorchDemoApp&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_textclassification1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_textclassification2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;machine-translation&quot;&gt;Machine Translation&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to convert a sequence-to-sequence neural machine translation model trained with the code in the PyTorch NMT tutorial for french to english translation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/Seq2SeqNMT&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/Seq2SeqNMT&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/17Edk-yAyfzijHPR_2ZDAIX7VY-TkQnLf/view&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://drive.google.com/file/d/110KN3Pa9DprkBWnzj8Ppa8KMymhmBI61/view?usp=sharing&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_machinetranslation1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_machinetranslation2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;question-answering&quot;&gt;Question Answering&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to use the DistilBERT Hugging Face transformer model to answer questions about Pytorch Mobile itself.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/QuestionAnswering&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/QuestionAnswering&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/screencast.png&quot; /&gt; &lt;a href=&quot;https://drive.google.com/file/d/1QIB3yoP4I3zUU0bLCpvUqPV5Kv8f8JvB/view&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://drive.google.com/file/d/10hwGNFo5tylalKwut_CWFPJmV7JRdDKF/view?usp=sharing&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_qa1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_qa2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;audio&quot;&gt;Audio&lt;/h2&gt;
&lt;h3 id=&quot;speech-recognition&quot;&gt;Speech Recognition&lt;/h3&gt;
&lt;p&gt;This app demonstrates how to convert Facebook AI’s torchaudio-powered wav2vec 2.0, one of the leading models in speech recognition to TorchScript before deploying it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github_logo_32.png&quot; /&gt; &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/SpeechRecognition&quot;&gt;iOS&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/SpeechRecognition&quot;&gt;Android&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/screenshot_mobile_asr1.png&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/screenshot_mobile_asr2.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We really hope one of these demo apps stood out for you. For the full list, make sure to visit the &lt;a href=&quot;https://github.com/pytorch/ios-demo-app&quot;&gt;iOS&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/android-demo-app&quot;&gt;Android&lt;/a&gt; demo app repos. You should also definitely check out the video &lt;a href=&quot;https://www.youtube.com/watch?v=Qb4vDm-ruwI&quot;&gt;An Overview of the PyTorch Mobile Demo Apps&lt;/a&gt; which provides both an overview of the PyTorch mobile demo apps and a deep dive into the PyTorch Video app for iOS and Android.&lt;/p&gt;</content><author><name>Jeff Tang and Mark Saroufim</name></author><summary type="html">PyTorch Mobile provides a runtime environment to execute state-of-the-art machine learning models on mobile devices. Latency is reduced, privacy preserved, and models can run on mobile devices anytime, anywhere.</summary></entry><entry><title type="html">Everything You Need To Know About Torchvision’s SSD Implementation</title><link href="https://pytorch.org/blog/torchvision-ssd-implementation/" rel="alternate" type="text/html" title="Everything You Need To Know About Torchvision’s SSD Implementation" /><published>2021-06-16T00:00:00-07:00</published><updated>2021-06-16T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchvision-ssd-implementation</id><content type="html" xml:base="https://pytorch.org/blog/torchvision-ssd-implementation/">&lt;p&gt;In TorchVision v0.10, we’ve released two new Object Detection models based on the SSD architecture. Our plan is to cover the key implementation details of the algorithms along with information on how they were trained in a two-part article.&lt;/p&gt;

&lt;p&gt;In part 1 of the series, we will focus on the original implementation of the SSD algorithm as described on the &lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot;&gt;Single Shot MultiBox Detector paper&lt;/a&gt;. We will briefly give a high-level description of how the algorithm works, then go through its main components, highlight key parts of its code, and finally discuss how we trained the released model. Our goal is to cover all the necessary details to reproduce the model including those optimizations which are not covered on the paper but are part on the &lt;a href=&quot;https://github.com/weiliu89/caffe/tree/ssd&quot;&gt;original implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;how-does-ssd-work&quot;&gt;How Does SSD Work?&lt;/h1&gt;

&lt;p&gt;Reading the aforementioned paper is highly recommended but here is a quick oversimplified refresher. Our target is to detect the locations of objects in an image along with their categories. Here is the Figure 5 from the &lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot;&gt;SSD paper&lt;/a&gt; with prediction examples of the model:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/prediction examples.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The SSD algorithm uses a CNN backbone, passes the input image through it and takes the convolutional outputs from different levels of the network. The list of these outputs are called feature maps. These feature maps are then passed through the Classification and Regression heads which are responsible for predicting the class and the location of the boxes.&lt;/p&gt;

&lt;p&gt;Since the feature maps of each image contain outputs from different levels of the network, their size varies and thus they can capture objects of different dimensions. On top of each, we tile several default boxes which can be thought as our rough prior guesses. For each default box, we predict whether there is an object (along with its class) and its offset (correction over the original location). During training time, we need to first match the ground truth to the default boxes and then we use those matches to estimate our loss. During inference, similar prediction boxes are combined to estimate the final predictions.&lt;/p&gt;

&lt;h1 id=&quot;the-ssd-network-architecture&quot;&gt;The SSD Network Architecture&lt;/h1&gt;

&lt;p&gt;In this section, we will discuss the key components of SSD. Our code follows closely &lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot;&gt;the paper&lt;/a&gt; and makes use of many of the undocumented optimizations included in &lt;a href=&quot;https://github.com/weiliu89/caffe/tree/ssd&quot;&gt;the official implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;defaultboxgenerator&quot;&gt;DefaultBoxGenerator&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L134&quot;&gt;DefaultBoxGenerator class&lt;/a&gt; is responsible for generating the default boxes of SSD and operates similarly to the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L9&quot;&gt;AnchorGenerator&lt;/a&gt; of FasterRCNN (for more info on their differences see pages 4-6 of the paper). It produces a set of predefined boxes of specific width and height which are tiled across the image and serve as the first rough prior guesses of where objects might be located. Here is Figure 1 from the SSD paper with a visualization of ground truths and default boxes:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/visualization of ground truths.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The class is parameterized by a set of hyperparameters that control &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L139&quot;&gt;their shape&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L140-L149&quot;&gt;tiling&lt;/a&gt;. The implementation will provide &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L162-L171&quot;&gt;automatically good guesses&lt;/a&gt; with the default parameters for those who want to experiment with new backbones/datasets but one can also pass &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/anchor_utils.py#L144-L147&quot;&gt;optimized custom values&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;ssdmatcher&quot;&gt;SSDMatcher&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L348&quot;&gt;SSDMatcher class&lt;/a&gt; extends the standard &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L227&quot;&gt;Matcher&lt;/a&gt; used by FasterRCNN and it is responsible for matching the default boxes to the ground truth. After estimating the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L349&quot;&gt;IoUs of all combinations&lt;/a&gt;, we use the matcher to find for each default box the best &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L296&quot;&gt;candidate&lt;/a&gt; ground truth with overlap higher than the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L350-L351&quot;&gt;IoU threshold&lt;/a&gt;. The SSD version of the matcher has an extra step to ensure that each ground truth is matched with the default box that has the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L356-L360&quot;&gt;highest overlap&lt;/a&gt;. The results of the matcher are used in the loss estimation during the training process of the model.&lt;/p&gt;

&lt;h3 id=&quot;classification-and-regression-heads&quot;&gt;Classification and Regression Heads&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L38&quot;&gt;SSDHead class&lt;/a&gt; is responsible for initializing the Classification and Regression parts of the network. Here are a few notable details about their code:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Both the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L90&quot;&gt;Classification&lt;/a&gt; and the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L99&quot;&gt;Regression&lt;/a&gt; head inherit from the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L51&quot;&gt;same class&lt;/a&gt; which is responsible for making the predictions for each feature map.&lt;/li&gt;
  &lt;li&gt;Each level of the feature map uses a separate 3x3 Convolution to estimate the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L92-L94&quot;&gt;class logits&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L101-L103&quot;&gt;box locations&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L79&quot;&gt;number of predictions&lt;/a&gt; that each head makes per level depends on the number of default boxes and the sizes of the feature maps.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;backbone-feature-extractor&quot;&gt;Backbone Feature Extractor&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L413&quot;&gt;feature extractor&lt;/a&gt; reconfigures and enhances a standard VGG backbone with extra layers as depicted on the Figure 2 of the SSD paper:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/feature extractor.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The class supports all VGG models of TorchVision and one can create a similar extractor class for other types of CNNs (see &lt;a href=&quot;https://github.com/pytorch/vision/blob/644bdcdc438c1723714950d0771da76333b53954/torchvision/models/detection/ssd.py#L600&quot;&gt;this example for ResNet&lt;/a&gt;). Here are a few implementation details of the class:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L419-L420&quot;&gt;Patching&lt;/a&gt; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ceil_mode parameter&lt;/code&gt; of the 3rd Maxpool layer is necessary to get the same feature map sizes as the paper. This is due to small differences between PyTorch and the original Caffe implementation of the model.&lt;/li&gt;
  &lt;li&gt;It adds a series of &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L430-L456&quot;&gt;extra feature layers&lt;/a&gt;on top of VGG. If the highres parameter is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt; during its construction, it will append an &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L457-L464&quot;&gt;extra convolution&lt;/a&gt;. This is useful for the SSD512 version of the model.&lt;/li&gt;
  &lt;li&gt;As discussed on section 3 of the paper, the fully connected layers of the original VGG are converted to convolutions with the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L469&quot;&gt;first one using Atrous&lt;/a&gt;. Moreover maxpool5’s stride and kernel size is &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L468&quot;&gt;modified&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;As described on section 3.1, L2 normalization is used on the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L484&quot;&gt;output of conv4_3&lt;/a&gt; and a set of &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L422-L423&quot;&gt;learnable weights&lt;/a&gt; are introduced to control its scaling.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ssd-algorithm&quot;&gt;SSD Algorithm&lt;/h3&gt;

&lt;p&gt;The final key piece of the implementation is on the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L108&quot;&gt;SSD class&lt;/a&gt;. Here are some notable details:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The algorithm is &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L167-L176&quot;&gt;parameterized&lt;/a&gt; by a set of arguments similar to other detection models. The mandatory parameters are: the backbone which is responsible for &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L137-L139&quot;&gt;estimating the feature maps&lt;/a&gt;, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anchor_generator&lt;/code&gt; which should be a &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L140-L141&quot;&gt;configured instance&lt;/a&gt; of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DefaultBoxGenerator&lt;/code&gt; class, the size to which the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L142-L143&quot;&gt;input images&lt;/a&gt; will be resized and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_classes&lt;/code&gt; for classification &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L144&quot;&gt;excluding&lt;/a&gt; the background.&lt;/li&gt;
  &lt;li&gt;If a &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L150-L151&quot;&gt;head&lt;/a&gt; is not provided, the constructor will &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L194&quot;&gt;initialize&lt;/a&gt; the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SSDHead&lt;/code&gt;. To do so, we need to know the number of output channels for each feature map produced by the backbone. Initially we try to &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L186&quot;&gt;retrieve this information&lt;/a&gt; from the backbone but if not available we will &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L189&quot;&gt;dynamically estimate it&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The algorithm &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L183&quot;&gt;reuses&lt;/a&gt; the standard &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/_utils.py#L129&quot;&gt;BoxCoder class&lt;/a&gt; used by other Detection models. The class is responsible for &lt;a href=&quot;https://leimao.github.io/blog/Bounding-Box-Encoding-Decoding/&quot;&gt;encoding and decoding&lt;/a&gt; the bounding boxes and is configured to use the same prior variances as the &lt;a href=&quot;https://github.com/weiliu89/caffe/blob/2c4e4c2899ad7c3a997afef2c1fbac76adca1bad/examples/ssd/ssd_coco.py#L326&quot;&gt;original implementation&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Though we reuse the standard &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/transform.py#L64&quot;&gt;GeneralizedRCNNTransform class&lt;/a&gt; to resize and normalize the input images, the SSD algorithm &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L203-L204&quot;&gt;configures&lt;/a&gt; it to ensure that the image size will remain fixed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are the two core methods of the implementation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_loss&lt;/code&gt; method estimates the standard Multi-box loss as described on page 5 of the SSD paper. It uses the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L244&quot;&gt;smooth L1 loss&lt;/a&gt; for regression and the standard &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L262-L266&quot;&gt;cross-entropy loss&lt;/a&gt; with &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L268-L276&quot;&gt;hard-negative sampling&lt;/a&gt; for classification.&lt;/li&gt;
  &lt;li&gt;As in all detection models, the forward method currently has different behaviour depending on whether the model is on training or eval mode. It starts by &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L309-L310&quot;&gt;resizing &amp;amp; normalizing the input images&lt;/a&gt; and then &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L324-L325&quot;&gt;passes them through the backbone&lt;/a&gt; to get the feature maps. The feature maps are then &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L331-L332&quot;&gt;passed through the head&lt;/a&gt; to get the predictions and then the method &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L334-L335&quot;&gt;generates the default boxes&lt;/a&gt;.
    &lt;ul&gt;
      &lt;li&gt;If the model is on &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L339-L352&quot;&gt;training mode&lt;/a&gt;, the forward will estimate the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L349&quot;&gt;IoUs of the default boxes with the ground truth&lt;/a&gt;, use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SSDmatcher&lt;/code&gt; to &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L350&quot;&gt;produce matches&lt;/a&gt; and finally &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L352&quot;&gt;estimate the losses&lt;/a&gt; by calling the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_loss method&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;If the model is on &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L353-L355&quot;&gt;eval mode&lt;/a&gt;, we first select the best detections by keeping only the ones that &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L384&quot;&gt;pass the score threshold&lt;/a&gt;, select the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L388-L391&quot;&gt;most promising boxes&lt;/a&gt; and run NMS to &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L401-L403&quot;&gt;clean up and select&lt;/a&gt; the best predictions. Finally we &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L355&quot;&gt;postprocess the predictions&lt;/a&gt; to resize them to the original image size.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;the-ssd300-vgg16-model&quot;&gt;The SSD300 VGG16 Model&lt;/h1&gt;

&lt;p&gt;The SSD is a family of models because it can be configured with different backbones and different Head configurations. In this section, we will focus on the provided &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L522-L523&quot;&gt;SSD pre-trained model&lt;/a&gt;. We will discuss the details of its configuration and the training process used to reproduce the reported results.&lt;/p&gt;

&lt;h3 id=&quot;training-process&quot;&gt;Training process&lt;/h3&gt;

&lt;p&gt;The model was trained using the COCO dataset and all of its hyper-parameters and scripts can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/blob/e35793a1a4000db1f9f99673437c514e24e65451/references/detection/README.md#ssd300-vgg16&quot;&gt;references&lt;/a&gt; folder. Below we provide details on the most notable aspects of the training process.&lt;/p&gt;

&lt;h3 id=&quot;paper-hyperparameters&quot;&gt;Paper Hyperparameters&lt;/h3&gt;

&lt;p&gt;In order to achieve the best possible results on COCO, we adopted the hyperparameters described on the section 3 of the paper concerning the optimizer configuration, the weight regularization etc. Moreover we found it useful to adopt the optimizations that appear in the &lt;a href=&quot;https://github.com/weiliu89/caffe/blob/ssd/examples/ssd/ssd_coco.py#L310-L321&quot;&gt;official implementation&lt;/a&gt; concerning the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L579-L581&quot;&gt;tiling configuration&lt;/a&gt; of the DefaultBox generator. This optimization was not described in the paper but it was crucial for improving the detection precision of smaller objects.&lt;/p&gt;

&lt;h3 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h3&gt;

&lt;p&gt;Implementing the &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/references/detection/transforms.py#L20-L239&quot;&gt;SSD Data Augmentation strategy&lt;/a&gt; as described on page 6 and page 12 of the paper was critical to reproducing the results. More specifically the use of random “Zoom In” and “Zoom Out” transformations make the model robust to various input sizes and improve its precision on the small and medium objects. Finally since the VGG16 has quite a few parameters, the photometric distortions &lt;a href=&quot;https://github.com/pytorch/vision/blob/43d772067fe77965ec8fc49c799de5cea44b8aa2/references/detection/presets.py#L11-L18&quot;&gt;included in the augmentations&lt;/a&gt; have a regularization effect and help avoid the overfitting.&lt;/p&gt;

&lt;h3 id=&quot;weight-initialization--input-scaling&quot;&gt;Weight Initialization &amp;amp; Input Scaling&lt;/h3&gt;

&lt;p&gt;Another aspect that we found beneficial was to follow the &lt;a href=&quot;https://github.com/intel/caffe/blob/master/models/intel_optimized_models/ssd/VGGNet/coco/SSD_300x300/train.prototxt&quot;&gt;weight initialization scheme&lt;/a&gt; proposed by the paper. To do that, we had to adapt our input scaling method by &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L583-L587&quot;&gt;undoing the 0-1 scaling&lt;/a&gt; performed by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ToTensor()&lt;/code&gt; and use &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L24-L26&quot;&gt;pre-trained ImageNet weights&lt;/a&gt; fitted with this scaling (shoutout to &lt;a href=&quot;https://github.com/amdegroot&quot;&gt;Max deGroot&lt;/a&gt; for providing them in his repo). All the weights of new convolutions were &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L30-L35&quot;&gt;initialized using Xavier&lt;/a&gt; and their biases were set to zero. After initialization, the network was &lt;a href=&quot;https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L571-L572&quot;&gt;trained end-to-end&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;lr-scheme&quot;&gt;LR Scheme&lt;/h3&gt;

&lt;p&gt;As reported on the paper, after applying aggressive data augmentations it’s necessary to train the models for longer. Our experiments confirm this and we had to tweak the Learning rate, batch sizes and overall steps to achieve the best results. Our &lt;a href=&quot;https://github.com/pytorch/vision/blob/e35793a1a4000db1f9f99673437c514e24e65451/references/detection/README.md#ssd300-vgg16&quot;&gt;proposed learning scheme&lt;/a&gt; is configured to be rather on the safe side, showed signs of plateauing between the steps and thus one is likely to be able to train a similar model by doing only 66% of our epochs.&lt;/p&gt;

&lt;h1 id=&quot;breakdown-of-key-accuracy-improvements&quot;&gt;Breakdown of Key Accuracy Improvements&lt;/h1&gt;

&lt;p&gt;It is important to note that implementing a model directly from a paper is an iterative process that circles between coding, training, bug fixing and adapting the configuration until we match the accuracies reported on the paper. Quite often it also involves simplifying the training recipe or enhancing it with more recent methodologies. It is definitely not a linear process where incremental accuracy improvements are achieved by improving a single direction at a time but instead involves exploring different hypothesis, making incremental improvements in different aspects and doing a lot of backtracking.&lt;/p&gt;

&lt;p&gt;With that in mind, below we try to summarize the optimizations that affected our accuracy the most. We did this by grouping together the various experiments in 4 main groups and attributing the experiment improvements to the closest match. Note that the Y-axis of the graph starts from 18 instead from 0 to make the difference between optimizations more visible:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/Key optimizations for improving the mAP of SSD300 VGG16.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model Configuration&lt;/th&gt;
      &lt;th&gt;mAP delta&lt;/th&gt;
      &lt;th&gt;mAP&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Baseline with “FasterRCNN-style” Hyperparams&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;19.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Paper Hyperparams&lt;/td&gt;
      &lt;td&gt;1.6&lt;/td&gt;
      &lt;td&gt;21.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Data Augmentation&lt;/td&gt;
      &lt;td&gt;1.8&lt;/td&gt;
      &lt;td&gt;22.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Weight Initialization &amp;amp; Input Scaling&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;23.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ LR scheme&lt;/td&gt;
      &lt;td&gt;1.2&lt;/td&gt;
      &lt;td&gt;25.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Our final model achieves an mAP of 25.1 and reproduces exactly the COCO results reported on the paper. Here is a &lt;a href=&quot;https://github.com/pytorch/vision/pull/3403&quot;&gt;detailed breakdown&lt;/a&gt; of the accuracy metrics.&lt;/p&gt;

&lt;p&gt;We hope you found the part 1 of the series interesting. On the part 2, we will focus on the implementation of SSDlite and discuss its differences from SSD. Until then, we are looking forward to your feedback.&lt;/p&gt;</content><author><name>Vasilis Vryniotis</name></author><summary type="html">In TorchVision v0.10, we’ve released two new Object Detection models based on the SSD architecture. Our plan is to cover the key implementation details of the algorithms along with information on how they were trained in a two-part article.</summary></entry><entry><title type="html">New PyTorch Library Releases in PyTorch 1.9, including TorchVision, TorchAudio, and more</title><link href="https://pytorch.org/blog/pytorch-1.9-new-library-releases/" rel="alternate" type="text/html" title="New PyTorch Library Releases in PyTorch 1.9, including TorchVision, TorchAudio, and more" /><published>2021-06-15T00:00:00-07:00</published><updated>2021-06-15T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.9-new-library-releases</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.9-new-library-releases/">&lt;p&gt;Today, we are announcing updates to a number of PyTorch libraries, alongside the &lt;a href=&quot;https://pytorch.org/blog/pytorch-1.9-released/&quot;&gt;PyTorch 1.9 release&lt;/a&gt;. The updates include new releases for the domain libraries including TorchVision, TorchText and TorchAudio. These releases, along with the PyTorch 1.9 release, include a number of new features and improvements that will provide a broad set of updates for the PyTorch community.&lt;/p&gt;

&lt;p&gt;Some highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;TorchVision&lt;/strong&gt; - Added new SSD and SSDLite models, quantized kernels for object detection, GPU Jpeg decoding, and iOS support. See &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;release notes&lt;/a&gt; here.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchAudio&lt;/strong&gt; - Added wav2vec 2.0 model deployable in non-Python environments (including C++, Android, and iOS). Many performance improvements in lfilter, spectral operations, resampling. Added options for quality control in sampling (i.e. Kaiser window support). Initiated the migration of complex tensors operations. Improved autograd support. See &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;release notes&lt;/a&gt; here.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchText&lt;/strong&gt; - Added a new high-performance Vocab module that provides common functional APIs for NLP workflows. See &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;release notes&lt;/a&gt; here.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’d like to thank the community for their support and work on this latest release.&lt;/p&gt;

&lt;p&gt;Features in PyTorch releases are classified as Stable, Beta, and Prototype. You can learn more about the definitions in &lt;a href=&quot;https://pytorch.org/blog/pytorch-feature-classification-changes/&quot;&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;torchvision-010&quot;&gt;TorchVision 0.10&lt;/h1&gt;

&lt;h3 id=&quot;stable-quantized-kernels-for-object-detection&quot;&gt;(Stable) Quantized kernels for object detection&lt;/h3&gt;
&lt;p&gt;The forward pass of the nms and roi_align operators now support tensors with a quantized dtype, which can help lower the memory footprint of object detection models, particularly on mobile environments. For more details, refer to &lt;a href=&quot;https://pytorch.org/vision/stable/ops.html#torchvision.ops.roi_align&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stable-speed-optimizations-for-tensor-transforms&quot;&gt;(Stable) Speed optimizations for Tensor transforms&lt;/h3&gt;
&lt;p&gt;The resize and flip transforms have been optimized and its runtime improved by up to 5x on the CPU.&lt;/p&gt;

&lt;h3 id=&quot;stable-documentation-improvements&quot;&gt;(Stable) Documentation improvements&lt;/h3&gt;
&lt;p&gt;Significant improvements were made to the documentation. In particular, a new gallery of examples is available. These examples visually illustrate how each transform acts on an image, and also properly documents and illustrates the output of the segmentation models.&lt;/p&gt;

&lt;p&gt;The example gallery will be extended in the future to provide more comprehensive examples and serve as a reference for common torchvision tasks. For more details, refer to &lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/index.html&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-new-models-for-detection&quot;&gt;(Beta) New models for detection&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot;&gt;SSD&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;SSDlite&lt;/a&gt; are two popular object detection architectures that are efficient in terms of speed and provide good results for low resolution pictures. In this release, we provide implementations for the original SSD model with VGG16 backbone and for its mobile-friendly variant SSDlite with MobileNetV3-Large backbone.&lt;/p&gt;

&lt;p&gt;The models were pre-trained on COCO train2017 and can be used as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Original SSD variant
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssd300_vgg16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Mobile-friendly SSDlite variant
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;320&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;320&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssdlite320_mobilenet_v3_large&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following accuracies can be obtained on COCO val2017 (full results available in &lt;a href=&quot;https://github.com/pytorch/vision/pull/3403&quot;&gt;#3403&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/vision/pull/3757&quot;&gt;#3757&lt;/a&gt;):&lt;/p&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;mAP&lt;/th&gt;
      &lt;th&gt;mAP@50&lt;/th&gt;
      &lt;th&gt;mAP@75&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;SSD300 VGG16&lt;/td&gt;
      &lt;td&gt;25.1&lt;/td&gt;
      &lt;td&gt;41.5&lt;/td&gt;
      &lt;td&gt;26.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SSDlite320 MobileNetV3-Large&lt;/td&gt;
      &lt;td&gt;21.3&lt;/td&gt;
      &lt;td&gt;34.3&lt;/td&gt;
      &lt;td&gt;22.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;For more details, refer to &lt;a href=&quot;https://pytorch.org/vision/stable/models.html#id37&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-jpeg-decoding-on-the-gpu&quot;&gt;(Beta) JPEG decoding on the GPU&lt;/h3&gt;
&lt;p&gt;Decoding jpegs is now possible on GPUs with the use of &lt;a href=&quot;https://developer.nvidia.com/nvjpeg&quot;&gt;nvjpeg&lt;/a&gt;, which should be readily available in your CUDA setup. The decoding time of a single image should be about 2 to 3 times faster than with libjpeg on CPU. While the resulting tensor will be stored on the GPU device, the input raw tensor still needs to reside on the host (CPU), because the first stages of the decoding process take place on the host:
from torchvision.io.image import read_file, decode_jpeg&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'path_to_image.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# raw data is on CPU
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decode_jpeg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# decoded image in on GPU
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For more details, see &lt;a href=&quot;https://pytorch.org/vision/stable/io.html#torchvision.io.decode_jpeg&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-ios-support&quot;&gt;(Beta) iOS support&lt;/h3&gt;
&lt;p&gt;TorchVision 0.10 now provides pre-compiled iOS binaries for its C++ operators, which means you can run Faster R-CNN and Mask R-CNN on iOS. An example app on how to build a program leveraging those ops can be found &lt;a href=&quot;https://github.com/pytorch/vision/tree/master/ios/VisionTestApp&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;torchaudio-090&quot;&gt;TorchAudio 0.9.0&lt;/h1&gt;

&lt;h3 id=&quot;stable-complex-tensor-migration&quot;&gt;(Stable) Complex Tensor Migration&lt;/h3&gt;
&lt;p&gt;TorchAudio has functions that handle complex-valued tensors. These functions follow a convention to use an extra dimension to represent real and imaginary parts. In PyTorch 1.6, the native complex type was introduced. As its API is getting stable, torchaudio has started to migrate to the native complex type.&lt;/p&gt;

&lt;p&gt;In this release, we added support for native complex tensors, and you can opt-in to use them. Using the native complex types, we have verified that affected functions continue to support autograd and TorchScript, moreover, switching to native complex types improves their performance. For more details, refer to &lt;a href=&quot;https://github.com/pytorch/audio/issues/1337&quot;&gt;pytorch/audio#1337&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stable-filtering-improvement&quot;&gt;(Stable) Filtering Improvement&lt;/h3&gt;
&lt;p&gt;In release 0.8, we added the C++ implementation of the core part of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lfilter&lt;/code&gt; for CPU, which improved the performance. In this release, we optimized some internal operations of the CPU implementation for further performance improvement. We also added autograd support to both CPU and GPU. Now &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lfilter&lt;/code&gt; and all the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;biquad&lt;/code&gt; filters (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;band_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bass_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;treble_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;allpass_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lowpass_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;highpass_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bandpass_biquad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;equalizer_biquad&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bandrefect_biquad&lt;/code&gt;) benefit from the performance improvement and support autograd. We also moved the implementation of overdrive to C++ for performance improvement. For more details, refer to &lt;a href=&quot;https://pytorch.org/audio/0.9.0/functional.html#lfilter&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stable-improved-autograd-support&quot;&gt;(Stable) Improved Autograd Support&lt;/h3&gt;
&lt;p&gt;Along with the work of Complex Tensor Migration and Filtering Improvement, we also added autograd tests to transforms. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lfilter&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;biquad&lt;/code&gt; and its variants, and most transforms are now guaranteed to support autograd. For more details, refer to &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;the release note&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stable-improved-windows-support&quot;&gt;(Stable) Improved Windows Support&lt;/h3&gt;
&lt;p&gt;Torchaudio implements some operations in C++ for reasons such as performance and integration with third-party libraries. These C++ components were only available on Linux and macOS. In this release, we have added support to Windows. With this, the efficient filtering implementation mentioned above is also available on Windows.&lt;/p&gt;

&lt;p&gt;However, please note that not all the C++ components are available for Windows. “sox_io” backend and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.functional.compute_kaldi_pitch&lt;/code&gt; are not supported.&lt;/p&gt;

&lt;h3 id=&quot;stable-io-functions-migration&quot;&gt;(Stable) I/O Functions Migration&lt;/h3&gt;
&lt;p&gt;Since the 0.6 release, we have continuously improved I/O functionality. Specifically, in 0.8 we changed the default backend from “sox” to “sox_io” and applied the same switch to API of the “soundfile” backend. The 0.9 release concludes this migration by removing the deprecated backends. For more details, please refer to &lt;a href=&quot;https://github.com/pytorch/audio/issues/903&quot;&gt;#903&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-wav2vec20-model&quot;&gt;(Beta) Wav2Vec2.0 Model&lt;/h3&gt;
&lt;p&gt;We have added the model architectures from &lt;a href=&quot;https://arxiv.org/abs/2006.11477&quot;&gt;Wav2Vec2.0&lt;/a&gt;. You can import fine-tuned models parameters published on &lt;a href=&quot;https://github.com/pytorch/fairseq/tree/master/examples/wav2vec&quot;&gt;fairseq&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/models?filter=wav2vec2&quot;&gt;Hugging Face Hub&lt;/a&gt;. Our model definition supports TorchScript, and it is possible to deploy the model to non-Python environments, such as C++, &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/SpeechRecognition&quot;&gt;Android&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/ios-demo-app/tree/master/SpeechRecognition&quot;&gt;iOS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The following code snippet illustrates such a use case. Please check out our &lt;a href=&quot;https://github.com/pytorch/audio/tree/master/examples/libtorchaudio&quot;&gt;c++ example directory&lt;/a&gt; for the complete example. Currently, it is designed for running inference. If you would like more support for training, please file a feature request.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Import fine-tuned model from Hugging Face Hub
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchaudio.models.wav2vec2.utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import_huggingface_model&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;original&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Wav2Vec2ForCTC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;facebook/wav2vec2-base-960h&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imported&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import_huggingface_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Import fine-tuned model from fairseq
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fairseq&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchaudio.models.wav2vec2.utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import_fairseq_model&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fairseq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint_utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_model_ensemble_and_task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;wav2vec_small_960h.pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg_overrides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;data_dir&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imported&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import_fairseq_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2v_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Build uninitialized model and load state dict
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchaudio.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wav2vec2_base&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wav2vec2_base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imported&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Quantize / script / optimize for mobile
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantized_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantize_dynamic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qconfig_spec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qint8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scripted_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantized_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimized_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimize_for_mobile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scripted_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimized_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;model_for_deployment.pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more details, see &lt;a href=&quot;https://pytorch.org/audio/0.9.0/models.html#wav2vec2-0&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-resampling-improvement&quot;&gt;(Beta) Resampling Improvement&lt;/h3&gt;
&lt;p&gt;In release 0.8, we vectorized the operation in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.compliance.kaldi.resample_waveform&lt;/code&gt;, which improved the performance of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resample_waveform&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.transforms.Resample&lt;/code&gt;. In this release, we have further revised the way the resampling algorithm is implemented.&lt;/p&gt;

&lt;p&gt;We have:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Added Kaiser Window support for a wider range of resampling quality.&lt;/li&gt;
  &lt;li&gt;Added &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rolloff&lt;/code&gt; parameter for anti-aliasing control.&lt;/li&gt;
  &lt;li&gt;Added the mechanism to precompute the kernel and cache it in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.transforms.Resample&lt;/code&gt; for even faster operation.&lt;/li&gt;
  &lt;li&gt;Moved the implementation from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.compliance.kaldi.resample_waveform&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.functional.resample&lt;/code&gt; and deprecated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.compliance.kaldi.resample_waveform&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more details, see &lt;a href=&quot;https://pytorch.org/audio/0.9.0/transforms.html#resample&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-rnn-transducer-loss&quot;&gt;(Prototype) RNN Transducer Loss&lt;/h3&gt;
&lt;p&gt;The RNN transducer loss is used in training RNN transducer models, which is a popular architecture for speech recognition tasks. The prototype loss in torchaudio currently supports autograd, torchscript, float16 and float32, and can also be run on both CPU and CUDA. For more details, please refer to &lt;a href=&quot;https://pytorch.org/audio/master/rnnt_loss.html&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;torchtext-0100&quot;&gt;TorchText 0.10.0&lt;/h1&gt;

&lt;h3 id=&quot;beta-new-vocab-module&quot;&gt;(Beta) New Vocab Module&lt;/h3&gt;
&lt;p&gt;In this release, we introduce a new Vocab module that replaces the current Vocab class. The new Vocab provides common functional APIs for NLP workflows. This module is backed by an efficient C++ implementation that reduces batch look-up time by up-to ~85% (refer to summary of &lt;a href=&quot;https://github.com/pytorch/text/pull/1248&quot;&gt;#1248&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/text/pull/1290&quot;&gt;#1290&lt;/a&gt; for further information on benchmarks), and provides support for TorchScript. We provide accompanying factory functions that can be used to build the Vocab object either through a python ordered dictionary or an Iterator that yields lists of tokens.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#creating Vocab from text file
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;io&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchtext.vocab&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_vocab_from_iterator&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#generator that yield list of tokens
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;yield_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'utf-8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#get Vocab object
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_vocab_from_iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yield_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;specials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;unk&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#creating Vocab through ordered dict
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchtext.vocab&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OrderedDict&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;counter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sorted_by_freq_tuples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ordered_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OrderedDict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sorted_by_freq_tuples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ordered_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#common API usage
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#look-up index
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#batch look-up indices
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;looup_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#support forward API of PyTorch nn Modules
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#batch look-up tokens
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lookup_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#set default index to return when token not found
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_default_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vocab_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;out_of_vocabulary&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#prints 0
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more details, refer to &lt;a href=&quot;https://pytorch.org/text/stable/vocab.html&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks for reading. If you’re interested in these updates and want to join the PyTorch community, we encourage you to join &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;the discussion&lt;/a&gt; forums and &lt;a href=&quot;https://github.com/pytorch/pytorch/issues&quot;&gt;open GitHub issues&lt;/a&gt;. To get the latest news from PyTorch, follow us on &lt;a href=&quot;https://www.facebook.com/pytorch/&quot;&gt;Facebook&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/PyTorch&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;https://medium.com/pytorch&quot;&gt;Medium&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/pytorch&quot;&gt;YouTube&lt;/a&gt; or &lt;a href=&quot;https://www.linkedin.com/company/pytorch&quot;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;-Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we are announcing updates to a number of PyTorch libraries, alongside the PyTorch 1.9 release. The updates include new releases for the domain libraries including TorchVision, TorchText and TorchAudio. These releases, along with the PyTorch 1.9 release, include a number of new features and improvements that will provide a broad set of updates for the PyTorch community.</summary></entry><entry><title type="html">PyTorch 1.9 Release, including torch.linalg and Mobile Interpreter</title><link href="https://pytorch.org/blog/pytorch-1.9-released/" rel="alternate" type="text/html" title="PyTorch 1.9 Release, including torch.linalg and Mobile Interpreter" /><published>2021-06-15T00:00:00-07:00</published><updated>2021-06-15T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.9-released</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.9-released/">&lt;p&gt;We are excited to announce the release of PyTorch 1.9. The release is composed of more than 3,400 commits since 1.8, made by 398 contributors. The release notes are available &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;. Highlights include:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Major improvements to support scientific computing, including &lt;em&gt;torch.linalg&lt;/em&gt;, &lt;em&gt;torch.special&lt;/em&gt;, and Complex Autograd&lt;/li&gt;
  &lt;li&gt;Major improvements in on-device binary size with Mobile Interpreter&lt;/li&gt;
  &lt;li&gt;Native support for elastic-fault tolerance training through the upstreaming of TorchElastic into PyTorch Core&lt;/li&gt;
  &lt;li&gt;Major updates to the PyTorch RPC framework to support large scale distributed training with GPU support&lt;/li&gt;
  &lt;li&gt;New APIs to optimize performance and packaging for model inference deployment&lt;/li&gt;
  &lt;li&gt;Support for Distributed training, GPU utilization and SM efficiency in the PyTorch Profiler&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Along with 1.9, we are also releasing major updates to the PyTorch libraries, which you can read about in &lt;a href=&quot;https://pytorch.org/blog/pytorch-1.9-new-library-releases/&quot;&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We’d like to thank the community for their support and work on this latest release. We’d especially like to thank Quansight and Microsoft for their contributions.&lt;/p&gt;

&lt;p&gt;Features in PyTorch releases are classified as Stable, Beta, and Prototype. You can learn more about the definitions in &lt;a href=&quot;https://pytorch.org/blog/pytorch-feature-classification-changes/&quot;&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;frontend-apis&quot;&gt;Frontend APIs&lt;/h1&gt;

&lt;h3 id=&quot;stable-torchlinalg&quot;&gt;(Stable) &lt;em&gt;torch.linalg&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;In 1.9, the &lt;em&gt;torch.linalg&lt;/em&gt; module is moving to a stable release. Linear algebra is essential to deep learning and scientific computing, and the &lt;em&gt;torch.linalg&lt;/em&gt; module extends PyTorch’s support for it with implementations of every function from &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.linalg.html&quot;&gt;NumPy’s linear algebra module&lt;/a&gt; (now with support for accelerators and autograd) and more, like &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.linalg.matrix_norm.html?highlight=matrix_norm#torch.linalg.matrix_norm&quot;&gt;&lt;em&gt;torch.linalg.matrix_norm&lt;/em&gt;&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.linalg.householder_product.html?highlight=householder_product#torch.linalg.householder_product&quot;&gt;&lt;em&gt;torch.linalg.householder_product&lt;/em&gt;&lt;/a&gt;. This makes the module immediately familiar to users who have worked with NumPy. Refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/linalg.html?highlight=linalg#module-torch.linalg&quot;&gt;the documentation&lt;/a&gt; here.&lt;/p&gt;

&lt;p&gt;We plan to publish another blog post with more details on the &lt;em&gt;torch.linalg&lt;/em&gt; module next week!&lt;/p&gt;

&lt;h3 id=&quot;stable-complex-autograd&quot;&gt;(Stable) Complex Autograd&lt;/h3&gt;

&lt;p&gt;The Complex Autograd feature, released as a beta in PyTorch 1.8, is now stable. Since the beta release, we have extended support for Complex Autograd for over 98% operators in PyTorch 1.9, improved testing for complex operators by adding more OpInfos, and added greater validation through TorchAudio migration to native complex tensors (refer to &lt;a href=&quot;https://github.com/pytorch/audio/issues/1337&quot;&gt;this issue&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This feature provides users the functionality to calculate complex gradients and optimize real valued loss functions with complex variables. This is a required feature for multiple current and downstream prospective users of complex numbers in PyTorch like TorchAudio, ESPNet, Asteroid, and FastMRI. Refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/notes/autograd.html#autograd-for-complex-numbers&quot;&gt;the documentation&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&quot;stable-torchuse_deterministic_algorithms&quot;&gt;(Stable) torch.use_deterministic_algorithms()&lt;/h3&gt;

&lt;p&gt;To help with debugging and writing reproducible programs, PyTorch 1.9 includes a &lt;em&gt;torch.use_determinstic_algorithms&lt;/em&gt; option. When this setting is enabled, operations will behave deterministically, if possible, or throw a runtime error if they might behave nondeterministically. Here are a couple examples:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sparse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Sparse-dense CUDA bmm is usually nondeterministic
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;use_deterministic_algorithms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Now torch.bmm gives the same result each time, but with reduced performance
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# CUDA kthvalue has no deterministic algorithm, so it throws a runtime error
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kthvalue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;RuntimeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kthvalue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CUDA&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;does&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;have&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;implementation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch 1.9 adds deterministic implementations for a number of indexing operations, too, including &lt;em&gt;index_add&lt;/em&gt;, &lt;em&gt;index_copy&lt;/em&gt;, and &lt;em&gt;index_put with accum=False&lt;/em&gt;. For more details, refer to the &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.use_deterministic_algorithms.html?highlight=use_deterministic#torch.use_deterministic_algorithms&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/1.9.0/notes/randomness.html?highlight=reproducibility&quot;&gt;reproducibility note&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchspecial&quot;&gt;(Beta) &lt;em&gt;torch.special&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;A &lt;em&gt;torch.special&lt;/em&gt; module, analogous to &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/special.html&quot;&gt;SciPy’s special module&lt;/a&gt;, is now available in beta. This module contains many functions useful for scientific computing and working with distributions such as &lt;em&gt;iv&lt;/em&gt;, &lt;em&gt;ive&lt;/em&gt;, &lt;em&gt;erfcx&lt;/em&gt;, &lt;em&gt;logerfc&lt;/em&gt;, and &lt;em&gt;logerfcx&lt;/em&gt;. Refer to &lt;a href=&quot;https://pytorch.org/docs/master/special.html&quot;&gt;the documentation&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&quot;beta-nnmodule-parameterization&quot;&gt;(Beta) nn.Module parameterization&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt; parameterization allows users to parametrize any parameter or buffer of an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt; without modifying the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt; itself. It allows you to constrain the space in which your parameters live without the need for special optimization methods.&lt;/p&gt;

&lt;p&gt;This also contains a new implementation of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spectral_norm&lt;/code&gt; parametrization for PyTorch 1.9. More parametrization will be added to this feature (weight_norm, matrix constraints and part of pruning) for the feature to become stable in 1.10. For more details, refer to the &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.nn.utils.parametrizations.spectral_norm.html?highlight=parametrize&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/parametrizations.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;pytorch-mobile&quot;&gt;PyTorch Mobile&lt;/h1&gt;

&lt;h3 id=&quot;beta-mobile-interpreter&quot;&gt;(Beta) Mobile Interpreter&lt;/h3&gt;

&lt;p&gt;We are releasing Mobile Interpreter, a streamlined version of the PyTorch runtime, in beta. The Interpreter will execute PyTorch programs in edge devices, with reduced binary size footprint.&lt;/p&gt;

&lt;p&gt;Mobile Interpreter is one of the top requested features for PyTorch Mobile. This new release will significantly reduce binary size compared with the current on-device runtime. In order for you to get the binary size improvements with our interpreter (which can reduce the binary size up to ~75% for a typical application) follow these instructions. As an example, using Mobile Interpreter, we can reach 2.6 MB compressed with MobileNetV2 in arm64-v7a Android. With this latest release we are making it much simpler to integrate the interpreter by providing pre-built libraries for iOS and Android.&lt;/p&gt;

&lt;h3 id=&quot;torchvision-library&quot;&gt;TorchVision Library&lt;/h3&gt;

&lt;p&gt;Starting from 1.9, users can use the TorchVision library on their iOS/Android apps. The Torchvision library contains the C++ TorchVision ops and needs to be linked together with the main PyTorch library for iOS, for Android it can be added as a gradle dependency. This allows using TorchVision prebuilt MaskRCNN operators for object detections and segmentation. To learn more about the library, please refer to our tutorials and &lt;a href=&quot;https://github.com/pytorch/android-demo-app/tree/master/D2Go&quot;&gt;demo apps&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;demo-apps&quot;&gt;Demo apps&lt;/h3&gt;

&lt;p&gt;We are releasing a new video app based on &lt;a href=&quot;https://pytorchvideo.org/&quot;&gt;PyTorch Video&lt;/a&gt; library and an updated speech recognition app based on the latest torchaudio, wave2vec model. Both are available on &lt;a href=&quot;https://github.com/pytorch/ios-demo-app&quot;&gt;iOS&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/android-demo-app&quot;&gt;Android&lt;/a&gt;. In addition, we have updated the seven Computer Vision and three Natural Language Processing demo apps, including the HuggingFace DistilBERT, and the DeiT vision transformer models, with PyTorch Mobile v1.9. With the addition of these two apps, we now offer a full suite of demo apps covering image, text, audio, and video. To get started check out our &lt;a href=&quot;https://github.com/pytorch/ios-demo-app&quot;&gt;iOS demo apps&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/android-demo-app&quot;&gt;Android demo apps&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/android-demo-app.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h1 id=&quot;distributed-training&quot;&gt;Distributed Training&lt;/h1&gt;

&lt;h3 id=&quot;beta-torchelastic-is-now-part-of-core&quot;&gt;(Beta) TorchElastic is now part of core&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/50621&quot;&gt;TorchElastic&lt;/a&gt;, which was open sourced over a year ago in the &lt;a href=&quot;https://github.com/pytorch/elastic&quot;&gt;pytorch/elastic&lt;/a&gt; github repository, is a runner and coordinator for PyTorch worker processes. Since then, it has been adopted by various distributed torch use-cases: 1) &lt;a href=&quot;https://medium.com/pytorch/training-deepspeech-using-torchelastic-ad013539682&quot;&gt;deepspeech.pytorch&lt;/a&gt; 2) &lt;a href=&quot;https://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html#torchelastic&quot;&gt;pytorch-lightning&lt;/a&gt; 3) &lt;a href=&quot;https://github.com/pytorch/elastic/blob/master/kubernetes/README.md&quot;&gt;Kubernetes CRD&lt;/a&gt;. Now, it is part of PyTorch core.&lt;/p&gt;

&lt;p&gt;As its name suggests, the core function of TorcheElastic is to gracefully handle scaling events. A notable corollary of elasticity is that peer discovery and rank assignment are built into TorchElastic enabling users to run distributed training on preemptible instances without requiring a gang scheduler. As a side note, &lt;a href=&quot;https://etcd.io/&quot;&gt;etcd&lt;/a&gt; used to be a hard dependency of TorchElastic. With the upstream, this is no longer the case since we have added a “standalone” rendezvous based on c10d::Store. For more details, refer to the &lt;a href=&quot;https://pytorch.org/docs/1.9.0/distributed.elastic.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-distributed-training-updates&quot;&gt;(Beta) Distributed Training Updates&lt;/h3&gt;

&lt;p&gt;In addition to TorchElastic, there are a number of beta features available in the distributed package:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;(Beta) CUDA support is available in RPC&lt;/strong&gt;: Compared to CPU RPC and general-purpose RPC frameworks, CUDA RPC is a much more efficient way for P2P Tensor communication. It is built on top of TensorPipe which can automatically choose a communication channel for each Tensor based on Tensor device type and channel availability on both the caller and the callee. Existing TensorPipe channels cover NVLink, InfiniBand, SHM, CMA, TCP, etc. See &lt;a href=&quot;https://pytorch.org/tutorials/recipes/cuda_rpc.html&quot;&gt;this recipe&lt;/a&gt; for how CUDA RPC helps to attain 34x speedup compared to CPU RPC.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;(Beta) ZeroRedundancyOptimizer&lt;/strong&gt;: ZeroRedundancyOptimizer can be used in conjunction with DistributedDataParallel to reduce the size of per-process optimizer states. The idea of ZeroRedundancyOptimizer comes from &lt;a href=&quot;https://github.com/microsoft/DeepSpeed&quot;&gt;DeepSpeed/ZeRO project&lt;/a&gt; and &lt;a href=&quot;https://github.com/marian-nmt/marian-dev&quot;&gt;Marian&lt;/a&gt;, where the optimizer in each process owns a shard of model parameters and their corresponding optimizer states. When running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step()&lt;/code&gt;, each optimizer only updates its own parameters, and then uses collective communication to synchronize updated parameters across all processes. Refer to &lt;a href=&quot;https://pytorch.org/docs/master/distributed.optim.html&quot;&gt;this documentation&lt;/a&gt; and this &lt;a href=&quot;https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html&quot;&gt;tutorial&lt;/a&gt; to learn more.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;(Beta) Support for profiling distributed collectives&lt;/strong&gt;: PyTorch’s profiler tools, &lt;em&gt;torch.profiler&lt;/em&gt; and &lt;em&gt;torch.autograd.profiler&lt;/em&gt;, are able to profile distributed collectives and point to point communication primitives including allreduce, alltoall, allgather, send/recv, etc. This is enabled for all backends supported natively by PyTorch: gloo, mpi, and nccl. This can be used to debug performance issues, analyze traces that contain distributed communication, and gain insight into performance of applications that use distributed training. To learn more, refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/distributed.html#profiling-collective-communication&quot;&gt;this documentation&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;performance-optimization-and-tooling&quot;&gt;Performance Optimization and Tooling&lt;/h1&gt;

&lt;h3 id=&quot;stable-freezing-api&quot;&gt;(Stable) Freezing API&lt;/h3&gt;

&lt;p&gt;Module Freezing is the process of inlining module parameters and attributes values as constants into the TorchScript internal representation. This allows further optimization and specialization of your program, both for TorchScript optimizations and lowering to other backends. It is used by &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/utils/mobile_optimizer.py&quot;&gt;optimize_for_mobile API&lt;/a&gt;, ONNX, and others.&lt;/p&gt;

&lt;p&gt;Freezing is recommended for model deployment. It helps TorchScript JIT optimizations optimize away overhead and bookkeeping that is necessary for training, tuning, or debugging PyTorch models. It enables graph fusions that are not semantically valid on non-frozen graphs - such as fusing Conv-BN. For more details, refer to the &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.jit.freeze.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-pytorch-profiler&quot;&gt;(Beta) PyTorch Profiler&lt;/h3&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/pytorch-profiler.gif&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The new PyTorch Profiler graduates to beta and leverages &lt;a href=&quot;https://github.com/pytorch/kineto/&quot;&gt;Kineto&lt;/a&gt; for GPU profiling, TensorBoard for visualization and is now the standard across our tutorials and documentation.&lt;/p&gt;

&lt;p&gt;PyTorch 1.9 extends support for the new &lt;em&gt;torch.profiler&lt;/em&gt; API to more builds, including Windows and Mac and is recommended in most cases instead of the previous &lt;em&gt;torch.autograd.profiler&lt;/em&gt; API. The new API supports existing profiler features, integrates with CUPTI library (Linux-only) to trace on-device CUDA kernels and provides support for long-running jobs, e.g.:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;trace_handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;self_cuda_time_total&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_limit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;export_chrome_trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/tmp/trace_&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;activities&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ProfilerActivity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ProfilerActivity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CUDA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# schedule argument specifies the iterations on which the profiler is active
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;schedule&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;warmup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# on_trace_ready argument specifies the handler for the traces
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;on_trace_ready&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace_handler&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# profiler will trace iterations 2 and 3, and then 6 and 7 (counting from zero)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;More usage examples can be found on the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html&quot;&gt;profiler recipe page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The PyTorch Profiler Tensorboard plugin has new features for:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Distributed Training summary view with communications overview for NCCL&lt;/li&gt;
  &lt;li&gt;GPU Utilization and SM Efficiency in Trace view and GPU operators view&lt;/li&gt;
  &lt;li&gt;Memory Profiling view&lt;/li&gt;
  &lt;li&gt;Jump to source when launched from Microsoft VSCode&lt;/li&gt;
  &lt;li&gt;Ability for load traces from cloud object storage systems&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;beta-inference-mode-api&quot;&gt;(Beta) Inference Mode API&lt;/h3&gt;

&lt;p&gt;Inference Mode API allows significant speed-up for inference workloads while remaining safe and ensuring no incorrect gradients can ever be computed. It offers the best possible performance when no autograd is required. For more details, refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/generated/torch.inference_mode.html?highlight=inference%20mode#torch.inference_mode&quot;&gt;the documentation for inference mode itself&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/1.9.0/notes/autograd.html#locally-disabling-gradient-computation&quot;&gt;the documentation explaining when to use it and the difference with no_grad mode&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchpackage&quot;&gt;(Beta) &lt;em&gt;torch.package&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;torch.package&lt;/em&gt; is a new way to package PyTorch models in a self-contained, stable format. A package will include both the model’s data (e.g. parameters, buffers) and its code (model architecture). Packaging a model with its full set of Python dependencies, combined with a description of a conda environment with pinned versions, can be used to easily reproduce training. Representing a model in a self-contained artifact will also allow it to be published and transferred throughout a production ML pipeline while retaining the flexibility of a pure-Python representation. For more details, refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/package.html&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-prepare_for_inference&quot;&gt;(Prototype) prepare_for_inference&lt;/h3&gt;

&lt;p&gt;prepare_for_inference is a new prototype feature that takes in a module and performs graph-level optimizations to improve inference performance, depending on the device. It is meant to be a PyTorch-native option that requires minimal changes to user’s workflows. For more details, see &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/jit/_freeze.py#L168&quot;&gt;the documentation&lt;/a&gt; for the Torchscript version &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/jit/_freeze.py#L168&quot;&gt;here&lt;/a&gt; or the FX version &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/fx/experimental/optimization.py#L234&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-profile-directed-typing-in-torchscript&quot;&gt;(Prototype) Profile-directed typing in TorchScript&lt;/h3&gt;

&lt;p&gt;TorchScript has a hard requirement for source code to have type annotations in order for compilation to be successful. For a long time, it was only possible to add missing or incorrect type annotations through trial and error (i.e., by fixing the type-checking errors generated by &lt;em&gt;torch.jit.script&lt;/em&gt; one by one), which was inefficient and time consuming. Now, we have enabled profile directed typing for &lt;em&gt;torch.jit.script&lt;/em&gt; by leveraging existing tools like MonkeyType, which makes the process much easier, faster, and more efficient. For more details, refer to &lt;a href=&quot;https://pytorch.org/docs/1.9.0/jit.html&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks for reading. If you’re interested in these updates and want to join the PyTorch community, we encourage you to join the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;discussion forums&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/issues&quot;&gt;open GitHub issues&lt;/a&gt;. To get the latest news from PyTorch, follow us on &lt;a href=&quot;https://www.facebook.com/pytorch/&quot;&gt;Facebook&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/PyTorch&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;https://medium.com/pytorch&quot;&gt;Medium&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/pytorch&quot;&gt;YouTube&lt;/a&gt;, or &lt;a href=&quot;https://www.linkedin.com/company/pytorch&quot;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">We are excited to announce the release of PyTorch 1.9. The release is composed of more than 3,400 commits since 1.8, made by 398 contributors. The release notes are available here. Highlights include: Major improvements to support scientific computing, including torch.linalg, torch.special, and Complex Autograd Major improvements in on-device binary size with Mobile Interpreter Native support for elastic-fault tolerance training through the upstreaming of TorchElastic into PyTorch Core Major updates to the PyTorch RPC framework to support large scale distributed training with GPU support New APIs to optimize performance and packaging for model inference deployment Support for Distributed training, GPU utilization and SM efficiency in the PyTorch Profiler</summary></entry><entry><title type="html">Overview of PyTorch Autograd Engine</title><link href="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/" rel="alternate" type="text/html" title="Overview of PyTorch Autograd Engine" /><published>2021-06-08T00:00:00-07:00</published><updated>2021-06-08T00:00:00-07:00</updated><id>https://pytorch.org/blog/overview-of-pytorch-autograd-engine</id><content type="html" xml:base="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/">&lt;p&gt;This blog post is based on PyTorch version 1.8, although it should apply for older versions too, since most of the mechanics have remained constant.&lt;/p&gt;

&lt;p&gt;To help understand the concepts explained here, it is recommended that you read the awesome blog post by &lt;a href=&quot;https://twitter.com/ezyang&quot;&gt;@ezyang&lt;/a&gt;: &lt;a href=&quot;http://blog.ezyang.com/2019/05/pytorch-internals/&quot;&gt;PyTorch internals&lt;/a&gt; if you are not familiar with PyTorch architecture components such as ATen or c10d.&lt;/p&gt;

&lt;h3 id=&quot;what-is-autograd&quot;&gt;What is autograd?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;PyTorch computes the gradient of a function with respect to the inputs by using automatic differentiation. Automatic differentiation is a technique that, given a computational graph, calculates the gradients of the inputs. Automatic differentiation can be performed in two different ways; forward and reverse mode. Forward mode means that we calculate the gradients along with the result of the function, while reverse mode requires us to evaluate the function first, and then we calculate the gradients starting from the output. While both modes have their pros and cons, the reverse mode is the de-facto choice since the number of outputs is smaller than the number of inputs, which allows a much more efficient computation. Check [3] to learn more about this.&lt;/p&gt;

&lt;p&gt;Automatic differentiation relies on a classic calculus formula known as the chain-rule. The chain rule allows us to calculate very complex derivatives by splitting them and recombining them later.&lt;/p&gt;

&lt;p&gt;Formally speaking, given a composite function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(g(x))&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(g(x))&quot; title=&quot;f(g(x))&quot; /&gt;&lt;/a&gt;, we can calculate its derivative as &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial}{\partial&amp;space;x}&amp;space;f(g(x))&amp;space;=&amp;space;f'(g(x))g'(x)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial}{\partial&amp;space;x}&amp;space;f(g(x))&amp;space;=&amp;space;f'(g(x))g'(x)&quot; title=&quot;\frac{\partial}{\partial x} f(g(x)) = f'(g(x))g'(x)&quot; /&gt;&lt;/a&gt;. This result is what makes automatic differentiation work.
By combining the derivatives of the simpler functions that compose a larger one, such as a neural network, it is possible to compute the exact value of the gradient at a given point rather than relying on the numerical approximation, which would require multiple perturbations in the input to obtain a value.&lt;/p&gt;

&lt;p&gt;To get the intuition of how the reverse mode works, let’s look at a simple function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(x,&amp;space;y)&amp;space;=&amp;space;log(x*y)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x,&amp;space;y)&amp;space;=&amp;space;log(x*y)&quot; title=&quot;f(x, y) = log(x*y)&quot; /&gt;&lt;/a&gt;. Figure 1 shows its computational graph where the inputs x, y in the left, flow through a series of operations to generate the output z.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/f_x_y_graph.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 1: Computational graph of f(x, y) = log(x*y)&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The automatic differentiation engine will normally execute this graph. It will also extend it to calculate the derivatives of w with respect to the inputs x, y, and the intermediate result v.&lt;/p&gt;

&lt;p&gt;The example function can be decomposed in f and g, where &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(x,&amp;space;y)&amp;space;=&amp;space;log(g(x,&amp;space;y))&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x,&amp;space;y)&amp;space;=&amp;space;log(g(x,&amp;space;y))&quot; title=&quot;f(x, y) = log(g(x, y))&quot; /&gt;&lt;/a&gt; and &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=g(x,&amp;space;y)&amp;space;=&amp;space;xy&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?g(x,&amp;space;y)&amp;space;=&amp;space;xy&quot; title=&quot;g(x, y) = xy&quot; /&gt;&lt;/a&gt;.  Every time the engine executes an operation in the graph, the derivative of that operation is added to the graph to be executed later in the backward pass. Note, that the engine knows the derivatives of the basic functions.&lt;/p&gt;

&lt;p&gt;In the example above, when multiplying x and y to obtain v, the engine will extend the graph to calculate the partial derivatives of the multiplication by using the multiplication derivative definition that it already knows. &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial}{\partial&amp;space;x}&amp;space;g(x,&amp;space;y)&amp;space;=&amp;space;y&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial}{\partial&amp;space;x}&amp;space;g(x,&amp;space;y)&amp;space;=&amp;space;y&quot; title=&quot;\frac{\partial}{\partial x} g(x, y) = y&quot; /&gt;&lt;/a&gt; and &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial}{\partial&amp;space;y}&amp;space;g(x,&amp;space;y)&amp;space;=&amp;space;x&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial}{\partial&amp;space;y}&amp;space;g(x,&amp;space;y)&amp;space;=&amp;space;x&quot; title=&quot;\frac{\partial}{\partial y} g(x, y) = x&quot; /&gt;&lt;/a&gt; . The resulting extended graph is shown in Figure 2, where the &lt;em&gt;MultDerivative&lt;/em&gt; node also calculates the product of the resulting gradients by an input gradient to apply the chain rule; this will be explicitly seen in the following operations. Note that the backward graph (green nodes) will not be executed until all the forward steps are completed.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/multi_derivative_graph.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 2: Computational graph extended after executing the logarithm&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Continuing, the engine now calculates the &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=log(v)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?log(v)&quot; title=&quot;log(v)&quot; /&gt;&lt;/a&gt; operation and extends the graph again with the log derivative that it knows to be &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{1}{v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{1}{v}&quot; title=&quot;\frac{1}{v}&quot; /&gt;&lt;/a&gt;. This is shown in figure 3. This operation generates the result &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;w}{\partial&amp;space;v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;w}{\partial&amp;space;v}&quot; title=&quot;\frac{\partial w}{\partial v}&quot; /&gt;&lt;/a&gt; that when propagated backward and multiplied by the multiplication derivative as in the chain rule, generates the derivatives &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;w}{\partial&amp;space;x}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;w}{\partial&amp;space;x}&quot; title=&quot;\frac{\partial w}{\partial x}&quot; /&gt;&lt;/a&gt;, &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;w}{\partial&amp;space;x}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;w}{\partial&amp;space;x}&quot; title=&quot;\frac{\partial w}{\partial x}&quot; /&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/extended_computational_graph.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 3: Computational graph extended after executing the logarithm&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The original computation graph is extended with a new dummy variable z that is the same w. The derivative of z with respect to w is 1 as they are the same variable, this trick allows us to apply the chain rule to calculate the derivatives of the inputs. After the forward pass is complete, we start the backward pass, by supplying the initial value of 1.0 for &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;w}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;w}&quot; title=&quot;\frac{\partial z}{\partial w}&quot; /&gt;&lt;/a&gt;. This is shown in Figure 4.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/computational_graph_reverse_auto_differentiation.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 4: Computational graph extended for reverse auto differentiation&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Then following the green graph we execute the LogDerivative operation &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{1}{v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{1}{v}&quot; title=&quot;\frac{1}{v}&quot; /&gt;&lt;/a&gt; that the auto differentiation engine introduced, and multiply its result by &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;w}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;w}&quot; title=&quot;\frac{\partial z}{\partial w}&quot; /&gt;&lt;/a&gt; to obtain the gradient &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;v}&quot; title=&quot;\frac{\partial z}{\partial v}&quot; /&gt;&lt;/a&gt; as per the chain rule states. Next, the multiplication derivative is executed in the same way, and the desired derivatives &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;x}&amp;space;\frac{\partial&amp;space;z}{\partial&amp;space;y}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;x}&amp;space;\frac{\partial&amp;space;z}{\partial&amp;space;y}&quot; title=&quot;\frac{\partial z}{\partial x} \frac{\partial z}{\partial y}&quot; /&gt;&lt;/a&gt; are finally obtained.&lt;/p&gt;

&lt;p&gt;Formally, what we are doing here, and PyTorch autograd engine also does, is computing a Jacobian-vector product (Jvp) to calculate the gradients of the model parameters, since the model parameters and inputs are vectors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Jacobian-vector product&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When we calculate the gradient of a vector-valued function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(\overline{x})&amp;space;=&amp;space;\overline{y}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(\overline{x})&amp;space;=&amp;space;\overline{y}&quot; title=&quot;f(\overline{x}) = \overline{y}&quot; /&gt;&lt;/a&gt; (a function whose inputs and outputs are vectors), we are essentially constructing a Jacobian matrix .&lt;/p&gt;

&lt;p&gt;Thanks to the chain rule, multiplying the Jacobian matrix of a function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(\overline{x})&amp;space;=&amp;space;\overline{y}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(\overline{x})&amp;space;=&amp;space;\overline{y}&quot; title=&quot;f(\overline{x}) = \overline{y}&quot; /&gt;&lt;/a&gt; by a vector &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=v&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?v&quot; title=&quot;v&quot; /&gt;&lt;/a&gt; with the previously calculated gradients of a scalar function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=z&amp;space;=&amp;space;g(\overline{y})&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?z&amp;space;=&amp;space;g(\overline{y})&quot; title=&quot;z = g(\overline{y})&quot; /&gt;&lt;/a&gt; results in the gradients &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;x_1}&amp;space;\cdots&amp;space;\frac{\partial&amp;space;z}{\partial&amp;space;x_n}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;x_1}&amp;space;\cdots&amp;space;\frac{\partial&amp;space;z}{\partial&amp;space;x_n}&quot; title=&quot;\frac{\partial z}{\partial x_1} \cdots \frac{\partial z}{\partial x_n}&quot; /&gt;&lt;/a&gt; of the scalar output with respect to the vector-valued function inputs.&lt;/p&gt;

&lt;p&gt;As an example, let’s look at some functions in python notation to show how the chain rule applies.&lt;/p&gt;
&lt;div class=&quot;outlined-code-block&quot;&gt;
  &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(x_1,&amp;space;x_2)&amp;space;=&amp;space;(log(x_1&amp;space;x_2),&amp;space;sin(x_2))&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x_1,&amp;space;x_2)&amp;space;=&amp;space;(log(x_1&amp;space;x_2),&amp;space;sin(x_2))&quot; title=&quot;f(x_1, x_2) = (log(x_1 x_2), sin(x_2))&quot; /&gt;&lt;/a&gt;

  &lt;pre&gt;def f(x1, x2):
      a = x1 * x2
      y1 = log(a)
      y2 = sin(x2)
      return (y1, y2)
  &lt;/pre&gt;

  &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=g(y_1,&amp;space;y_2)&amp;space;=&amp;space;y_1&amp;space;y_2&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?g(y_1,&amp;space;y_2)&amp;space;=&amp;space;y_1&amp;space;y_2&quot; title=&quot;g(y_1, y_2) = y_1 y_2&quot; /&gt;&lt;/a&gt;

  &lt;pre&gt;def g(y1, y2):
      return y1 * y2
  &lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;Now, if we derive this by hand using the chain rule and the definition of the derivatives, we obtain the following set of identities that we can directly plug into the Jacobian matrix of &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(x_1,&amp;space;x_2)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x_1,&amp;space;x_2)&quot; title=&quot;f(x_1, x_2)&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;outlined-code-block&quot;&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y_1}{\partial&amp;space;x_1}&amp;space;=&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;a}\frac{\partial&amp;space;a}{\partial&amp;space;x_1}&amp;space;=&amp;space;\frac{1}{x_1}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y_1}{\partial&amp;space;x_1}&amp;space;=&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;a}\frac{\partial&amp;space;a}{\partial&amp;space;x_1}&amp;space;=&amp;space;\frac{1}{x_1}&quot; title=&quot;\frac{\partial y_1}{\partial x_1} = \frac{\partial y_1}{\partial a}\frac{\partial a}{\partial x_1} = \frac{1}{x_1}&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y_1}{\partial&amp;space;x_2}&amp;space;=&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;a}\frac{\partial&amp;space;a}{\partial&amp;space;x_2}&amp;space;=&amp;space;\frac{1}{x_2}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y_1}{\partial&amp;space;x_2}&amp;space;=&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;a}\frac{\partial&amp;space;a}{\partial&amp;space;x_2}&amp;space;=&amp;space;\frac{1}{x_2}&quot; title=&quot;\frac{\partial y_1}{\partial x_2} = \frac{\partial y_1}{\partial a}\frac{\partial a}{\partial x_2} = \frac{1}{x_2}&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y_2}{\partial&amp;space;x_1}&amp;space;=&amp;space;0&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y_2}{\partial&amp;space;x_1}&amp;space;=&amp;space;0&quot; title=&quot;\frac{\partial y_2}{\partial x_1} = 0&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y_2}{\partial&amp;space;x_2}&amp;space;=&amp;space;cos(x_2)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y_2}{\partial&amp;space;x_2}&amp;space;=&amp;space;cos(x_2)&quot; title=&quot;\frac{\partial y_2}{\partial x_2} = cos(x_2)&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Next, let’s consider the gradients for the scalar function &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=z&amp;space;=&amp;space;g(y_1,&amp;space;y_2)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?z&amp;space;=&amp;space;g(y_1,&amp;space;y_2)&quot; title=&quot;z = g(y_1, y_2)&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;outlined-code-block&quot;&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{y_1}&amp;space;=&amp;space;y_2&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{y_1}&amp;space;=&amp;space;y_2&quot; title=&quot;\frac{\partial z}{y_1} = y_2&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{y_2}&amp;space;=&amp;space;y_1&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{y_2}&amp;space;=&amp;space;y_1&quot; title=&quot;\frac{\partial z}{y_2} = y_1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;If we now calculate the transpose-Jacobian vector product obeying the chain rule, we obtain the following expression:&lt;/p&gt;
&lt;div style=&quot;overflow:scroll&quot;&gt;
  &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\begin{pmatrix}\frac{\partial&amp;space;y_1}{\partial&amp;space;x_1}&amp;space;&amp;amp;&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;x_2}&amp;space;\\\\&amp;space;\frac{\partial&amp;space;y_2}{\partial&amp;space;x_1}&amp;space;&amp;amp;&amp;space;\frac{\partial&amp;space;y_2}{\partial&amp;space;x_2}&amp;space;\end{pmatrix}^{t}&amp;space;\begin{pmatrix}&amp;space;y_2\\y_1&amp;space;\end{pmatrix}&amp;space;=&amp;space;\begin{pmatrix}\frac{1}{x_1}&amp;space;&amp;amp;&amp;space;\frac{1}{x_2}&amp;space;\\\\&amp;space;0&amp;space;&amp;amp;&amp;space;cos(x_2))&amp;space;\end{pmatrix}^{t}&amp;space;\begin{pmatrix}&amp;space;y_2\\y_1&amp;space;\end{pmatrix}&amp;space;=&amp;space;\begin{pmatrix}&amp;space;\frac{1}{x_1}y_2\\\frac{1}{x_2}y_2&amp;plus;cos(x_2)y_1&amp;space;\end{pmatrix}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\begin{pmatrix}\frac{\partial&amp;space;y_1}{\partial&amp;space;x_1}&amp;space;&amp;amp;&amp;space;\frac{\partial&amp;space;y_1}{\partial&amp;space;x_2}&amp;space;\\\\&amp;space;\frac{\partial&amp;space;y_2}{\partial&amp;space;x_1}&amp;space;&amp;amp;&amp;space;\frac{\partial&amp;space;y_2}{\partial&amp;space;x_2}&amp;space;\end{pmatrix}^{t}&amp;space;\begin{pmatrix}&amp;space;y_2\\y_1&amp;space;\end{pmatrix}&amp;space;=&amp;space;\begin{pmatrix}\frac{1}{x_1}&amp;space;&amp;amp;&amp;space;\frac{1}{x_2}&amp;space;\\\\&amp;space;0&amp;space;&amp;amp;&amp;space;cos(x_2))&amp;space;\end{pmatrix}^{t}&amp;space;\begin{pmatrix}&amp;space;y_2\\y_1&amp;space;\end{pmatrix}&amp;space;=&amp;space;\begin{pmatrix}&amp;space;\frac{1}{x_1}y_2\\\frac{1}{x_2}y_2&amp;plus;cos(x_2)y_1&amp;space;\end{pmatrix}&quot; title=&quot;\begin{pmatrix}\frac{\partial y_1}{\partial x_1} &amp;amp; \frac{\partial y_1}{\partial x_2} \\\\ \frac{\partial y_2}{\partial x_1} &amp;amp; \frac{\partial y_2}{\partial x_2} \end{pmatrix}^{t} \begin{pmatrix} y_2\\y_1 \end{pmatrix} = \begin{pmatrix}\frac{1}{x_1} &amp;amp; \frac{1}{x_2} \\\\ 0 &amp;amp; cos(x_2)) \end{pmatrix}^{t} \begin{pmatrix} y_2\\y_1 \end{pmatrix} = \begin{pmatrix} \frac{1}{x_1}y_2\\\frac{1}{x_2}y_2+cos(x_2)y_1 \end{pmatrix}&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;Evaluating the Jvp for &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=(x_1,&amp;space;x_2)&amp;space;=&amp;space;(0.5,&amp;space;0.75)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?(x_1,&amp;space;x_2)&amp;space;=&amp;space;(0.5,&amp;space;0.75)&quot; title=&quot;(x_1, x_2) = (0.5, 0.75)&quot; /&gt;&lt;/a&gt; yields the result:
&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=(\frac{dy}{x_1},&amp;space;\frac{dy}{x_2})&amp;space;=&amp;space;(1.3633,&amp;space;0.1912)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?(\frac{dy}{x_1},&amp;space;\frac{dy}{x_2})&amp;space;=&amp;space;(1.3633,&amp;space;0.1912)&quot; title=&quot;(\frac{dy}{x_1}, \frac{dy}{x_2}) = (1.3633, 0.1912)&quot; /&gt;&lt;/a&gt;
We can execute the same expression in PyTorch and calculate the gradient of the input:&lt;/p&gt;
&lt;div class=&quot;outlined-code-block&quot;&gt;
  &lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; import torch&lt;/pre&gt;
  &lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; x = torch.tensor([0.5, 0.75], requires_grad=True)&lt;/pre&gt;
  &lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; y = torch.log(x[0] * x[1]) * torch.sin(x[1])&lt;/pre&gt;
  &lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; y.backward(1.0)&lt;/pre&gt;
  &lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; x.grad&lt;/pre&gt;
  tensor([1.3633,
          0.1912])&amp;lt;/pre&amp;gt;
&lt;/div&gt;

&lt;p&gt;The result is the same as our hand-calculated Jacobian-vector product!
However, PyTorch never constructed the matrix as it could grow prohibitively large but instead, created a graph of operations that traversed backward while applying the Jacobian-vector products defined in &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml&quot;&gt;tools/autograd/derivatives.yaml&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Going through the graph&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every time PyTorch executes an operation, the autograd engine constructs the graph to be traversed backward.
The reverse mode auto differentiation starts by adding a scalar variable at the end &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=z&amp;space;=&amp;space;w&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?z&amp;space;=&amp;space;w&quot; title=&quot;z = w&quot; /&gt;&lt;/a&gt; so that &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;w}&amp;space;=&amp;space;1&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;w}&amp;space;=&amp;space;1&quot; title=&quot;\frac{\partial z}{\partial w} = 1&quot; /&gt;&lt;/a&gt; as we saw in the introduction. This is the initial gradient value that is supplied to the Jvp engine calculation as we saw in the section above.&lt;/p&gt;

&lt;p&gt;In PyTorch, the initial gradient is explicitly set by the user when he calls the backward method.&lt;/p&gt;

&lt;p&gt;Then, the Jvp calculation starts but it never constructs the matrix. Instead, when PyTorch records the computational graph, the derivatives of the executed forward operations are added (Backward Nodes). Figure 5 shows a backward graph generated by the execution of the functions &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=f(x_1,&amp;space;x_2)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x_1,&amp;space;x_2)&quot; title=&quot;f(x_1, x_2)&quot; /&gt;&lt;/a&gt; and &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=g(y_1,&amp;space;y_2)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?g(y_1,&amp;space;y_2)&quot; title=&quot;g(y_1, y_2)&quot; /&gt;&lt;/a&gt; seen before.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/computational_graph_backward_pass.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 5: Computational Graph extended with the backward pass&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Once the forward pass is done, the results are used in the backward pass where the derivatives in the computational graph are executed. The basic derivatives are stored in the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml&quot;&gt;tools/autograd/derivatives.yaml&lt;/a&gt; file and they are not regular derivatives but the Jvp versions of them [3]. They take their primitive function inputs and outputs as parameters along with the gradient of the function outputs with respect to the final outputs. By repeatedly multiplying the resulting gradients by the next Jvp derivatives in the graph, the gradients up to the inputs will be generated following the chain rule.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/chain_rule_backward_differentiation.png&quot; width=&quot;100%&quot; /&gt;
  &lt;p&gt;Figure 6: How the chain rule is applied in backward differentiation&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Figure 6 represents the process by showing the chain rule. We started with a value of 1.0 as detailed before which is the already calculated gradient &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y}{\partial&amp;space;u}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y}{\partial&amp;space;u}&quot; title=&quot;\frac{\partial y}{\partial u}&quot; /&gt;&lt;/a&gt; highlighted in green. And we move to the next node in the graph. The &lt;em&gt;backward&lt;/em&gt; function registered in &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/a0a7a2d648f05b0192e6943c9684406cdf404fbf/tools/autograd/derivatives.yaml#L635-L636&quot;&gt;derivatives.yaml&lt;/a&gt; will calculate the associated
&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;u}{\partial&amp;space;v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;u}{\partial&amp;space;v}&quot; title=&quot;\frac{\partial u}{\partial v}&quot; /&gt;&lt;/a&gt; value highlighted in red and multiply it by &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y}{\partial&amp;space;u}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y}{\partial&amp;space;u}&quot; title=&quot;\frac{\partial y}{\partial u}&quot; /&gt;&lt;/a&gt;. By the chain rule this results in &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;y}{\partial&amp;space;v}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;y}{\partial&amp;space;v}&quot; title=&quot;\frac{\partial y}{\partial v}&quot; /&gt;&lt;/a&gt; which will be the already calculated gradient (green) when we process the next backward node in the graph.&lt;/p&gt;

&lt;p&gt;You may also have noticed that in Figure 5 there is a gradient generated from two different sources. When two different functions share an input, the gradients with respect to the output are aggregated for that input, and calculations using that gradient can’t proceed unless all the paths have been aggregated together.&lt;/p&gt;

&lt;p&gt;Let’s see an example of how the derivatives are stored in PyTorch.&lt;/p&gt;

&lt;p&gt;Suppose that we are currently processing the backward propagation of the &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=log&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?log&quot; title=&quot;log&quot; /&gt;&lt;/a&gt; function, in the &lt;em&gt;LogBackward&lt;/em&gt; node in Figure 2.  The derivative of &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=log&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?log&quot; title=&quot;log&quot; /&gt;&lt;/a&gt; in &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/a0a7a2d648f05b0192e6943c9684406cdf404fbf/tools/autograd/derivatives.yaml#L635-L636&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;derivatives.yaml&lt;/code&gt;&lt;/a&gt; is specified as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad.div(self.conj())&lt;/code&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad&lt;/code&gt; is the already calculated gradient &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{\partial&amp;space;y1}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{\partial&amp;space;y1}&quot; title=&quot;\frac{\partial z}{\partial y1}&quot; /&gt;&lt;/a&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.conj()&lt;/code&gt; is the complex conjugate of the input vector. For complex numbers PyTorch calculates a special derivative called the conjugate Wirtinger derivative [6]. This derivative takes the complex number and its conjugate and by operating some magic that is described in [6], they are the direction of steepest descent when plugged into optimizers.&lt;/p&gt;

&lt;p&gt;This code translates to &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=(\frac{\partial&amp;space;z}{\partial&amp;space;y1}&amp;space;\frac{1}{v})&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?(\frac{\partial&amp;space;z}{\partial&amp;space;y1}&amp;space;\frac{1}{v})&quot; title=&quot;(\frac{\partial z}{\partial y1} \frac{1}{v})&quot; /&gt;&lt;/a&gt;, the corresponding green, and red squares in Figure 3. Continuing, the autograd engine will execute the next operation; backward of the multiplication. As before, the inputs are the original function’s inputs and the gradient calculated from the &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=log&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?log&quot; title=&quot;log&quot; /&gt;&lt;/a&gt; backward step. This step will keep repeating until we reach the gradient with respect to the inputs and the computation will be finished. The gradient of &lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&amp;space;z}{x2}&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\frac{\partial&amp;space;z}{x2}&quot; title=&quot;\frac{\partial z}{x2}&quot; /&gt;&lt;/a&gt; is only completed once the multiplication and sin gradients are added together. As you can see, we computed the equivalent of the Jvp but without constructing the matrix.&lt;/p&gt;

&lt;p&gt;In the next post we will dive inside PyTorch code to see how this graph is constructed and where are the relevant pieces should you want to experiment with it!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ol class=&quot;reference-list&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html&quot;&gt;https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf&quot;&gt;https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf&quot;&gt;https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mustafaghali11.medium.com/how-pytorch-backward-function-works-55669b3b7c62&quot;&gt;https://mustafaghali11.medium.com/how-pytorch-backward-function-works-55669b3b7c62&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://indico.cern.ch/event/708041/contributions/3308814/attachments/1813852/2963725/automatic_differentiation_and_deep_learning.pdf&quot;&gt;https://indico.cern.ch/event/708041/contributions/3308814/attachments/1813852/2963725/automatic_differentiation_and_deep_learning.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/autograd.html#complex-autograd-doc&quot;&gt;https://pytorch.org/docs/stable/notes/autograd.html#complex-autograd-doc&lt;/a&gt;&lt;/li&gt;
  &lt;p&gt;Recommended: shows why the backprop is formally expressed with the Jacobian&lt;/p&gt;
  &lt;li&gt;&lt;a href=&quot;cs.ubc.ca/~fwood/CS340/lectures/AD1.pdf&quot;&gt;cs.ubc.ca/~fwood/CS340/lectures/AD1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Preferred Networks, Inc.</name></author><summary type="html">This blog post is based on PyTorch version 1.8, although it should apply for older versions too, since most of the mechanics have remained constant.</summary></entry><entry><title type="html">Everything you need to know about TorchVision’s MobileNetV3 implementation</title><link href="https://pytorch.org/blog/torchvision-mobilenet-v3-implementation/" rel="alternate" type="text/html" title="Everything you need to know about TorchVision’s MobileNetV3 implementation" /><published>2021-05-26T00:00:00-07:00</published><updated>2021-05-26T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchvision-mobilenet-v3-implementation</id><content type="html" xml:base="https://pytorch.org/blog/torchvision-mobilenet-v3-implementation/">&lt;p&gt;In TorchVision v0.9, we released a series of &lt;a href=&quot;https://pytorch.org/blog/ml-models-torchvision-v0.9/&quot;&gt;new mobile-friendly models&lt;/a&gt; that can be used for Classification, Object Detection and Semantic Segmentation. In this article, we will dig deep into the code of the models, share notable implementation details, explain how we configured and trained them, and highlight important tradeoffs we made during their tuning. Our goal is to disclose technical details that typically remain undocumented in the original papers and repos of the models.&lt;/p&gt;

&lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt;

&lt;p&gt;The implementation of the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py&quot;&gt;MobileNetV3 architecture&lt;/a&gt; follows closely the &lt;a href=&quot;https://arxiv.org/abs/1905.02244&quot;&gt;original paper&lt;/a&gt;. It is customizable and offers different configurations for building Classification, Object Detection and Semantic Segmentation backbones. It was designed to follow a similar structure to MobileNetV2 and the two share &lt;a href=&quot;https://github.com/pytorch/vision/blob/cac8a97b0bd14eddeff56f87a890d5cc85776e18/torchvision/models/mobilenetv2.py#L32&quot;&gt;common building blocks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Off-the-shelf, we offer the two variants described on the paper: the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L196-L214&quot;&gt;Large&lt;/a&gt; and the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L215-L229&quot;&gt;Small&lt;/a&gt;. Both are constructed using the same code with the only difference being their configuration which describes the number of blocks, their sizes, their activation functions etc.&lt;/p&gt;

&lt;h3 id=&quot;configuration-parameters&quot;&gt;Configuration parameters&lt;/h3&gt;

&lt;p&gt;Even though one can write a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L105&quot;&gt;custom InvertedResidual setting&lt;/a&gt; and pass it to the MobileNetV3 class directly, for the majority of applications we can adapt the existing configs by passing parameters to the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L253&quot;&gt;model building methods&lt;/a&gt;. Some of the key configuration parameters are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;width_mult&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L188&quot;&gt;parameter&lt;/a&gt; is a multiplier that affects the number of channels of the model. The default value is 1 and by increasing or decreasing it one can change the number of filters of all convolutions, including the ones of the first and last layers. The implementation ensures that the number of filters is always a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L56-L57&quot;&gt;multiple of 8&lt;/a&gt;. This is a hardware optimization trick which allows for faster vectorization of operations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduced_tail&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L188&quot;&gt;parameter&lt;/a&gt; halves the number of channels on the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L210-L214&quot;&gt;last blocks&lt;/a&gt; of the network. This version is used by some Object Detection and Semantic Segmentation models. It’s a speed optimization which is described on the &lt;a href=&quot;https://arxiv.org/abs/1905.02244&quot;&gt;MobileNetV3 paper&lt;/a&gt; and reportedly leads to a 15% latency reduction without a significant negative effect on accuracy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dilated&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L188&quot;&gt;parameter&lt;/a&gt; affects the &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L210-L212&quot;&gt;last 3&lt;/a&gt; InvertedResidual blocks of the model and turns their normal depthwise Convolutions to Atrous Convolutions. This is used to control the output stride of these blocks and has a &lt;a href=&quot;https://arxiv.org/abs/1706.05587&quot;&gt;significant positive effect&lt;/a&gt; on the accuracy of Semantic Segmentation models.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation details&lt;/h3&gt;

&lt;p&gt;Below we provide additional information on some notable implementation details of the architecture.
The &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L101&quot;&gt;MobileNetV3 class&lt;/a&gt; is responsible for building a network out of the provided configuration. Here are some implementation details of the class:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The last convolution block expands the output of the last InvertedResidual block by a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L149&quot;&gt;factor of 6&lt;/a&gt;. The implementation is aligned with the Large and Small configurations described on the paper and can adapt to different values of the multiplier parameter.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly to other models such as MobileNetV2, a dropout layer is placed just before the final Linear layer of the classifier.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L60&quot;&gt;InvertedResidual class&lt;/a&gt; is the main building block of the network. Here are some notable implementation details of the block along with its visualization which comes from Figure 4 of the paper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There is no &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L73-L76&quot;&gt;expansion step&lt;/a&gt; if the input channels and the expanded channels are the same. This happens on the first convolution block of the network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is always a &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L86-L88&quot;&gt;projection step&lt;/a&gt; even when the expanded channels are the same as the output channels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The activation method of the depthwise block is placed &lt;a href=&quot;https://github.com/pytorch/vision/blob/11bf27e37190b320216c349e39b085fb33aefed1/torchvision/models/mobilenetv3.py#L82-L84&quot;&gt;before&lt;/a&gt; the Squeeze-and-Excite layer as this improves marginally the accuracy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/mobilenet-v3-block.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;

&lt;p&gt;In this section we provide benchmarks of the pre-trained models and details on how they were configured, trained and quantized.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here is how to initialize the pre-trained models:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;large = torchvision.models.mobilenet_v3_large(pretrained=True, width_mult=1.0,  reduced_tail=False, dilated=False)
small = torchvision.models.mobilenet_v3_small(pretrained=True)
quantized = torchvision.models.quantization.mobilenet_v3_large(pretrained=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below we have the detailed benchmarks between new and selected previous models. As we can see MobileNetV3-Large is a viable replacement of ResNet50 for users who are willing to sacrifice a bit of accuracy for a roughly 6x speed-up:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@5&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# Params (M)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;74.042&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.340&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0411&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MobileNetV3-Small&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;67.668&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;87.402&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0165&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.54&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Quantized MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;73.004&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.858&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0162&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.96&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MobileNetV2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.880&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.290&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0608&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet50&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;76.150&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92.870&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2545&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;25.56&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet18&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;69.760&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89.080&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1032&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11.69&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Note that the inference times are measured on CPU. They are not absolute benchmarks, but they allow for relative comparisons between models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All pre-trained models are configured with a width multiplier of 1, have full tails, are non-dilated, and were fitted on ImageNet. Both the Large and Small variants were trained using the same hyper-parameters and scripts which can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/c2ab0c59f42babf9ad01aa616cd8a901daac86dd/references/classification#mobilenetv3-large--small&quot;&gt;references&lt;/a&gt; folder. Below we provide details on the most notable aspects of the training process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Achieving fast and stable training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/vision/blob/c2ab0c59f42babf9ad01aa616cd8a901daac86dd/references/classification/train.py#L172-L173&quot;&gt;Configuring RMSProp&lt;/a&gt; correctly was crucial to achieve fast training with numerical stability. The authors of the paper used TensorFlow in their experiments and in their runs they reported using &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet#v3&quot;&gt;quite high&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rmsprop_epsilon&lt;/code&gt; comparing to the default. Typically this hyper-parameter takes small values as it’s used to avoid zero denominators, but in this specific model choosing the right value seems important to avoid numerical instabilities in the loss.&lt;/p&gt;

&lt;p&gt;Another important detail is that though PyTorch’s and TensorFlow’s RMSProp implementations typically behave similarly, there are &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/32545&quot;&gt;a few differences&lt;/a&gt; with the most notable in our setup being how the epsilon hyperparameter is handled. More specifically, PyTorch adds the epsilon &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/training/rmsprop.py#L25&quot;&gt;outside of the square root calculation&lt;/a&gt; while TensorFlow &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/training/rmsprop.py#L25&quot;&gt;adds it inside&lt;/a&gt;. The result of this implementation detail is that one needs to adjust the epsilon value while porting the hyper parameter of the paper. A reasonable approximation can be taken with the formula &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PyTorch_eps = sqrt(TF_eps)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Increasing our accuracy by tuning hyperparameters &amp;amp; improving our training recipe&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After configuring the optimizer to achieve fast and stable training, we turned into optimizing the accuracy of the model. There are a few techniques that helped us achieve this. First of all, to avoid overfitting we augmented out data using the AutoAugment algorithm, followed by RandomErasing. Additionally we tuned parameters such as the weight decay using cross validation. We also found beneficial to perform &lt;a href=&quot;https://github.com/pytorch/vision/blob/674e8140042c2a3cbb1eb9ebad1fa49501599130/references/classification/utils.py#L259&quot;&gt;weight averaging&lt;/a&gt; across different epoch checkpoints after the end of the training. Finally, though not used in our published training recipe, we found that using Label Smoothing, Stochastic Depth and LR noise injection improve the overall accuracy by over &lt;a href=&quot;https://rwightman.github.io/pytorch-image-models/training_hparam_examples/#mobilenetv3-large-100-75766-top-1-92542-top-5&quot;&gt;1.5 points&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The graph and table depict a simplified summary of the most important iterations for improving the accuracy of the MobileNetV3 Large variant. Note that the actual number of iterations done while training the model was significantly larger and that the progress in accuracy was not always monotonically increasing. Also note that the Y-axis of the graph starts from 70% instead from 0% to make the difference between iterations more visible:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/key-iterations-for-improving-the-accuracyof-mobilenetV3.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Iteration&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@5&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Baseline with “MobileNetV2-style” Hyperparams&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.542&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.068&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ RMSProp with default eps&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;70.684&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ RMSProp with adjusted eps &amp;amp; LR scheme&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.764&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.178&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Data Augmentation &amp;amp; Tuned Hyperparams&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;73.86&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.292&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Checkpoint Averaging&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;74.028&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.382&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ Label Smoothing &amp;amp; Stochastic Depth &amp;amp; LR noise&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;75.536&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92.368&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Note that once we’ve achieved an acceptable accuracy, we verified the model performance on the hold-out test dataset which hasn’t been used before for training or hyper-parameter tuning. This process helps us detect overfitting and is always performed for all pre-trained models prior their release.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Quantization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We currently offer quantized weights for the QNNPACK backend of the &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/quantization/mobilenetv3.py#L115&quot;&gt;MobileNetV3-Large variant&lt;/a&gt; which provides a speed-up of 2.5x. To quantize the model, Quantized Aware Training (QAT) was used. The hyper parameters and the scripts used to train the model can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/c2ab0c59f42babf9ad01aa616cd8a901daac86dd/references/classification#quantized&quot;&gt;references&lt;/a&gt; folder.&lt;/p&gt;

&lt;p&gt;Note that QAT allows us to model the effects of quantization and adjust the weights so that we can improve the model accuracy. This translates to an accuracy increase of 1.8 points comparing to simple post-training quantization:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Quantization Status&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Acc@5&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Non-quantized&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;74.042&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.340&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Quantized Aware Training&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;73.004&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.858&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Post-training Quantization&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;71.160&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89.834&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h3&gt;

&lt;p&gt;In this section, we will first provide benchmarks of the released models, and then discuss how the MobileNetV3-Large backbone was used in a Feature Pyramid Network along with the FasterRCNN detector to perform Object Detection. We will also explain how the network was trained and tuned alongside with any tradeoffs we had to make. We will not cover details about how it was used with &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/detection/ssdlite.py&quot;&gt;SSDlite&lt;/a&gt; as this will be discussed on a future article.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here is how the models are initialized:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;high_res = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) 
low_res = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are some benchmarks between new and selected previous models. As we can see the high resolution Faster R-CNN with MobileNetV3-Large FPN backbone seems a viable replacement of the equivalent ResNet50 model for those users who are willing to sacrifice few accuracy points for a 5x speed-up:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mAP&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# Params (M)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN MobileNetV3-Large FPN (High-Res)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;32.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.8409&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;19.39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN MobileNetV3-Large 320 FPN (Low-Res)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;22.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1679&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;19.39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;37.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1514&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;41.76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RetinaNet ResNet-50 FPN&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;36.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.8825&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;34.01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Implementation details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Detector uses a FPN-style backbone which extracts features from different convolutions of the MobileNetV3 model. &lt;a href=&quot;https://github.com/pytorch/vision/blob/eca37cf735064702189ff5d5b1428cbe25ab2bcf/torchvision/models/detection/backbone_utils.py#L165-L166&quot;&gt;By default&lt;/a&gt; the pre-trained model uses the output of the 13th InvertedResidual block and the output of the Convolution prior to the pooling layer but the implementation supports using the outputs of &lt;a href=&quot;https://github.com/pytorch/vision/blob/eca37cf735064702189ff5d5b1428cbe25ab2bcf/torchvision/models/detection/backbone_utils.py#L147-L150&quot;&gt;more stages&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All feature maps extracted from the network have their output projected down to &lt;a href=&quot;https://github.com/pytorch/vision/blob/eca37cf735064702189ff5d5b1428cbe25ab2bcf/torchvision/models/detection/backbone_utils.py#L160&quot;&gt;256 channels&lt;/a&gt; by the FPN block as this greatly improves the speed of the network. These feature maps provided by the FPN backbone are used by the FasterRCNN detector to provide box and class predictions at &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L382-L389&quot;&gt;different scales&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training &amp;amp; Tuning process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We currently offer two pre-trained models capable of doing object detection at different resolutions. Both models were trained on the COCO dataset using the same hyper-parameters and scripts which can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/e35793a1a4000db1f9f99673437c514e24e65451/references/detection#faster-r-cnn-mobilenetv3-large-fpn&quot;&gt;references&lt;/a&gt; folder.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L398-L399&quot;&gt;High Resolution detector&lt;/a&gt; was trained with images of 800-1333px, while the mobile-friendly &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L398-L399&quot;&gt;Low Resolution detector&lt;/a&gt; was trained with images of 320-640px. The reason why we provide two separate sets of pre-trained weights is because training a detector directly on the smaller images leads to a 5 mAP increase in precision comparing to passing small images to the pre-trained high-res model. Both backbones were initialized with weights fitted on ImageNet and the &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L377-L378&quot;&gt;3 last stages&lt;/a&gt; of their weights where fined-tuned during the training process.&lt;/p&gt;

&lt;p&gt;An additional speed optimization can be applied on the mobile-friendly model by &lt;a href=&quot;https://github.com/pytorch/vision/blob/7af30ee9ab64039d04150d118e8b72473184fd6e/torchvision/models/detection/faster_rcnn.py#L423-L424&quot;&gt;tuning the RPN NMS thresholds&lt;/a&gt;. By sacrificing only 0.2 mAP of precision we were able to improve the CPU speed of the model by roughly 45%. The details of the optimization can be seen below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Tuning Status&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mAP&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Before&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;23.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2904&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;After&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;22.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1679&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Below we provide some examples of visualizing the predictions of the Faster R-CNN MobileNetV3-Large FPN model:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/detection.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;semantic-segmentation&quot;&gt;Semantic Segmentation&lt;/h3&gt;

&lt;p&gt;In this section we will start by providing some benchmarks of the released pre-trained models. Then we will discuss how a MobileNetV3-Large backbone was combined with segmentation heads such as &lt;a href=&quot;https://arxiv.org/abs/1905.02244&quot;&gt;LR-ASPP&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1706.05587&quot;&gt;DeepLabV3&lt;/a&gt; and the &lt;a href=&quot;https://arxiv.org/abs/1411.4038&quot;&gt;FCN&lt;/a&gt; to conduct Semantic Segmentation. We will also explain how the network was trained and propose a few optional optimization techniques for speed critical applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is how to initialize the pre-trained models:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(pretrained=True) 
deeplabv3 = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are the detailed benchmarks between new and selected existing models. As we can see, the DeepLabV3 with a MobileNetV3-Large backbone is a viable replacement of FCN with ResNet50 for the majority of applications as it achieves similar accuracy with a 8.5x speed-up. We also observe that the LR-ASPP network supersedes the equivalent FCN in all metrics:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mIoU&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Global Pixel Acc&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# Params (M)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;LR-ASPP MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;57.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.3278&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;60.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5869&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11.03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN MobileNetV3-Large (not released)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;57.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.3702&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 ResNet50&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;66.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.3531&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;39.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN ResNet50&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;60.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.0146&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;32.96&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;implementation-details-1&quot;&gt;Implementation details&lt;/h3&gt;

&lt;p&gt;In this section we will discuss important implementation details of tested segmentation heads. Note that all models described in this section use a dilated MobileNetV3-Large backbone.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LR-ASPP&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The LR-ASPP is the Lite variant of the Reduced Atrous Spatial Pyramid Pooling model proposed by the authors of the MobileNetV3 paper. Unlike the other segmentation models in TorchVision, it does not make use of an &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L185-L186&quot;&gt;auxiliary loss&lt;/a&gt;. Instead it uses &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L92-L100&quot;&gt;low and high-level features&lt;/a&gt; with output strides of 8 and 16 respectively.&lt;/p&gt;

&lt;p&gt;Unlike the paper where a 49x49 AveragePooling layer with variable strides is used, &lt;a href=&quot;https://github.com/pytorch/vision/blob/e2db2eddbb1699a59fbb5ccbec912979048ef3bf/torchvision/models/segmentation/lraspp.py#L53&quot;&gt;our implementation&lt;/a&gt; uses an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AdaptiveAvgPool2d&lt;/code&gt; layer to process the global features. This is because the authors of the paper tailored the head to the Cityscapes dataset while our focus is to provide a general purpose implementation that can work on multiple datasets. Finally our implementation always has a bilinear interpolation &lt;a href=&quot;https://github.com/pytorch/vision/blob/e2db2eddbb1699a59fbb5ccbec912979048ef3bf/torchvision/models/segmentation/lraspp.py#L35&quot;&gt;before returning the output&lt;/a&gt; to ensure that the sizes of the input and output images match exactly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DeepLabV3 &amp;amp; FCN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The combination of MobileNetV3 with DeepLabV3 and FCN follows closely the ones of other models and the stage estimation for these methods is identical to LR-ASPP. The only notable difference is that instead of using high and low level features, &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L37-L45&quot;&gt;we attach&lt;/a&gt; the normal loss to the feature map with output stride 16 and an auxiliary loss on the feature map with output stride 8.&lt;/p&gt;

&lt;p&gt;Finally we should note that the FCN version of the model was not released because it was completely superseded by the LR-ASPP both in terms of speed and accuracy. The &lt;a href=&quot;https://github.com/pytorch/vision/pull/3276/commits/1641d5f4c7d41f534444fab340c598d61a91bd12#diff-ccff7af514d99eeb40416c8b9ec30f032d1a3f450aaa4057958ca39ab174452eL17&quot;&gt;pre-trained weights&lt;/a&gt; are still available and can be used with minimal changes to the code.&lt;/p&gt;

&lt;h3 id=&quot;training--tuning-process&quot;&gt;Training &amp;amp; Tuning process&lt;/h3&gt;

&lt;p&gt;We currently offer two MobileNetV3 pre-trained models capable of doing semantic segmentation: the LR-ASPP and the DeepLabV3. The backbones of the models were &lt;a href=&quot;https://github.com/pytorch/vision/blob/b94a4014a68d08f37697f4672729571a46f0042d/torchvision/models/segmentation/segmentation.py#L89-L90&quot;&gt;initialized with ImageNet weights&lt;/a&gt; and trained end-to-end. Both architectures were trained on the COCO dataset using the same scripts with similar hyper-parameters. Their details can be found in our &lt;a href=&quot;https://github.com/pytorch/vision/tree/a78d0d83d0a499fe8480d7a9f493676e746c4699/references/segmentation#deeplabv3_mobilenet_v3_large&quot;&gt;references&lt;/a&gt; folder.&lt;/p&gt;

&lt;p&gt;Normally, during inference the images are &lt;a href=&quot;https://github.com/pytorch/vision/blob/a78d0d83d0a499fe8480d7a9f493676e746c4699/references/segmentation/train.py#L30-L33&quot;&gt;resized to 520 pixels&lt;/a&gt;. An optional speed optimization is to construct a Low Res configuration of the model by using the High-Res pre-trained weights and reducing the inference resizing to 320 pixels. This will improve the CPU execution times by roughly 60% while sacrificing a couple of mIoU points. The detailed numbers of this optimization can be found on the table below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Low-Res Configuration&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mIoU Difference&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Speed Improvement&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;mIoU&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Global Pixel Acc&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Inference on CPU (sec)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;LR-ASPP MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-2.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;65.26%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;55.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1139&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 MobileNetV3-Large&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-3.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;63.86%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;56.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2121&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN MobileNetV3-Large (not released)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;57.57%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;54.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1571&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here are some examples of visualizing the predictions of the LR-ASPP MobileNetV3-Large model:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;/assets/images/segmentation.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We hope that you found this article interesting. We are looking forward to your feedback to see if this is the type of content you would like us to publish more often. If the community finds that such posts are useful, we will be happy to publish more articles that cover the implementation details of newly introduced Machine Learning models.&lt;/p&gt;</content><author><name>Vasilis Vryniotis and Francisco Massa</name></author><summary type="html">In TorchVision v0.9, we released a series of new mobile-friendly models that can be used for Classification, Object Detection and Semantic Segmentation. In this article, we will dig deep into the code of the models, share notable implementation details, explain how we configured and trained them, and highlight important tradeoffs we made during their tuning. Our goal is to disclose technical details that typically remain undocumented in the original papers and repos of the models.</summary></entry></feed>