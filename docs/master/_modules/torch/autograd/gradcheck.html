


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.autograd.gradcheck &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/autograd/gradcheck.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/elastic/">
                  <span class="dropdown-title">TorchElastic</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>master (1.9.0a0+gitb881571 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/autograd/gradcheck.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../autograd.html">torch.autograd</a> &gt;</li>
        
      <li>torch.autograd.gradcheck</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.autograd.gradcheck</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.types</span> <span class="kn">import</span> <span class="n">_TensorOrTensors</span>
<span class="kn">import</span> <span class="nn">torch.testing</span>
<span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">is_tensor_like</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">torch._vmap_internals</span> <span class="kn">import</span> <span class="n">vmap</span>
<span class="kn">import</span> <span class="nn">functools</span>


<span class="k">def</span> <span class="nf">is_float_or_complex_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_complex</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">allocate_jacobians_with_inputs</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">numel_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="c1"># Makes zero-filled tensors from inputs. If `numel_output` is not None, for each tensor in</span>
    <span class="c1"># `input_tensors`, returns a new zero-filled tensor with height of `t.numel` and width</span>
    <span class="c1"># of `numel_output`. Otherwise, for each tensor, returns a 1-d tensor with size `(t.numel,)`.</span>
    <span class="c1"># Each new tensor will be strided and have the same dtype and device as those of the</span>
    <span class="c1"># corresponding input</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_tensors</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_float_or_complex_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="n">t</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">numel_output</span><span class="p">),</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">allocate_jacobians_with_outputs</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">numel_input</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="c1"># Makes zero-filled tensors from outputs. If `dim` is not None, for each tensor in</span>
    <span class="c1"># `output_tensors`, returns a new zero-filled tensor with height of `dim` and width of</span>
    <span class="c1"># `t.numel`. Otherwise, for each tensor, returns a 1-d tensor with size (t.numel,).</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">options</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;layout&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">}</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">output_tensors</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_float_or_complex_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="n">numel_input</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">numel</span><span class="p">()),</span> <span class="o">**</span><span class="n">options</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">iter_tensors</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">only_requiring_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="c1"># mypy doesn&#39;t narrow type of `x` to torch.Tensor</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">only_requiring_grad</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="k">yield</span> <span class="n">x</span>  <span class="c1"># type: ignore</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">iter_tensors</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="n">only_requiring_grad</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">iter_tensor</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">):</span>
    <span class="c1"># Enumerates over a tensor and provides a corresponding flat index that translates</span>
    <span class="c1"># to a given rol/col in the jacobian matrix. The order is the same as as if we flatten</span>
    <span class="c1"># a contiguous tensor. iter_tensor also returns a strided version of the original</span>
    <span class="c1"># tensor that is able to be modified inplace. If the input tensor is strided or sparse,</span>
    <span class="c1"># the returned tensor will share storage with the original. Otherwise, for opaque tensor</span>
    <span class="c1"># types like mkldnn, a copy is returned.</span>
    <span class="c1">#</span>
    <span class="c1"># Example:</span>
    <span class="c1">#   for a tensor t with size (2, 2), it will yield:</span>
    <span class="c1">#     `x, (0, 0), 0`, `x, (0, 1), 1`, `x, (1, 0), 2`, `x, (1, 1), 3`</span>
    <span class="c1">#</span>
    <span class="c1">#   where x is the t.data of the original tensor. Since input t has numel 4, the</span>
    <span class="c1">#   Jacobian should have 4 columns. So having a d_idx of 3 and idx of (1, 1)</span>
    <span class="c1">#   indicates that perturbing t[(1, 1)] allows us to updating the third (last)</span>
    <span class="c1">#   column of any jacobian corresponding to this particular input.</span>
    <span class="c1">#</span>
    <span class="k">if</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">get_stride</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
            <span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">stride</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">dim</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)):</span>
                <span class="n">stride</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span>
                <span class="n">tmp</span> <span class="o">*=</span> <span class="n">size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">stride</span>

        <span class="n">x_nnz</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">_nnz</span><span class="p">()</span>
        <span class="n">x_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">x_indices</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="n">x_values</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
        <span class="n">x_stride</span> <span class="o">=</span> <span class="n">get_stride</span><span class="p">(</span><span class="n">x_size</span><span class="p">)</span>

        <span class="c1"># Use .data here to get around the version check</span>
        <span class="n">x_values</span> <span class="o">=</span> <span class="n">x_values</span><span class="o">.</span><span class="n">data</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_nnz</span><span class="p">):</span>
            <span class="n">x_value</span> <span class="o">=</span> <span class="n">x_values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">x_idx</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">x_values</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]]):</span>
                <span class="n">indices</span> <span class="o">=</span> <span class="n">x_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_idx</span><span class="p">)</span>
                <span class="n">d_idx</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">indices</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_stride</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_size</span><span class="p">)))</span>
                <span class="k">yield</span> <span class="n">x_value</span><span class="p">,</span> <span class="n">x_idx</span><span class="p">,</span> <span class="n">d_idx</span>
    <span class="k">elif</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
        <span class="k">for</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">x_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()])):</span>
            <span class="c1"># this is really inefficient, but without indexing implemented, there&#39;s</span>
            <span class="c1"># not really a better way than converting back and forth</span>
            <span class="n">x_tensor_dense</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
            <span class="k">yield</span> <span class="n">x_tensor_dense</span><span class="p">,</span> <span class="n">x_idx</span><span class="p">,</span> <span class="n">d_idx</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Use .data here to get around the version check</span>
        <span class="n">x_tensor</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">data</span>
        <span class="k">for</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">x_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()])):</span>
            <span class="k">yield</span> <span class="n">x_tensor</span><span class="p">,</span> <span class="n">x_idx</span><span class="p">,</span> <span class="n">d_idx</span>


<span class="k">def</span> <span class="nf">_get_numerical_jacobian</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                            <span class="n">grad_out</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Computes the numerical jacobian for a given fn and inputs. Returns M * N jacobians</span>
<span class="sd">    where M is the number of input tensors that require grad, and N is the number of output</span>
<span class="sd">    float/complex tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn: the function to compute the jacobian for</span>
<span class="sd">        inputs: inputs to `fn`</span>
<span class="sd">        outputs: provide precomputed outputs to avoid one extra invocation of fn</span>
<span class="sd">        target: the Tensors wrt whom Jacobians are calculated (default=`inputs`)</span>
<span class="sd">        eps: the magnitude of the perturbation during finite differencing (default=`1e-3`)</span>
<span class="sd">        grad_out: grad output value used to calculate gradients.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of M N-tuples of tensors</span>

<span class="sd">    Note that `target` may not even be part of `input` to `fn`, so please be</span>
<span class="sd">    **very careful** in this to not clone `target`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">jacobians</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">inp_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">inp_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">iter_tensors</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span> <span class="n">inp_indices</span><span class="p">)):</span>
        <span class="n">jacobians</span> <span class="o">+=</span> <span class="p">[</span><span class="n">get_numerical_jacobian_wrt_specific_input</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inp_idx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">inp</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">jacobians</span>


<span class="k">def</span> <span class="nf">get_numerical_jacobian</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">grad_out</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated api to compute numerical jacobian for a given fn and inputs.</span>
<span class="sd">    Args:</span>
<span class="sd">        fn: the function to compute the jacobian for (must take inputs as a tuple)</span>
<span class="sd">        input: input to `fn`</span>
<span class="sd">        target: the Tensors wrt whom Jacobians are calculated (default=`input`)</span>
<span class="sd">        eps: the magnitude of the perturbation during finite differencing (default=`1e-3`)</span>
<span class="sd">        grad_out: grad output value used to calculate gradients.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of jacobians wrt each input (or target) and the first output</span>

<span class="sd">    Note that `target` may not even be part of `input` to `fn`, so please be</span>
<span class="sd">    **very careful** in this to not clone `target`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;get_numerical_jacobian was part of PyTorch&#39;s private API and not &quot;</span>
                  <span class="s2">&quot;meant to be exposed. We are deprecating it and it will be removed &quot;</span>
                  <span class="s2">&quot;in a future version of PyTorch. If you have a specific use for &quot;</span>
                  <span class="s2">&quot;this or feature request for this to be a stable API, please file &quot;</span>
                  <span class="s2">&quot;us an issue at https://github.com/pytorch/pytorch/issues/new&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fn_pack_inps</span><span class="p">(</span><span class="o">*</span><span class="n">inps</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">inps</span><span class="p">)</span>
    <span class="n">jacobians</span> <span class="o">=</span> <span class="n">_get_numerical_jacobian</span><span class="p">(</span><span class="n">fn_pack_inps</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">jacobian_for_each_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">jacobian_for_each_output</span> <span class="ow">in</span> <span class="n">jacobians</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">compute_numerical_gradient</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">entry</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">norm_v</span><span class="p">,</span> <span class="n">nbhd_checks_fn</span><span class="p">):</span>
    <span class="c1"># Performs finite differencing by perturbing `entry` in-place by `v` and</span>
    <span class="c1"># returns the gradient of each of the outputs wrt to x at idx.</span>
    <span class="n">orig</span> <span class="o">=</span> <span class="n">entry</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">entry</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">orig</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">outa</span> <span class="o">=</span> <span class="n">fn</span><span class="p">()</span>
    <span class="n">entry</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">orig</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">outb</span> <span class="o">=</span> <span class="n">fn</span><span class="p">()</span>
    <span class="n">entry</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">orig</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="n">nbhd_checks_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">norm_v</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">compute</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">outa</span><span class="p">,</span> <span class="n">outb</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">compute_numerical_jacobian_cols</span><span class="p">(</span><span class="n">jvp_fn</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">input_is_complex</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="c1"># Computing the jacobian only works for pure real or pure imaginary delta</span>
    <span class="c1"># for details on the algorithm used here, refer:</span>
    <span class="c1"># Section 3.5.3 https://arxiv.org/pdf/1701.00392.pdf</span>
    <span class="c1"># s = fn(z) where z = x for real valued input</span>
    <span class="c1"># and z = x + yj for complex valued input</span>
    <span class="n">jacobians_cols</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ds_dx_tup</span> <span class="o">=</span> <span class="n">jvp_fn</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">input_is_complex</span><span class="p">:</span>            <span class="c1"># C -&gt; C, C -&gt; R</span>
        <span class="n">ds_dy_tup</span> <span class="o">=</span> <span class="n">jvp_fn</span><span class="p">(</span><span class="n">delta</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">ds_dx</span><span class="p">,</span> <span class="n">ds_dy</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ds_dx_tup</span><span class="p">,</span> <span class="n">ds_dy_tup</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">ds_dx</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad_out</span><span class="p">,</span> <span class="nb">complex</span><span class="p">):</span>
                <span class="c1"># placeholder if grad_out is complex but output is not</span>
                <span class="n">jacobians_cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">ds_dx</span><span class="p">))</span>
                <span class="k">continue</span>
            <span class="c1"># conjugate wirtinger derivative</span>
            <span class="n">conj_w_d</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">ds_dx</span> <span class="o">+</span> <span class="n">ds_dy</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
            <span class="c1"># wirtinger derivative</span>
            <span class="n">w_d</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">ds_dx</span> <span class="o">-</span> <span class="n">ds_dy</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
            <span class="n">jacobians_cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_out</span><span class="o">.</span><span class="n">conjugate</span><span class="p">()</span> <span class="o">*</span> <span class="n">conj_w_d</span> <span class="o">+</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">w_d</span><span class="o">.</span><span class="n">conj</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">ds_dx</span> <span class="ow">in</span> <span class="n">ds_dx_tup</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">ds_dx</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>  <span class="c1"># R -&gt; C</span>
                <span class="c1"># w_d = conj_w_d = 0.5 * ds_dx</span>
                <span class="c1"># dL_dz_conj = 0.5 * [grad_out.conj() * ds_dx + grad_out * ds_dx.conj()]</span>
                <span class="c1">#            = 0.5 * [grad_out.conj() * ds_dx + (grad_out.conj() * ds_dx).conj()]</span>
                <span class="c1">#            = 0.5 * 2 * real(grad_out.conj() * ds_dx)</span>
                <span class="c1">#            = real(grad_out.conj() * ds_dx)</span>
                <span class="n">jacobians_cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">grad_out</span><span class="o">.</span><span class="n">conjugate</span><span class="p">()</span> <span class="o">*</span> <span class="n">ds_dx</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>                   <span class="c1"># R -&gt; R</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad_out</span><span class="p">,</span> <span class="nb">complex</span><span class="p">):</span>
                    <span class="c1"># placeholder if grad_out is complex but output is not</span>
                    <span class="n">jacobians_cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">ds_dx</span><span class="p">))</span>
                    <span class="k">continue</span>
                <span class="n">jacobians_cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ds_dx</span> <span class="o">*</span> <span class="n">grad_out</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jacobians_cols</span>


<span class="k">def</span> <span class="nf">combine_jacobian_cols</span><span class="p">(</span><span class="n">jacobians_cols</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">outputs</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span>
                          <span class="n">numel</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="c1"># jacobian_cols is a data structure that maps column_idx -&gt; output_idx -&gt; column of jacobian Tensor</span>
    <span class="c1"># we return a list that maps output_idx -&gt; full jacobian Tensor</span>
    <span class="n">jacobians</span> <span class="o">=</span> <span class="n">allocate_jacobians_with_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">numel</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">jacobian</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">jacobians</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">jacobians_cols</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">jacobian</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">jacobians</span>


<span class="k">def</span> <span class="nf">prepped_input</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">maybe_perturbed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                  <span class="n">fast_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># Prepares the inputs to be passed into the function while including the new modified input.</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span><span class="p">:</span>  <span class="c1"># type: ignore # no attr _mkldnn</span>
        <span class="c1"># Convert back to mkldnn</span>
        <span class="k">if</span> <span class="n">maybe_perturbed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">maybe_perturbed_input</span><span class="o">.</span><span class="n">to_mkldnn</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">input</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">fast_mode</span> <span class="ow">and</span> <span class="n">maybe_perturbed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># entry is already a &quot;cloned&quot; version of the original tensor</span>
            <span class="c1"># thus changes to entry are not reflected in the input</span>
            <span class="k">return</span> <span class="n">maybe_perturbed_input</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">input</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># We cannot use entry (input.data) if we want gradgrad to work because</span>
        <span class="c1"># fn (in the gradgrad case) needs to compute grad wrt input</span>
        <span class="k">return</span> <span class="nb">input</span>


<span class="k">def</span> <span class="nf">check_outputs_same_dtype_and_shape</span><span class="p">(</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">idx</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Check that the returned outputs don&#39;t have different dtype or shape when you</span>
    <span class="c1"># perturb the input</span>
    <span class="n">on_index</span> <span class="o">=</span> <span class="s2">&quot;on index </span><span class="si">{idx}</span><span class="s2"> &quot;</span> <span class="k">if</span> <span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">output1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">output2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> \
        <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected `func` to return outputs with the same shape&quot;</span>
         <span class="sa">f</span><span class="s2">&quot; when inputs are perturbed </span><span class="si">{</span><span class="n">on_index</span><span class="si">}</span><span class="s2">by </span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2">, but got:&quot;</span>
         <span class="sa">f</span><span class="s2">&quot; shapes </span><span class="si">{</span><span class="n">output1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">output2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">output1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">output2</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> \
        <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected `func` to return outputs with the same dtype&quot;</span>
         <span class="sa">f</span><span class="s2">&quot; when inputs are perturbed </span><span class="si">{</span><span class="n">on_index</span><span class="si">}</span><span class="s2">by </span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2">, but got:&quot;</span>
         <span class="sa">f</span><span class="s2">&quot; dtypes </span><span class="si">{</span><span class="n">output1</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">output2</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_numerical_jacobian_wrt_specific_input</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                                              <span class="n">grad_out</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="c1"># Computes the numerical jacobians wrt to a single input. Returns N jacobian</span>
    <span class="c1"># tensors, where N is the number of outputs</span>
    <span class="c1"># We use a dictionary because for sparse inputs, d_idx aren&#39;t necessarily consecutive</span>
    <span class="n">jacobian_cols</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">input_idx</span><span class="p">]</span> <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">input</span>
    <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">requires_grad</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">d_idx</span> <span class="ow">in</span> <span class="n">iter_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
        <span class="n">wrapped_fn</span> <span class="o">=</span> <span class="n">with_prepped_inputs</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">input_to_perturb</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">nbhd_checks_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">check_outputs_same_dtype_and_shape</span><span class="p">,</span> <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">jvp_fn</span> <span class="o">=</span> <span class="n">get_jvp_fn</span><span class="p">(</span><span class="n">wrapped_fn</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nbhd_checks_fn</span><span class="p">)</span>
        <span class="n">jacobian_cols</span><span class="p">[</span><span class="n">d_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_numerical_jacobian_cols</span><span class="p">(</span><span class="n">jvp_fn</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(),</span> <span class="n">grad_out</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">combine_jacobian_cols</span><span class="p">(</span><span class="n">jacobian_cols</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">get_input_to_perturb</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span><span class="p">:</span>  <span class="c1"># type: ignore # no attr _mkldnn</span>
        <span class="c1"># Convert to dense so we can perform operations that require strided tensors</span>
        <span class="n">input_to_perturb</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
        <span class="c1"># Clone because input may require grad, and copy_ calls resize_,</span>
        <span class="c1"># which is not allowed for .data</span>
        <span class="n">input_to_perturb</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">input_to_perturb</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">data</span>
    <span class="k">return</span> <span class="n">input_to_perturb</span>


<span class="k">def</span> <span class="nf">with_prepped_inputs</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="n">fast_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">wrapped_fn</span><span class="p">():</span>
        <span class="n">inp</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">prepped_input</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">input_to_perturb</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">input_idx</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">fast_mode</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">else</span> <span class="n">a</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">inp</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">wrapped_fn</span>


<span class="k">def</span> <span class="nf">get_jvp_fn</span><span class="p">(</span><span class="n">wrapped_fn</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nbhd_checks_fn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">jvp_fn</span><span class="p">(</span><span class="n">delta</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">compute_numerical_gradient</span><span class="p">(</span><span class="n">wrapped_fn</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nbhd_checks_fn</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jvp_fn</span>


<span class="k">def</span> <span class="nf">get_jvp_wrt_specific_input</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="c1"># If fast_mode=False, iter_tensor handles the below cases:</span>
    <span class="c1"># basically we want to prepare the input so that it can be modified in-place and do certain</span>
    <span class="c1"># operations that require the tensor to have strides</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">input_idx</span><span class="p">]</span>
    <span class="n">input_to_perturb</span> <span class="o">=</span> <span class="n">get_input_to_perturb</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">wrapped_fn</span> <span class="o">=</span> <span class="n">with_prepped_inputs</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">nbhd_checks_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">check_outputs_same_dtype_and_shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="n">jvp_fn</span> <span class="o">=</span> <span class="n">get_jvp_fn</span><span class="p">(</span><span class="n">wrapped_fn</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nbhd_checks_fn</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">u</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_to_perturb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">compute_numerical_jacobian_cols</span><span class="p">(</span><span class="n">jvp_fn</span><span class="p">,</span> <span class="n">u</span> <span class="o">*</span> <span class="n">eps</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(),</span> <span class="n">grad_out</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_jacobians_equal</span><span class="p">(</span><span class="n">j1</span><span class="p">,</span> <span class="n">j2</span><span class="p">,</span> <span class="n">atol</span><span class="p">):</span>
    <span class="c1"># Check whether the max diff between two jacobians are within some tolerance `atol`</span>
    <span class="k">for</span> <span class="n">j1_x</span><span class="p">,</span> <span class="n">j2_x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">j1</span><span class="p">,</span> <span class="n">j2</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">j1_x</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="p">(</span><span class="n">j1_x</span> <span class="o">-</span> <span class="n">j2_x</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">atol</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">stack_and_check_tensors</span><span class="p">(</span><span class="n">jacobians_rows</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">numel_outputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
    <span class="n">out_jacobians</span> <span class="o">=</span> <span class="n">allocate_jacobians_with_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">numel_outputs</span><span class="p">)</span>
    <span class="n">diff_input_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">iter_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
    <span class="n">correct_grad_sizes</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">correct_grad_types</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">rows</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">jacobians_rows</span><span class="p">):</span>
        <span class="n">inp</span> <span class="o">=</span> <span class="n">diff_input_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">out_jacobian</span> <span class="o">=</span> <span class="n">out_jacobians</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rows</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">row</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">row</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">inp</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
                <span class="n">correct_grad_sizes</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">elif</span> <span class="n">row</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">row</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">inp</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                <span class="n">correct_grad_types</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">row</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">out_jacobian</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">row_dense</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">row</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span> <span class="k">else</span> <span class="n">row</span>
                <span class="k">assert</span> <span class="n">out_jacobian</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">row_dense</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="n">out_jacobian</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">row_dense</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out_jacobians</span><span class="p">,</span> <span class="n">correct_grad_sizes</span><span class="p">,</span> <span class="n">correct_grad_types</span>


<span class="n">FAILED_NONDET_MSG</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>

<span class="s2">NOTE: If your op relies on non-deterministic operations i.e., it is listed here:</span>
<span class="s2">https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html</span>
<span class="s2">this failure might be expected.</span>

<span class="s2">If you are adding a new operator, please file an issue and then use one of the</span>
<span class="s2">workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.</span>
<span class="s2">If the test</span>
<span class="s2">- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck</span>
<span class="s2">  with `nondet_tol=&lt;tol&gt;` as a keyword argument.</span>
<span class="s2">- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test</span>
<span class="s2">  to have `gradcheck_nondet_tol=&lt;tol&gt;`.</span>
<span class="s2">- is a Module test (e.g., in common_nn.py), then modify the corresponding</span>
<span class="s2">  module_test entry to have `gradcheck_nondet_tol=&lt;tol&gt;`</span>
<span class="s2">&quot;&quot;&quot;</span>


<span class="k">def</span> <span class="nf">check_analytical_jacobian_attributes</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="n">grad_out_scale</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span>
                                         <span class="n">raise_exception</span><span class="p">,</span> <span class="n">custom_vjp_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fast_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                         <span class="n">v</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]:</span>
    <span class="n">diff_input_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">iter_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">backward_fn</span><span class="p">(</span><span class="n">grad_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span>
                                   <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">vjp_fn</span> <span class="o">=</span> <span class="n">custom_vjp_fn</span> <span class="k">if</span> <span class="n">custom_vjp_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">backward_fn</span>

    <span class="k">if</span> <span class="n">fast_mode</span><span class="p">:</span>
        <span class="c1"># vjp can be seen as a linear combination of the jacobians rows, we still call into stack_and_check</span>
        <span class="c1"># because we&#39;d like to reuse the checks for dtype and shape</span>
        <span class="n">jacobians_rows</span> <span class="o">=</span> <span class="n">get_vjp_wrt_specific_output</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">v</span><span class="p">,</span> <span class="n">grad_out_scale</span><span class="p">)</span>
        <span class="n">jacobians_rows_reentrant</span> <span class="o">=</span> <span class="n">get_vjp_wrt_specific_output</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">v</span><span class="p">,</span> <span class="n">grad_out_scale</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">jacobians_rows</span> <span class="o">=</span> <span class="n">compute_analytical_jacobian_rows</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">grad_out_scale</span><span class="p">)</span>
        <span class="n">jacobians_rows_reentrant</span> <span class="o">=</span> <span class="n">compute_analytical_jacobian_rows</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">grad_out_scale</span><span class="p">)</span>
    <span class="n">output_numel</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">fast_mode</span> <span class="k">else</span> <span class="mi">1</span>

    <span class="n">jacobians</span><span class="p">,</span> <span class="n">correct_grad_types</span><span class="p">,</span> <span class="n">correct_grad_sizes</span> <span class="o">=</span> <span class="n">stack_and_check_tensors</span><span class="p">(</span><span class="n">jacobians_rows</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_numel</span><span class="p">)</span>
    <span class="n">jacobians_reentrant</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">stack_and_check_tensors</span><span class="p">(</span><span class="n">jacobians_rows_reentrant</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_numel</span><span class="p">)</span>

    <span class="n">reentrant</span> <span class="o">=</span> <span class="n">check_jacobians_equal</span><span class="p">(</span><span class="n">jacobians</span><span class="p">,</span> <span class="n">jacobians_reentrant</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">)</span>

    <span class="n">complex_str</span> <span class="o">=</span> <span class="s1">&#39;(calculated using complex valued grad output) &#39;</span> \
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad_out_scale</span><span class="p">,</span> <span class="nb">complex</span><span class="p">)</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">fail_test</span><span class="p">(</span><span class="n">msg</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">raise_exception</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">correct_grad_types</span> <span class="ow">and</span> <span class="n">check_grad_dtypes</span><span class="p">:</span>
        <span class="n">fail_test</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Gradient</span><span class="si">{</span><span class="n">complex_str</span><span class="si">}</span><span class="s1"> has dtype mismatch&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">correct_grad_sizes</span><span class="p">:</span>
        <span class="n">fail_test</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Analytical gradient</span><span class="si">{</span><span class="n">complex_str</span><span class="si">}</span><span class="s1"> has incorrect size&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">reentrant</span><span class="p">:</span>
        <span class="n">fail_test</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Backward</span><span class="si">{</span><span class="n">complex_str</span><span class="si">}</span><span class="s1"> is not reentrant, i.e., running backward with &#39;</span>
                  <span class="s1">&#39;same input and grad_output multiple times gives different values, &#39;</span>
                  <span class="s1">&#39;although analytical gradient matches numerical gradient.&#39;</span>
                  <span class="sa">f</span><span class="s1">&#39;The tolerance for nondeterminism was </span><span class="si">{</span><span class="n">nondet_tol</span><span class="si">}</span><span class="s1">.&#39;</span> <span class="o">+</span> <span class="n">FAILED_NONDET_MSG</span><span class="p">)</span>
    <span class="n">failed</span> <span class="o">=</span> <span class="ow">not</span> <span class="p">(</span><span class="n">reentrant</span> <span class="ow">and</span> <span class="n">correct_grad_sizes</span> <span class="ow">and</span> <span class="n">correct_grad_types</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jacobians</span><span class="p">,</span> <span class="n">failed</span>


<span class="k">def</span> <span class="nf">get_analytical_jacobian</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">grad_out</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># Replicates the behavior of the old get_analytical_jacobian before the refactor</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;get_analytical_jacobian was part of PyTorch&#39;s private API and not &quot;</span>
                  <span class="s2">&quot;meant to be exposed. We are deprecating it and it will be removed &quot;</span>
                  <span class="s2">&quot;in a future version of PyTorch. If you have a specific use for &quot;</span>
                  <span class="s2">&quot;this or feature request for this to be a stable API, please file &quot;</span>
                  <span class="s2">&quot;us an issue at https://github.com/pytorch/pytorch/issues/new&quot;</span><span class="p">)</span>

    <span class="n">diff_input_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">iter_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">backward_fn</span><span class="p">(</span><span class="n">grad_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span>
                                   <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">jacobians_rows</span> <span class="o">=</span> <span class="n">compute_analytical_jacobian_rows</span><span class="p">(</span><span class="n">backward_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">grad_out</span><span class="p">)</span>
    <span class="n">jacobians_rows_reentrant</span> <span class="o">=</span> <span class="n">compute_analytical_jacobian_rows</span><span class="p">(</span><span class="n">backward_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">grad_out</span><span class="p">)</span>

    <span class="n">output_numel</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">jacobians</span><span class="p">,</span> <span class="n">correct_grad_types</span><span class="p">,</span> <span class="n">correct_grad_sizes</span> <span class="o">=</span> <span class="n">stack_and_check_tensors</span><span class="p">(</span><span class="n">jacobians_rows</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_numel</span><span class="p">)</span>
    <span class="n">jacobians_reentrant</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">stack_and_check_tensors</span><span class="p">(</span><span class="n">jacobians_rows_reentrant</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_numel</span><span class="p">)</span>
    <span class="n">reentrant</span> <span class="o">=</span> <span class="n">check_jacobians_equal</span><span class="p">(</span><span class="n">jacobians</span><span class="p">,</span> <span class="n">jacobians_reentrant</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">jacobians</span><span class="p">,</span> <span class="n">reentrant</span><span class="p">,</span> <span class="n">correct_grad_sizes</span><span class="p">,</span> <span class="n">correct_grad_types</span>


<span class="k">def</span> <span class="nf">_get_analytical_jacobian</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">):</span>
    <span class="c1"># Computes the analytical jacobian in slow mode for a single input_idx, output_idx pair</span>
    <span class="c1"># without performing checks for dtype, shape, and reentrancy</span>
    <span class="n">jacobians</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">check_analytical_jacobian_attributes</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">[</span><span class="n">output_idx</span><span class="p">],</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jacobians</span><span class="p">[</span><span class="n">input_idx</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">compute_analytical_jacobian_rows</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">sample_output</span><span class="p">,</span> <span class="n">grad_out_scale</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
    <span class="c1"># Computes Jacobian row-by-row using backward function `vjp_fn` = v^T J</span>
    <span class="c1"># NB: this function does not assume vjp_fn(v) to return tensors with</span>
    <span class="c1"># the same number of elements for different v. This is checked when we</span>
    <span class="c1"># later combine the rows into a single tensor.</span>
    <span class="n">grad_out_base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sample_output</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">legacy_contiguous_format</span><span class="p">)</span>
    <span class="n">flat_grad_out</span> <span class="o">=</span> <span class="n">grad_out_base</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># jacobians_rows[i][j] represents the jth row of the ith input</span>
    <span class="n">jacobians_rows</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flat_grad_out</span><span class="o">.</span><span class="n">numel</span><span class="p">()):</span>
        <span class="n">flat_grad_out</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="n">flat_grad_out</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_out_scale</span>
        <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">vjp_fn</span><span class="p">(</span><span class="n">grad_out_base</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">jacobians_rows</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
            <span class="n">jacobians_rows</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="p">[</span><span class="n">d_x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">d_x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">jacobians_rows</span>


<span class="k">def</span> <span class="nf">get_vjp_wrt_specific_output</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">sample_output</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">grad_out_scale</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
    <span class="n">jacobians_rows</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">vjp_fn</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sample_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad_out_scale</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">):</span>
        <span class="n">jacobians_rows</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">d_x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">d_x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">jacobians_rows</span>


<span class="k">def</span> <span class="nf">check_inputs</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">check_sparse_nnz</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">check_sparse_nnz</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">is_sparse</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tupled_inputs</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="s1">&#39;gradcheck expects all tensor inputs are dense when check_sparse_nnz is set to False.&#39;</span><span class="p">)</span>
    <span class="c1"># Make sure that gradients are saved for at least one input</span>
    <span class="n">any_input_requiring_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inp</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span> <span class="ow">or</span> <span class="n">inp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;Input #</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s1"> requires gradient and &#39;</span>
                    <span class="s1">&#39;is not a double precision floating point or complex. &#39;</span>
                    <span class="s1">&#39;This check will likely fail if all the inputs are &#39;</span>
                    <span class="s1">&#39;not of double precision floating point or complex. &#39;</span><span class="p">)</span>
            <span class="n">content</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span> <span class="k">if</span> <span class="n">inp</span><span class="o">.</span><span class="n">is_sparse</span> <span class="k">else</span> <span class="n">inp</span>
            <span class="c1"># TODO: To cover more problematic cases, replace stride = 0 check with</span>
            <span class="c1"># &quot;any overlap in memory&quot; once we have a proper function to check it.</span>
            <span class="k">if</span> <span class="n">content</span><span class="o">.</span><span class="n">layout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">st</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">sz</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">st</span><span class="p">,</span> <span class="n">sz</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">content</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span> <span class="n">content</span><span class="o">.</span><span class="n">size</span><span class="p">())):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s1">&#39;The </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s1">th input has a dimension with stride 0. gradcheck only &#39;</span>
                        <span class="s1">&#39;supports inputs that are non-overlapping to be able to &#39;</span>
                        <span class="s1">&#39;compute the numerical gradients correctly. You should call &#39;</span>
                        <span class="s1">&#39;.contiguous on the input before passing it to gradcheck.&#39;</span><span class="p">)</span>
            <span class="n">any_input_requiring_grad</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">inp</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">any_input_requiring_grad</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;gradcheck expects at least one input tensor to require gradient, &#39;</span>
            <span class="s1">&#39;but none of the them have requires_grad=True.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">check_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)):</span>
        <span class="c1"># it is easier to call to_dense() on the sparse output than</span>
        <span class="c1"># to modify analytical jacobian</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Sparse output is not supported at gradcheck yet. &#39;</span>
                         <span class="s1">&#39;Please call to_dense() on the output of fn for gradcheck.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)):</span>  <span class="c1"># type: ignore</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;MKLDNN output is not supported at gradcheck yet. &#39;</span>
                         <span class="s1">&#39;Please call to_dense() on the output of fn for gradcheck.&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_no_differentiable_outputs</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="c1"># When there are no differentiable outputs, numerical gradient for a function is</span>
    <span class="c1"># expected to be zero.</span>
    <span class="n">jacobians_all_inputs_outputs</span> <span class="o">=</span> <span class="n">_get_numerical_jacobian</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">jacobians_all_outputs_and_fixed_input</span> <span class="ow">in</span> <span class="n">jacobians_all_inputs_outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">jacobian</span> <span class="ow">in</span> <span class="n">jacobians_all_outputs_and_fixed_input</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">jacobian</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="s1">&#39;Numerical gradient for function expected to be zero&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">check_no_differentiable_outputs_fast</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">all_inputs</span><span class="p">,</span> <span class="n">inputs_indices</span><span class="p">,</span>
                                         <span class="n">all_u</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">inp_idx</span><span class="p">,</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs_indices</span><span class="p">,</span> <span class="n">all_u</span><span class="p">):</span>
        <span class="n">numerical_jacobians</span> <span class="o">=</span> <span class="n">get_jvp_wrt_specific_input</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inp_idx</span><span class="p">,</span> <span class="n">all_inputs</span><span class="p">,</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func_out</span><span class="p">),</span> <span class="n">u</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">jacobian</span> <span class="ow">in</span> <span class="n">numerical_jacobians</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">jacobian</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">jacobian</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">jacobian</span><span class="p">))</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">nondet_tol</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="s1">&#39;Numerical gradient for function expected to be zero&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="n">FAILED_BATCHED_GRAD_MSG</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">gradcheck or gradgradcheck failed while testing batched gradient computation.</span>
<span class="s2">This could have been invoked in a number of ways (via a test that calls</span>
<span class="s2">gradcheck/gradgradcheck directly or via an autogenerated test).</span>

<span class="s2">If you are adding a new operator, please file an issue and then use one of the</span>
<span class="s2">workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.</span>
<span class="s2">If the test</span>
<span class="s2">- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck</span>
<span class="s2">  with `check_batched_grad=False` as a keyword argument.</span>
<span class="s2">- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test</span>
<span class="s2">  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.</span>
<span class="s2">- is common_method_invocations-based, then add your test to the denylist</span>
<span class="s2">  EXCLUDE_BATCHED_GRAD_TESTS in test_autograd.py</span>

<span class="s2">If you&#39;re modifying an existing operator that supports batched grad computation,</span>
<span class="s2">or wish to make a new operator work with batched grad computation, please read</span>
<span class="s2">the following.</span>

<span class="s2">To compute batched grads (e.g., jacobians, hessians), we vmap over the backward</span>
<span class="s2">computation. The most common failure case is if there is a &#39;vmap-incompatible</span>
<span class="s2">operation&#39; in the backward pass. Please see</span>
<span class="s2">NOTE: [How to write vmap-compatible backward formulas]</span>
<span class="s2">in the codebase for an explanation of how to fix this.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">get_failed_batched_grad_test_msg</span><span class="p">(</span><span class="n">output_idx</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">exp</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">For output </span><span class="si">{</span><span class="n">output_idx</span><span class="si">}</span><span class="s2"> and input </span><span class="si">{</span><span class="n">input_idx</span><span class="si">}</span><span class="s2">:</span>

<span class="si">{</span><span class="n">FAILED_BATCHED_GRAD_MSG</span><span class="si">}</span><span class="s2"></span>

<span class="s2">Got:</span>
<span class="si">{</span><span class="n">res</span><span class="si">}</span><span class="s2"></span>

<span class="s2">Expected:</span>
<span class="si">{</span><span class="n">exp</span><span class="si">}</span><span class="s2"></span>
<span class="s2">&quot;&quot;&quot;</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">test_batched_grad</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="c1"># NB: test_batched_grad compares two autograd.grad invocations with a single</span>
    <span class="c1"># vmap(autograd.grad) invocation. It&#39;s not exactly a &quot;gradcheck&quot; in the</span>
    <span class="c1"># sense that we&#39;re not comparing an analytical jacobian with a numeric one,</span>
    <span class="c1"># but it is morally similar (we could have computed a full analytic jac</span>
    <span class="c1"># via vmap, but that is potentially slow)</span>
    <span class="n">diff_input_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">iter_tensors</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">grad</span> <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inp</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">inp</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">results</span>

    <span class="n">grad_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>

    <span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="n">vjp</span><span class="p">(</span><span class="n">gO</span><span class="p">)</span> <span class="k">for</span> <span class="n">gO</span> <span class="ow">in</span> <span class="n">grad_outputs</span><span class="p">]</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span> <span class="k">for</span> <span class="n">shards</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">expected</span><span class="p">)]</span>

    <span class="c1"># Squash warnings since these are expected to happen in most cases</span>
    <span class="c1"># NB: this doesn&#39;t work for CUDA tests: https://github.com/pytorch/pytorch/issues/50209</span>
    <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;Batching rule not implemented&quot;</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;torch.vmap is an experimental prototype&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">vjp</span><span class="p">)(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">))</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">ex</span><span class="p">:</span>
            <span class="c1"># It&#39;s OK that we&#39;re not raising the error at the correct callsite.</span>
            <span class="c1"># That&#39;s because the callsite is always going to inside the Python</span>
            <span class="c1"># autograd.grad instead of the C++ traceback of what line in the</span>
            <span class="c1"># backward formula</span>
            <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;While computing batched gradients, got: </span><span class="si">{</span><span class="n">ex</span><span class="si">}</span><span class="se">\n\n</span><span class="si">{</span><span class="n">FAILED_BATCHED_GRAD_MSG</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">input_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">exp</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">expected</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">exp</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="n">get_failed_batched_grad_test_msg</span><span class="p">(</span><span class="n">output_idx</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">exp</span><span class="p">))</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">test_backward_mul_by_grad_output</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">check_sparse_nnz</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="c1"># Tests that backward is multiplied by grad_output</span>
    <span class="n">diff_input_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">iter_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">diff_input_list</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;no Tensors requiring grad found in input&quot;</span><span class="p">)</span>
    <span class="n">grads_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">,</span>
                                      <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">legacy_contiguous_format</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">],</span>
                                      <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">gi</span><span class="p">,</span> <span class="n">di</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grads_input</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">gi</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gi</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">gi</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">layout</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="s1">&#39;grad is incorrect layout (&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">gi</span><span class="o">.</span><span class="n">layout</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; is not &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">di</span><span class="o">.</span><span class="n">layout</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;)&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">()</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">():</span>
                    <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="s1">&#39;grad is sparse tensor, but has incorrect sparse_dim&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">():</span>
                    <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="s1">&#39;grad is sparse tensor, but has incorrect dense_dim&#39;</span><span class="p">)</span>
            <span class="n">gi</span> <span class="o">=</span> <span class="n">gi</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
            <span class="n">di</span> <span class="o">=</span> <span class="n">di</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">check_sparse_nnz</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gi</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">gi</span><span class="p">)):</span>
                <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="s1">&#39;backward not multiplied by grad_output&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">gi</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="s1">&#39;backward not multiplied by grad_output&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">gi</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">device</span> <span class="ow">or</span> <span class="n">gi</span><span class="o">.</span><span class="n">is_sparse</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="s2">&quot;grad is incorrect type&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="s1">&#39;grad is incorrect size&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">test_undefined_grad</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">diff_input_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">iter_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">diff_input_list</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;no Tensors requiring grad found in input&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">warn_bc_breaking</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">((</span>
            <span class="s1">&#39;Backwards compatibility: New undefined gradient support checking &#39;</span>
            <span class="s1">&#39;feature is enabled by default, but it may break existing callers &#39;</span>
            <span class="s1">&#39;of this function. If this is true for you, you can call this &#39;</span>
            <span class="s1">&#39;function with &quot;check_undefined_grad=False&quot; to disable the feature&#39;</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">check_undefined_grad_support</span><span class="p">(</span><span class="n">output_to_check</span><span class="p">):</span>
        <span class="n">grads_output</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">legacy_contiguous_format</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">output_to_check</span><span class="p">]</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">grads_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">output_to_check</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">,</span>
                                              <span class="n">grads_output</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
            <span class="n">warn_bc_breaking</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">fail_test</span><span class="p">((</span>
                <span class="s1">&#39;Expected backward function to handle undefined output grads. &#39;</span>
                <span class="s1">&#39;Please look at &quot;Notes about undefined output gradients&quot; in &#39;</span>
                <span class="s1">&#39;&quot;tools/autograd/derivatives.yaml&quot;&#39;</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">gi</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grads_input</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">gi</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">gi</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()):</span>
                <span class="n">warn_bc_breaking</span><span class="p">()</span>
                <span class="k">return</span> <span class="n">fail_test</span><span class="p">((</span>
                    <span class="s1">&#39;Expected all input grads to be undefined or zero when all output grads are undefined &#39;</span>
                    <span class="s1">&#39;or zero. Please look at &quot;Notes about undefined output gradients&quot; in &#39;</span>
                    <span class="s1">&#39;&quot;tools/autograd/derivatives.yaml&quot;&#39;</span><span class="p">))</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="c1"># All backward functions must work properly if all output grads are undefined</span>
    <span class="n">outputs_to_check</span> <span class="o">=</span> <span class="p">[[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">UndefinedGrad</span><span class="p">()(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">))</span>
        <span class="c1"># This check filters out Tensor-likes that aren&#39;t instances of Tensor.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
    <span class="p">]]</span>

    <span class="c1"># If there are multiple output grads, we should be able to undef one at a time without error</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs_to_check</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">undef_grad_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)):</span>
            <span class="n">output_to_check</span> <span class="o">=</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">))</span>
            <span class="n">outputs_to_check</span><span class="o">.</span><span class="n">append</span><span class="p">([</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">UndefinedGrad</span><span class="p">()(</span><span class="n">o</span><span class="p">)</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">undef_grad_idx</span> <span class="k">else</span> <span class="n">o</span>
                <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_to_check</span><span class="p">)])</span>

    <span class="k">return</span> <span class="nb">all</span><span class="p">(</span><span class="n">check_undefined_grad_support</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs_to_check</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_as_tuple</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span>


<span class="k">def</span> <span class="nf">_differentiable_outputs</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_notallclose_msg</span><span class="p">(</span><span class="n">analytical</span><span class="p">,</span> <span class="n">numerical</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">error_str</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">error_str</span> <span class="o">+</span> <span class="s1">&#39;Jacobian mismatch for output </span><span class="si">%d</span><span class="s1"> with respect to input </span><span class="si">%d</span><span class="s1">,</span><span class="se">\n</span><span class="s1">&#39;</span> \
        <span class="s1">&#39;numerical:</span><span class="si">%s</span><span class="se">\n</span><span class="s1">analytical:</span><span class="si">%s</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">output_idx</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">numerical</span><span class="p">,</span> <span class="n">analytical</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="n">matrix_of_tensors</span><span class="p">):</span>
    <span class="c1"># returns list of tuples</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">matrix_of_tensors</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">slow_gradcheck</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span>
                   <span class="n">atol</span><span class="p">,</span> <span class="n">raise_exception</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">check_no_differentiable_outputs</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func_out</span><span class="p">),</span> <span class="n">eps</span><span class="p">)</span>

    <span class="n">numerical</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">_get_numerical_jacobian</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func_out</span><span class="p">)):</span>
        <span class="n">numerical_from_imag_grad_out</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">_get_numerical_jacobian</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">grad_out</span><span class="o">=</span><span class="mi">1</span><span class="n">j</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
        <span class="n">analytical</span><span class="p">,</span> <span class="n">failed</span> <span class="o">=</span> <span class="n">check_analytical_jacobian_attributes</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
                                                                  <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">raise_exception</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">failed</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
            <span class="n">analytical_from_imag_grad_out</span><span class="p">,</span> <span class="n">failed</span> <span class="o">=</span> <span class="n">check_analytical_jacobian_attributes</span><span class="p">(</span>
                <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="mi">1</span><span class="n">j</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">raise_exception</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">failed</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>

        <span class="n">inp_tensors</span> <span class="o">=</span> <span class="n">iter_tensors</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">inp</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">analytical</span><span class="p">,</span> <span class="n">numerical</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">inp_tensors</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">n</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>    <span class="c1"># C -&gt; C, R -&gt; C</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">analytical_from_imag_grad_out</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">numerical_from_imag_grad_out</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="n">get_notallclose_msg</span><span class="p">(</span><span class="n">analytical_from_imag_grad_out</span><span class="p">[</span><span class="n">j</span><span class="p">],</span>
                                                             <span class="n">numerical_from_imag_grad_out</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span>
                                                             <span class="s2">&quot;Gradients failed to compare equal for grad output = 1j. &quot;</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">inp</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>  <span class="c1"># C -&gt; R, C -&gt; C</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="n">get_notallclose_msg</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span>
                                                             <span class="s2">&quot;Gradients failed to compare equal for grad output = 1. &quot;</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>                 <span class="c1"># R -&gt; R, R -&gt; C</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="n">get_notallclose_msg</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">))</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">dot_with_type_promotion</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">u</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">v</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">u</span> <span class="o">*</span> <span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">allclose_with_type_promotion</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">):</span>
    <span class="n">promoted_type</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">promote_types</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">promoted_type</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">promoted_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">vec_from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span>
    <span class="c1"># Create a random vector with the same number of elements as x and the same dtype/device</span>
    <span class="c1"># If x is complex, we create a complex tensor with only real component</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
        <span class="c1"># For sparse, create a random sparse vec with random values in the same</span>
        <span class="c1"># indices. Make sure size is set so that it isn&#39;t inferred to be smaller.</span>
        <span class="n">x_values</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">x_values</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_values</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">/=</span> <span class="n">values</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
        <span class="n">vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">_indices</span><span class="p">(),</span> <span class="n">values</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">vec</span> <span class="o">/=</span> <span class="n">vec</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">vec</span>


<span class="k">def</span> <span class="nf">adjusted_atol</span><span class="p">(</span><span class="n">atol</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="c1"># In slow gradcheck, we compare A and B element-wise, i.e., for some a, b we allow |a - b| &lt; atol + rtol * b</span>
    <span class="c1"># but since we now compare q1 = v^T A u and q2 = v^T B u, we must allow |q1 - q2| &lt; v^T E u + rtol * v^T B u</span>
    <span class="c1"># where E is the correctly sized matrix where each entry is atol</span>
    <span class="c1">#</span>
    <span class="c1"># We see that atol needs to be scaled by v^T M u (where M is an all-ones M x N matrix):</span>
    <span class="c1"># v^T M u = \sum_{i} \sum_{j} u_i * v_j = (\sum_{i} u_i)(\sum_{i} v_i)</span>
    <span class="n">sum_u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="k">if</span> <span class="n">u</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span> <span class="k">else</span> <span class="n">u</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">sum_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span> <span class="k">else</span> <span class="n">v</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">atol</span> <span class="o">*</span> <span class="n">sum_u</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">sum_v</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>


<span class="n">FAST_FAIL_SLOW_OK_MSG</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Fast gradcheck failed but element-wise differences are small. This means that the</span>
<span class="s2">test might&#39;ve passed in slow_mode!</span>

<span class="s2">If you are adding a new operator, please file an issue and then use one of the</span>
<span class="s2">workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck:</span>

<span class="s2">If the test</span>
<span class="s2">- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck</span>
<span class="s2">  with `fast_mode=False` as a keyword argument.</span>
<span class="s2">- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test</span>
<span class="s2">  to have `gradcheck_fast_mode=False`</span>
<span class="s2">- is a Module test (e.g., in common_nn.py), then modify the corresponding</span>
<span class="s2">  module_test entry to have `gradcheck_fast_mode=False`</span>
<span class="s2">&quot;&quot;&quot;</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">run_slow_mode_and_get_error</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">):</span>
    <span class="c1"># Compute jacobians in slow mode for better error message</span>
    <span class="n">slow_numerical</span> <span class="o">=</span> <span class="n">_get_numerical_jacobian</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)[</span><span class="n">input_idx</span><span class="p">][</span><span class="n">output_idx</span><span class="p">]</span>
    <span class="n">slow_analytical</span> <span class="o">=</span> <span class="n">_get_analytical_jacobian</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">)</span>

    <span class="c1"># Assume jacobians are non-empty and have the same shape</span>
    <span class="n">slow_max_diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">slow_numerical</span> <span class="o">-</span> <span class="n">slow_analytical</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

    <span class="n">slow_allclose</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">slow_analytical</span><span class="p">,</span> <span class="n">slow_numerical</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">)</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The above quantities relating the numerical and analytical jacobians are computed </span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background </span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:</span><span class="se">\n\n</span><span class="s2">&quot;</span>
           <span class="sa">f</span><span class="s2">&quot;Numerical:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">slow_numerical</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="sa">f</span><span class="s2">&quot;Analytical:</span><span class="se">\n</span><span class="si">{</span><span class="n">slow_analytical</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">&quot;</span>
           <span class="sa">f</span><span class="s2">&quot;The max per-element difference (slow mode) is: </span><span class="si">{</span><span class="n">slow_max_diff</span><span class="si">}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">slow_allclose</span><span class="p">:</span>
        <span class="c1"># Slow gradcheck would&#39;ve passed!</span>
        <span class="n">msg</span> <span class="o">+=</span> <span class="n">FAST_FAIL_SLOW_OK_MSG</span>
    <span class="k">return</span> <span class="n">msg</span>


<span class="k">def</span> <span class="nf">fast_gradcheck</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span>
                   <span class="n">atol</span><span class="p">,</span> <span class="n">raise_exception</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">):</span>
    <span class="c1"># Perform the fast version of gradcheck</span>
    <span class="c1"># See https://github.com/pytorch/pytorch/issues/53876 for details</span>
    <span class="n">inp_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tupled_inputs</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>
    <span class="n">inp_tensor_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>

    <span class="c1"># Use our own generator to avoid messing with the user&#39;s RNG state</span>
    <span class="n">g_cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
    <span class="n">all_u</span> <span class="o">=</span> <span class="p">[</span><span class="n">vec_from_tensor</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">g_cpu</span><span class="p">)</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inp_tensors</span><span class="p">]</span>
    <span class="n">all_v</span> <span class="o">=</span> <span class="p">[</span><span class="n">vec_from_tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">g_cpu</span><span class="p">)</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">check_no_differentiable_outputs_fast</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">inp_tensor_indices</span><span class="p">,</span>
                                                    <span class="n">all_u</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="n">any_complex</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">)</span>
    <span class="n">complex_output_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()]</span>

    <span class="c1"># Initialize list of lists to store jacobians for each input, output pair</span>
    <span class="n">all_analytical</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span>
    <span class="n">all_numerical</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">inp_tensors</span><span class="p">]</span>
    <span class="n">all_analytical_from_imag_grad_out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">complex_output_indices</span><span class="p">]</span>
    <span class="n">all_numerical_from_imag_grad_out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">inp_tensors</span><span class="p">]</span>

    <span class="c1"># Numerically approximate v^T (J u)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">input_idx</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">inp_tensor_indices</span><span class="p">,</span> <span class="n">all_u</span><span class="p">)):</span>
        <span class="n">numerical</span> <span class="o">=</span> <span class="n">get_jvp_wrt_specific_input</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">numerical</span><span class="p">,</span> <span class="n">all_v</span><span class="p">)):</span>
            <span class="n">all_numerical</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dot_with_type_promotion</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">any_complex</span><span class="p">:</span>
            <span class="n">numerical_from_imag_grad_out</span> <span class="o">=</span> <span class="n">get_jvp_wrt_specific_input</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">complex_output_indices</span><span class="p">:</span>
                <span class="n">a</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">numerical_from_imag_grad_out</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">all_v</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                <span class="n">all_numerical_from_imag_grad_out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dot_with_type_promotion</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>

    <span class="c1"># Analytically calculate (v^T J) u</span>
    <span class="n">all_u_dense</span> <span class="o">=</span> <span class="p">[</span><span class="n">u</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">u</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span> <span class="k">else</span> <span class="n">u</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">all_u</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">all_v</span><span class="p">)):</span>
        <span class="n">analytical</span><span class="p">,</span> <span class="n">failed</span> <span class="o">=</span> <span class="n">check_analytical_jacobian_attributes</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span>
                                                                  <span class="n">raise_exception</span><span class="p">,</span> <span class="n">fast_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">failed</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">analytical</span><span class="p">,</span> <span class="n">all_u_dense</span><span class="p">):</span>
            <span class="n">all_analytical</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
            <span class="n">analytical_from_imag_grad_out</span><span class="p">,</span> <span class="n">failed</span> <span class="o">=</span> <span class="n">check_analytical_jacobian_attributes</span><span class="p">(</span>
                <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="mi">1</span><span class="n">j</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">raise_exception</span><span class="p">,</span> <span class="n">fast_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">failed</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>

            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">analytical_from_imag_grad_out</span><span class="p">,</span> <span class="n">all_u_dense</span><span class="p">)):</span>
                <span class="n">all_analytical_from_imag_grad_out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>

    <span class="n">prefix</span> <span class="o">=</span> <span class="s2">&quot;Gradients failed to compare equal for grad output = 1j (in fast mode). &quot;</span>
    <span class="c1"># Make sure analytical and numerical is same when calcaluted using grad_out = 1j</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">all_numerical_for_input_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_numerical_from_imag_grad_out</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_numerical_for_input_i</span><span class="p">):</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">all_analytical_from_imag_grad_out</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">allclose_with_type_promotion</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">adjusted_atol</span><span class="p">(</span><span class="n">atol</span><span class="p">,</span> <span class="n">all_u</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">all_v</span><span class="p">[</span><span class="n">j</span><span class="p">])):</span>
                <span class="n">jacobians_str</span> <span class="o">=</span> <span class="n">run_slow_mode_and_get_error</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="n">get_notallclose_msg</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span> <span class="o">+</span> <span class="n">jacobians_str</span><span class="p">)</span>

    <span class="c1"># Make sure analytical and numerical is the same</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">all_numerical_for_input_i</span><span class="p">,</span> <span class="n">inp</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">all_numerical</span><span class="p">,</span> <span class="n">inp_tensors</span><span class="p">)):</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">inp</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> \
            <span class="s2">&quot;Gradients failed to compare equal for grad output = 1 (in fast mode). &quot;</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_numerical_for_input_i</span><span class="p">):</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">all_analytical</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">allclose_with_type_promotion</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">adjusted_atol</span><span class="p">(</span><span class="n">atol</span><span class="p">,</span> <span class="n">all_u</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">all_v</span><span class="p">[</span><span class="n">j</span><span class="p">])):</span>
                <span class="n">jacobians_str</span> <span class="o">=</span> <span class="n">run_slow_mode_and_get_error</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">fail_test</span><span class="p">(</span><span class="n">get_notallclose_msg</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span> <span class="o">+</span> <span class="n">jacobians_str</span><span class="p">)</span>

    <span class="k">return</span> <span class="kc">True</span>


<span class="c1"># Note [VarArg of Tensors]</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1"># &#39;func&#39; accepts a vararg of tensors, which isn&#39;t expressable in the type system at the moment.</span>
<span class="c1"># If https://mypy.readthedocs.io/en/latest/additional_features.html?highlight=callable#extended-callable-types is accepted,</span>
<span class="c1"># the &#39;...&#39; first argument of Callable can be replaced with VarArg(Tensor).</span>
<span class="c1"># For now, we permit any input.</span>
<span class="c1"># the &#39;...&#39; first argument of Callable can be replaced with VarArg(Tensor).</span>
<span class="c1"># For now, we permit any input.</span>
<div class="viewcode-block" id="gradcheck"><a class="viewcode-back" href="../../../generated/torch.autograd.gradcheck.html#torch.autograd.gradcheck">[docs]</a><span class="k">def</span> <span class="nf">gradcheck</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">]],</span>  <span class="c1"># See Note [VarArg of Tensors]</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">raise_exception</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">check_sparse_nnz</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">nondet_tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">check_undefined_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">check_grad_dtypes</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_batched_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fast_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check gradients computed via small finite differences against analytical</span>
<span class="sd">    gradients w.r.t. tensors in :attr:`inputs` that are of floating point or complex type</span>
<span class="sd">    and with ``requires_grad=True``.</span>

<span class="sd">    The check between numerical and analytical gradients uses :func:`~torch.allclose`.</span>

<span class="sd">    For complex functions, no notion of Jacobian exists. Gradcheck verifies if the numerical and</span>
<span class="sd">    analytical values of Wirtinger and Conjugate Wirtinger derivative are consistent. The gradient</span>
<span class="sd">    computation is done under the assumption that the overall function has a real valued output.</span>
<span class="sd">    For functions with complex output, gradcheck compares the numerical and analytical gradients</span>
<span class="sd">    for two values of :attr:`grad_output`: 1 and 1j. For more details, check out</span>
<span class="sd">    :ref:`complex_autograd-doc`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The default values are designed for :attr:`input` of double precision.</span>
<span class="sd">        This check will likely fail if :attr:`input` is of less precision, e.g.,</span>
<span class="sd">        ``FloatTensor``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">       If any checked tensor in :attr:`input` has overlapping memory, i.e.,</span>
<span class="sd">       different indices pointing to the same memory address (e.g., from</span>
<span class="sd">       :func:`torch.expand`), this check will likely fail because the numerical</span>
<span class="sd">       gradients computed by point perturbation at such indices will change</span>
<span class="sd">       values at all other indices that share the same memory address.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): a Python function that takes Tensor inputs and returns</span>
<span class="sd">            a Tensor or a tuple of Tensors</span>
<span class="sd">        inputs (tuple of Tensor or Tensor): inputs to the function</span>
<span class="sd">        eps (float, optional): perturbation for finite differences</span>
<span class="sd">        atol (float, optional): absolute tolerance</span>
<span class="sd">        rtol (float, optional): relative tolerance</span>
<span class="sd">        raise_exception (bool, optional): indicating whether to raise an exception if</span>
<span class="sd">            the check fails. The exception gives more information about the</span>
<span class="sd">            exact nature of the failure. This is helpful when debugging gradchecks.</span>
<span class="sd">        check_sparse_nnz (bool, optional): if True, gradcheck allows for SparseTensor input,</span>
<span class="sd">            and for any SparseTensor at input, gradcheck will perform check at nnz positions only.</span>
<span class="sd">        nondet_tol (float, optional): tolerance for non-determinism. When running</span>
<span class="sd">            identical inputs through the differentiation, the results must either match</span>
<span class="sd">            exactly (default, 0.0) or be within this tolerance.</span>
<span class="sd">        check_undefined_grad (bool, optional): if True, check if undefined output grads</span>
<span class="sd">            are supported and treated as zeros, for ``Tensor`` outputs.</span>
<span class="sd">        check_batched_grad (bool, optional): if True, check if we can compute</span>
<span class="sd">            batched gradients using prototype vmap support. Defaults to False.</span>
<span class="sd">        fast_mode (bool, optional): Fast mode for gradcheck and gradgradcheck is currently only</span>
<span class="sd">            implemented for R to R functions. If none of the inputs and outputs are complex</span>
<span class="sd">            a faster implementation of gradcheck that no longer computes the entire jacobian</span>
<span class="sd">            is run; otherwise, we fall back to the slow implementation.</span>

<span class="sd">    Returns:</span>
<span class="sd">        True if all differences satisfy allclose condition</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">fail_test</span><span class="p">(</span><span class="n">msg</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">raise_exception</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="n">tupled_inputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">check_inputs</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">check_sparse_nnz</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="n">func_out</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">tupled_inputs</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="nb">any</span><span class="p">(</span><span class="n">is_tensor_like</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func_out</span><span class="p">))</span> <span class="ow">or</span>
            <span class="nb">any</span><span class="p">(</span><span class="n">is_tensor_like</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="ow">and</span> <span class="n">i</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tupled_inputs</span><span class="p">)):</span>
        <span class="n">fast_mode</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">func_out</span><span class="p">)</span>

    <span class="n">check_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fast_mode</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">fast_gradcheck</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span>
                              <span class="n">atol</span><span class="p">,</span> <span class="n">raise_exception</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">slow_gradcheck</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span>
                              <span class="n">atol</span><span class="p">,</span> <span class="n">raise_exception</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">check_batched_grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">test_batched_grad</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
                <span class="k">return</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">test_backward_mul_by_grad_output</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">check_sparse_nnz</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">check_undefined_grad</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">test_undefined_grad</span><span class="p">(</span><span class="n">fail_test</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="k">return</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="gradgradcheck"><a class="viewcode-back" href="../../../generated/torch.autograd.gradgradcheck.html#torch.autograd.gradgradcheck">[docs]</a><span class="k">def</span> <span class="nf">gradgradcheck</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">_TensorOrTensors</span><span class="p">],</span>  <span class="c1"># See Note [VarArg of Tensors]</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">,</span>
    <span class="n">grad_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">gen_non_contig_grad_outputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">raise_exception</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">nondet_tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">check_undefined_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">check_grad_dtypes</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_batched_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fast_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check gradients of gradients computed via small finite differences</span>
<span class="sd">    against analytical gradients w.r.t. tensors in :attr:`inputs` and</span>
<span class="sd">    :attr:`grad_outputs` that are of floating point or complex type and with</span>
<span class="sd">    ``requires_grad=True``.</span>

<span class="sd">    This function checks that backpropagating through the gradients computed</span>
<span class="sd">    to the given :attr:`grad_outputs` are correct.</span>

<span class="sd">    The check between numerical and analytical gradients uses :func:`~torch.allclose`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The default values are designed for :attr:`input` and</span>
<span class="sd">        :attr:`grad_outputs` of double precision. This check will likely fail if</span>
<span class="sd">        they are of less precision, e.g., ``FloatTensor``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">       If any checked tensor in :attr:`input` and :attr:`grad_outputs` has</span>
<span class="sd">       overlapping memory, i.e., different indices pointing to the same memory</span>
<span class="sd">       address (e.g., from :func:`torch.expand`), this check will likely fail</span>
<span class="sd">       because the numerical gradients computed by point perturbation at such</span>
<span class="sd">       indices will change values at all other indices that share the same</span>
<span class="sd">       memory address.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): a Python function that takes Tensor inputs and returns</span>
<span class="sd">            a Tensor or a tuple of Tensors</span>
<span class="sd">        inputs (tuple of Tensor or Tensor): inputs to the function</span>
<span class="sd">        grad_outputs (tuple of Tensor or Tensor, optional): The gradients with</span>
<span class="sd">            respect to the function&#39;s outputs.</span>
<span class="sd">        eps (float, optional): perturbation for finite differences</span>
<span class="sd">        atol (float, optional): absolute tolerance</span>
<span class="sd">        rtol (float, optional): relative tolerance</span>
<span class="sd">        gen_non_contig_grad_outputs (bool, optional): if :attr:`grad_outputs` is</span>
<span class="sd">            ``None`` and :attr:`gen_non_contig_grad_outputs` is ``True``, the</span>
<span class="sd">            randomly generated gradient outputs are made to be noncontiguous</span>
<span class="sd">        raise_exception (bool, optional): indicating whether to raise an exception if</span>
<span class="sd">            the check fails. The exception gives more information about the</span>
<span class="sd">            exact nature of the failure. This is helpful when debugging gradchecks.</span>
<span class="sd">        nondet_tol (float, optional): tolerance for non-determinism. When running</span>
<span class="sd">            identical inputs through the differentiation, the results must either match</span>
<span class="sd">            exactly (default, 0.0) or be within this tolerance. Note that a small amount</span>
<span class="sd">            of nondeterminism in the gradient will lead to larger inaccuracies in</span>
<span class="sd">            the second derivative.</span>
<span class="sd">        check_undefined_grad (bool, optional): if True, check if undefined output grads</span>
<span class="sd">            are supported and treated as zeros</span>
<span class="sd">        check_batched_grad (bool, optional): if True, check if we can compute</span>
<span class="sd">            batched gradients using prototype vmap support. Defaults to False.</span>
<span class="sd">        fast_mode (bool, optional): if True, run a faster implementation of gradgradcheck that</span>
<span class="sd">            no longer computes the entire jacobian.</span>

<span class="sd">    Returns:</span>
<span class="sd">        True if all differences satisfy allclose condition</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tupled_inputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">grad_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># If grad_outputs is not specified, create random Tensors of the same</span>
        <span class="c1"># shape, type, and device as the outputs</span>
        <span class="k">def</span> <span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span>
                <span class="n">x</span> <span class="k">if</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">is_complex</span><span class="p">())</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">legacy_contiguous_format</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">gen_non_contig_grad_outputs</span><span class="p">:</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">make_non_contiguous</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">tupled_inputs</span><span class="p">))</span>
        <span class="n">tupled_grad_outputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tupled_grad_outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">)</span>

    <span class="n">num_outputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tupled_grad_outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">new_func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">input_args</span> <span class="o">=</span> <span class="n">args</span><span class="p">[:</span><span class="o">-</span><span class="n">num_outputs</span><span class="p">]</span>
        <span class="n">grad_outputs</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="o">-</span><span class="n">num_outputs</span><span class="p">:]</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">input_args</span><span class="p">))</span>
        <span class="n">input_args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
        <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">input_args</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_inputs</span>

    <span class="k">return</span> <span class="n">gradcheck</span><span class="p">(</span>
        <span class="n">new_func</span><span class="p">,</span> <span class="n">tupled_inputs</span> <span class="o">+</span> <span class="n">tupled_grad_outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">raise_exception</span><span class="p">,</span>
        <span class="n">nondet_tol</span><span class="o">=</span><span class="n">nondet_tol</span><span class="p">,</span> <span class="n">check_undefined_grad</span><span class="o">=</span><span class="n">check_undefined_grad</span><span class="p">,</span>
        <span class="n">check_grad_dtypes</span><span class="o">=</span><span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">check_batched_grad</span><span class="o">=</span><span class="n">check_batched_grad</span><span class="p">,</span> <span class="n">fast_mode</span><span class="o">=</span><span class="n">fast_mode</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>